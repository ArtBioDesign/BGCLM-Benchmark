{
  "best_global_step": 52600,
  "best_metric": 1.1511740684509277,
  "best_model_checkpoint": "./pretraining/checkpoint-52600",
  "epoch": 208.5619512195122,
  "eval_steps": 200,
  "global_step": 53600,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.1951219512195122,
      "grad_norm": 106.17933654785156,
      "learning_rate": 1.2710765239948119e-08,
      "loss": 115.8536,
      "step": 50
    },
    {
      "epoch": 0.3902439024390244,
      "grad_norm": 106.34381103515625,
      "learning_rate": 2.568093385214008e-08,
      "loss": 115.7303,
      "step": 100
    },
    {
      "epoch": 0.5853658536585366,
      "grad_norm": 106.16355895996094,
      "learning_rate": 3.865110246433204e-08,
      "loss": 115.4833,
      "step": 150
    },
    {
      "epoch": 0.7804878048780488,
      "grad_norm": 106.23531341552734,
      "learning_rate": 5.1621271076524e-08,
      "loss": 115.1284,
      "step": 200
    },
    {
      "epoch": 0.7804878048780488,
      "eval_loss": 1.794109582901001,
      "eval_perplexity": 6.014117240905762,
      "eval_runtime": 83.0298,
      "eval_samples_per_second": 23.883,
      "eval_steps_per_second": 2.987,
      "step": 200
    },
    {
      "epoch": 0.975609756097561,
      "grad_norm": 101.95006561279297,
      "learning_rate": 6.459143968871595e-08,
      "loss": 114.6137,
      "step": 250
    },
    {
      "epoch": 1.1678048780487804,
      "grad_norm": 98.9219741821289,
      "learning_rate": 7.756160830090792e-08,
      "loss": 112.3077,
      "step": 300
    },
    {
      "epoch": 1.3629268292682926,
      "grad_norm": 98.16128540039062,
      "learning_rate": 9.053177691309988e-08,
      "loss": 113.2522,
      "step": 350
    },
    {
      "epoch": 1.558048780487805,
      "grad_norm": 97.64498138427734,
      "learning_rate": 1.0350194552529183e-07,
      "loss": 112.436,
      "step": 400
    },
    {
      "epoch": 1.558048780487805,
      "eval_loss": 1.7480112314224243,
      "eval_perplexity": 5.74316930770874,
      "eval_runtime": 47.0488,
      "eval_samples_per_second": 42.148,
      "eval_steps_per_second": 5.271,
      "step": 400
    },
    {
      "epoch": 1.753170731707317,
      "grad_norm": 95.58248138427734,
      "learning_rate": 1.164721141374838e-07,
      "loss": 111.4769,
      "step": 450
    },
    {
      "epoch": 1.9482926829268292,
      "grad_norm": 91.68624114990234,
      "learning_rate": 1.2944228274967577e-07,
      "loss": 110.4742,
      "step": 500
    },
    {
      "epoch": 2.140487804878049,
      "grad_norm": 90.23336791992188,
      "learning_rate": 1.424124513618677e-07,
      "loss": 107.6702,
      "step": 550
    },
    {
      "epoch": 2.335609756097561,
      "grad_norm": 87.07270050048828,
      "learning_rate": 1.5538261997405968e-07,
      "loss": 108.0833,
      "step": 600
    },
    {
      "epoch": 2.335609756097561,
      "eval_loss": 1.676364541053772,
      "eval_perplexity": 5.346085071563721,
      "eval_runtime": 47.1986,
      "eval_samples_per_second": 42.014,
      "eval_steps_per_second": 5.254,
      "step": 600
    },
    {
      "epoch": 2.530731707317073,
      "grad_norm": 87.53263092041016,
      "learning_rate": 1.6835278858625164e-07,
      "loss": 106.7865,
      "step": 650
    },
    {
      "epoch": 2.725853658536585,
      "grad_norm": 83.86491394042969,
      "learning_rate": 1.8132295719844358e-07,
      "loss": 105.4445,
      "step": 700
    },
    {
      "epoch": 2.9209756097560975,
      "grad_norm": 80.3230972290039,
      "learning_rate": 1.9429312581063553e-07,
      "loss": 104.1372,
      "step": 750
    },
    {
      "epoch": 3.113170731707317,
      "grad_norm": 76.91529846191406,
      "learning_rate": 2.0726329442282752e-07,
      "loss": 101.2681,
      "step": 800
    },
    {
      "epoch": 3.113170731707317,
      "eval_loss": 1.5922776460647583,
      "eval_perplexity": 4.914930820465088,
      "eval_runtime": 47.4804,
      "eval_samples_per_second": 41.765,
      "eval_steps_per_second": 5.223,
      "step": 800
    },
    {
      "epoch": 3.308292682926829,
      "grad_norm": 73.74827575683594,
      "learning_rate": 2.2023346303501946e-07,
      "loss": 101.4296,
      "step": 850
    },
    {
      "epoch": 3.5034146341463415,
      "grad_norm": 70.02806091308594,
      "learning_rate": 2.3320363164721143e-07,
      "loss": 100.1048,
      "step": 900
    },
    {
      "epoch": 3.6985365853658534,
      "grad_norm": 66.98492431640625,
      "learning_rate": 2.461738002594034e-07,
      "loss": 98.8747,
      "step": 950
    },
    {
      "epoch": 3.8936585365853658,
      "grad_norm": 63.51540756225586,
      "learning_rate": 2.5914396887159534e-07,
      "loss": 97.6858,
      "step": 1000
    },
    {
      "epoch": 3.8936585365853658,
      "eval_loss": 1.5136950016021729,
      "eval_perplexity": 4.543488025665283,
      "eval_runtime": 47.5302,
      "eval_samples_per_second": 41.721,
      "eval_steps_per_second": 5.218,
      "step": 1000
    },
    {
      "epoch": 4.085853658536585,
      "grad_norm": 58.77969741821289,
      "learning_rate": 2.721141374837873e-07,
      "loss": 95.1391,
      "step": 1050
    },
    {
      "epoch": 4.280975609756098,
      "grad_norm": 54.434391021728516,
      "learning_rate": 2.850843060959793e-07,
      "loss": 95.4688,
      "step": 1100
    },
    {
      "epoch": 4.47609756097561,
      "grad_norm": 50.53400802612305,
      "learning_rate": 2.9805447470817124e-07,
      "loss": 94.5279,
      "step": 1150
    },
    {
      "epoch": 4.671219512195122,
      "grad_norm": 46.89633560180664,
      "learning_rate": 3.1102464332036316e-07,
      "loss": 93.5594,
      "step": 1200
    },
    {
      "epoch": 4.671219512195122,
      "eval_loss": 1.4527404308319092,
      "eval_perplexity": 4.274813175201416,
      "eval_runtime": 46.6384,
      "eval_samples_per_second": 42.519,
      "eval_steps_per_second": 5.318,
      "step": 1200
    },
    {
      "epoch": 4.866341463414634,
      "grad_norm": 43.19329833984375,
      "learning_rate": 3.239948119325551e-07,
      "loss": 92.7048,
      "step": 1250
    },
    {
      "epoch": 5.058536585365854,
      "grad_norm": 39.56672668457031,
      "learning_rate": 3.369649805447471e-07,
      "loss": 90.6431,
      "step": 1300
    },
    {
      "epoch": 5.253658536585366,
      "grad_norm": 38.1009521484375,
      "learning_rate": 3.499351491569391e-07,
      "loss": 91.2524,
      "step": 1350
    },
    {
      "epoch": 5.4487804878048784,
      "grad_norm": 34.454833984375,
      "learning_rate": 3.6290531776913103e-07,
      "loss": 90.7061,
      "step": 1400
    },
    {
      "epoch": 5.4487804878048784,
      "eval_loss": 1.409406065940857,
      "eval_perplexity": 4.0935235023498535,
      "eval_runtime": 46.7451,
      "eval_samples_per_second": 42.422,
      "eval_steps_per_second": 5.305,
      "step": 1400
    },
    {
      "epoch": 5.64390243902439,
      "grad_norm": 31.90234375,
      "learning_rate": 3.75875486381323e-07,
      "loss": 90.0632,
      "step": 1450
    },
    {
      "epoch": 5.839024390243902,
      "grad_norm": 28.549837112426758,
      "learning_rate": 3.8884565499351496e-07,
      "loss": 89.464,
      "step": 1500
    },
    {
      "epoch": 6.031219512195122,
      "grad_norm": 27.60166358947754,
      "learning_rate": 4.018158236057069e-07,
      "loss": 87.6146,
      "step": 1550
    },
    {
      "epoch": 6.226341463414634,
      "grad_norm": 24.702611923217773,
      "learning_rate": 4.1478599221789885e-07,
      "loss": 88.534,
      "step": 1600
    },
    {
      "epoch": 6.226341463414634,
      "eval_loss": 1.3777656555175781,
      "eval_perplexity": 3.9660301208496094,
      "eval_runtime": 47.2059,
      "eval_samples_per_second": 42.007,
      "eval_steps_per_second": 5.254,
      "step": 1600
    },
    {
      "epoch": 6.421463414634147,
      "grad_norm": 24.771472930908203,
      "learning_rate": 4.2775616083009087e-07,
      "loss": 88.0076,
      "step": 1650
    },
    {
      "epoch": 6.616585365853658,
      "grad_norm": 20.777475357055664,
      "learning_rate": 4.407263294422828e-07,
      "loss": 87.7482,
      "step": 1700
    },
    {
      "epoch": 6.811707317073171,
      "grad_norm": 19.372419357299805,
      "learning_rate": 4.5369649805447475e-07,
      "loss": 87.4714,
      "step": 1750
    },
    {
      "epoch": 7.00390243902439,
      "grad_norm": 17.461458206176758,
      "learning_rate": 4.666666666666667e-07,
      "loss": 85.8927,
      "step": 1800
    },
    {
      "epoch": 7.00390243902439,
      "eval_loss": 1.3586180210113525,
      "eval_perplexity": 3.890812635421753,
      "eval_runtime": 46.9064,
      "eval_samples_per_second": 42.276,
      "eval_steps_per_second": 5.287,
      "step": 1800
    },
    {
      "epoch": 7.199024390243903,
      "grad_norm": 17.500680923461914,
      "learning_rate": 4.796368352788586e-07,
      "loss": 86.9103,
      "step": 1850
    },
    {
      "epoch": 7.394146341463415,
      "grad_norm": 15.025941848754883,
      "learning_rate": 4.926070038910507e-07,
      "loss": 86.7032,
      "step": 1900
    },
    {
      "epoch": 7.5892682926829265,
      "grad_norm": 14.753527641296387,
      "learning_rate": 5.055771725032426e-07,
      "loss": 86.4868,
      "step": 1950
    },
    {
      "epoch": 7.784390243902439,
      "grad_norm": 13.74337100982666,
      "learning_rate": 5.185473411154345e-07,
      "loss": 86.3771,
      "step": 2000
    },
    {
      "epoch": 7.784390243902439,
      "eval_loss": 1.3458021879196167,
      "eval_perplexity": 3.841266632080078,
      "eval_runtime": 47.081,
      "eval_samples_per_second": 42.119,
      "eval_steps_per_second": 5.268,
      "step": 2000
    },
    {
      "epoch": 7.979512195121951,
      "grad_norm": 12.404221534729004,
      "learning_rate": 5.315175097276265e-07,
      "loss": 86.0842,
      "step": 2050
    },
    {
      "epoch": 8.17170731707317,
      "grad_norm": 12.11827564239502,
      "learning_rate": 5.444876783398185e-07,
      "loss": 84.6297,
      "step": 2100
    },
    {
      "epoch": 8.366829268292683,
      "grad_norm": 13.06328010559082,
      "learning_rate": 5.574578469520104e-07,
      "loss": 85.8129,
      "step": 2150
    },
    {
      "epoch": 8.561951219512196,
      "grad_norm": 11.412004470825195,
      "learning_rate": 5.704280155642024e-07,
      "loss": 85.6261,
      "step": 2200
    },
    {
      "epoch": 8.561951219512196,
      "eval_loss": 1.335348129272461,
      "eval_perplexity": 3.801319122314453,
      "eval_runtime": 46.9496,
      "eval_samples_per_second": 42.237,
      "eval_steps_per_second": 5.282,
      "step": 2200
    },
    {
      "epoch": 8.757073170731708,
      "grad_norm": 10.442998886108398,
      "learning_rate": 5.833981841763944e-07,
      "loss": 85.4853,
      "step": 2250
    },
    {
      "epoch": 8.95219512195122,
      "grad_norm": 10.629544258117676,
      "learning_rate": 5.963683527885863e-07,
      "loss": 85.3762,
      "step": 2300
    },
    {
      "epoch": 9.14439024390244,
      "grad_norm": 9.568942070007324,
      "learning_rate": 6.093385214007783e-07,
      "loss": 83.9264,
      "step": 2350
    },
    {
      "epoch": 9.339512195121952,
      "grad_norm": 8.981913566589355,
      "learning_rate": 6.223086900129702e-07,
      "loss": 85.1556,
      "step": 2400
    },
    {
      "epoch": 9.339512195121952,
      "eval_loss": 1.3272343873977661,
      "eval_perplexity": 3.7706010341644287,
      "eval_runtime": 47.7142,
      "eval_samples_per_second": 41.56,
      "eval_steps_per_second": 5.198,
      "step": 2400
    },
    {
      "epoch": 9.534634146341464,
      "grad_norm": 9.884515762329102,
      "learning_rate": 6.352788586251622e-07,
      "loss": 85.0045,
      "step": 2450
    },
    {
      "epoch": 9.729756097560976,
      "grad_norm": 10.19356918334961,
      "learning_rate": 6.482490272373542e-07,
      "loss": 84.8911,
      "step": 2500
    },
    {
      "epoch": 9.924878048780489,
      "grad_norm": 9.42248249053955,
      "learning_rate": 6.612191958495461e-07,
      "loss": 84.8004,
      "step": 2550
    },
    {
      "epoch": 10.117073170731707,
      "grad_norm": 7.9049482345581055,
      "learning_rate": 6.741893644617381e-07,
      "loss": 83.4056,
      "step": 2600
    },
    {
      "epoch": 10.117073170731707,
      "eval_loss": 1.3217098712921143,
      "eval_perplexity": 3.7498276233673096,
      "eval_runtime": 47.3759,
      "eval_samples_per_second": 41.857,
      "eval_steps_per_second": 5.235,
      "step": 2600
    },
    {
      "epoch": 10.31219512195122,
      "grad_norm": 8.302741050720215,
      "learning_rate": 6.8715953307393e-07,
      "loss": 84.7342,
      "step": 2650
    },
    {
      "epoch": 10.507317073170732,
      "grad_norm": 6.776400566101074,
      "learning_rate": 7.001297016861219e-07,
      "loss": 84.5601,
      "step": 2700
    },
    {
      "epoch": 10.702439024390245,
      "grad_norm": 8.357100486755371,
      "learning_rate": 7.130998702983139e-07,
      "loss": 84.5511,
      "step": 2750
    },
    {
      "epoch": 10.897560975609757,
      "grad_norm": 6.7516560554504395,
      "learning_rate": 7.260700389105059e-07,
      "loss": 84.5418,
      "step": 2800
    },
    {
      "epoch": 10.897560975609757,
      "eval_loss": 1.3178893327713013,
      "eval_perplexity": 3.7355287075042725,
      "eval_runtime": 47.1407,
      "eval_samples_per_second": 42.066,
      "eval_steps_per_second": 5.261,
      "step": 2800
    },
    {
      "epoch": 11.089756097560976,
      "grad_norm": 6.4276556968688965,
      "learning_rate": 7.390402075226978e-07,
      "loss": 83.1448,
      "step": 2850
    },
    {
      "epoch": 11.284878048780488,
      "grad_norm": 6.280012607574463,
      "learning_rate": 7.520103761348899e-07,
      "loss": 84.3864,
      "step": 2900
    },
    {
      "epoch": 11.48,
      "grad_norm": 6.204626083374023,
      "learning_rate": 7.649805447470818e-07,
      "loss": 84.2789,
      "step": 2950
    },
    {
      "epoch": 11.675121951219513,
      "grad_norm": 6.844421863555908,
      "learning_rate": 7.779507133592737e-07,
      "loss": 84.3413,
      "step": 3000
    },
    {
      "epoch": 11.675121951219513,
      "eval_loss": 1.3150873184204102,
      "eval_perplexity": 3.725076198577881,
      "eval_runtime": 47.6368,
      "eval_samples_per_second": 41.627,
      "eval_steps_per_second": 5.206,
      "step": 3000
    },
    {
      "epoch": 11.870243902439025,
      "grad_norm": 6.242316722869873,
      "learning_rate": 7.909208819714658e-07,
      "loss": 84.3065,
      "step": 3050
    },
    {
      "epoch": 12.062439024390244,
      "grad_norm": 5.866445064544678,
      "learning_rate": 8.038910505836577e-07,
      "loss": 82.9569,
      "step": 3100
    },
    {
      "epoch": 12.257560975609756,
      "grad_norm": 6.3116230964660645,
      "learning_rate": 8.168612191958496e-07,
      "loss": 84.1103,
      "step": 3150
    },
    {
      "epoch": 12.452682926829269,
      "grad_norm": 7.980415344238281,
      "learning_rate": 8.298313878080416e-07,
      "loss": 84.0955,
      "step": 3200
    },
    {
      "epoch": 12.452682926829269,
      "eval_loss": 1.312862753868103,
      "eval_perplexity": 3.716798782348633,
      "eval_runtime": 47.2327,
      "eval_samples_per_second": 41.984,
      "eval_steps_per_second": 5.251,
      "step": 3200
    },
    {
      "epoch": 12.647804878048781,
      "grad_norm": 6.242307186126709,
      "learning_rate": 8.428015564202335e-07,
      "loss": 84.1687,
      "step": 3250
    },
    {
      "epoch": 12.842926829268293,
      "grad_norm": 5.4278388023376465,
      "learning_rate": 8.557717250324254e-07,
      "loss": 84.1163,
      "step": 3300
    },
    {
      "epoch": 13.035121951219512,
      "grad_norm": 6.270937919616699,
      "learning_rate": 8.687418936446175e-07,
      "loss": 82.8456,
      "step": 3350
    },
    {
      "epoch": 13.230243902439025,
      "grad_norm": 6.463988780975342,
      "learning_rate": 8.817120622568094e-07,
      "loss": 83.998,
      "step": 3400
    },
    {
      "epoch": 13.230243902439025,
      "eval_loss": 1.3109241724014282,
      "eval_perplexity": 3.7096004486083984,
      "eval_runtime": 46.6335,
      "eval_samples_per_second": 42.523,
      "eval_steps_per_second": 5.318,
      "step": 3400
    },
    {
      "epoch": 13.425365853658537,
      "grad_norm": 5.030392169952393,
      "learning_rate": 8.946822308690013e-07,
      "loss": 83.9289,
      "step": 3450
    },
    {
      "epoch": 13.62048780487805,
      "grad_norm": 5.821113109588623,
      "learning_rate": 9.076523994811934e-07,
      "loss": 83.9239,
      "step": 3500
    },
    {
      "epoch": 13.815609756097562,
      "grad_norm": 10.319461822509766,
      "learning_rate": 9.206225680933853e-07,
      "loss": 84.015,
      "step": 3550
    },
    {
      "epoch": 14.00780487804878,
      "grad_norm": 4.496682643890381,
      "learning_rate": 9.335927367055772e-07,
      "loss": 82.7798,
      "step": 3600
    },
    {
      "epoch": 14.00780487804878,
      "eval_loss": 1.3090944290161133,
      "eval_perplexity": 3.7028191089630127,
      "eval_runtime": 46.7007,
      "eval_samples_per_second": 42.462,
      "eval_steps_per_second": 5.31,
      "step": 3600
    },
    {
      "epoch": 14.202926829268293,
      "grad_norm": 5.1905717849731445,
      "learning_rate": 9.465629053177693e-07,
      "loss": 83.8261,
      "step": 3650
    },
    {
      "epoch": 14.398048780487805,
      "grad_norm": 4.7719407081604,
      "learning_rate": 9.595330739299612e-07,
      "loss": 83.8273,
      "step": 3700
    },
    {
      "epoch": 14.593170731707318,
      "grad_norm": 4.996704578399658,
      "learning_rate": 9.72503242542153e-07,
      "loss": 83.8099,
      "step": 3750
    },
    {
      "epoch": 14.78829268292683,
      "grad_norm": 7.472243785858154,
      "learning_rate": 9.854734111543452e-07,
      "loss": 83.799,
      "step": 3800
    },
    {
      "epoch": 14.78829268292683,
      "eval_loss": 1.3070499897003174,
      "eval_perplexity": 3.695256471633911,
      "eval_runtime": 46.7955,
      "eval_samples_per_second": 42.376,
      "eval_steps_per_second": 5.3,
      "step": 3800
    },
    {
      "epoch": 14.983414634146342,
      "grad_norm": 5.758685111999512,
      "learning_rate": 9.984435797665371e-07,
      "loss": 83.8432,
      "step": 3850
    },
    {
      "epoch": 15.175609756097561,
      "grad_norm": 5.981950759887695,
      "learning_rate": 1.011413748378729e-06,
      "loss": 82.4281,
      "step": 3900
    },
    {
      "epoch": 15.370731707317074,
      "grad_norm": 4.313140392303467,
      "learning_rate": 1.024383916990921e-06,
      "loss": 83.7325,
      "step": 3950
    },
    {
      "epoch": 15.565853658536586,
      "grad_norm": 4.974806785583496,
      "learning_rate": 1.0373540856031129e-06,
      "loss": 83.6397,
      "step": 4000
    },
    {
      "epoch": 15.565853658536586,
      "eval_loss": 1.3046166896820068,
      "eval_perplexity": 3.6862757205963135,
      "eval_runtime": 46.3393,
      "eval_samples_per_second": 42.793,
      "eval_steps_per_second": 5.352,
      "step": 4000
    },
    {
      "epoch": 15.760975609756098,
      "grad_norm": 5.323954105377197,
      "learning_rate": 1.0503242542153048e-06,
      "loss": 83.4402,
      "step": 4050
    },
    {
      "epoch": 15.95609756097561,
      "grad_norm": 6.846445560455322,
      "learning_rate": 1.063294422827497e-06,
      "loss": 83.6495,
      "step": 4100
    },
    {
      "epoch": 16.14829268292683,
      "grad_norm": 5.393444061279297,
      "learning_rate": 1.0762645914396888e-06,
      "loss": 82.3601,
      "step": 4150
    },
    {
      "epoch": 16.34341463414634,
      "grad_norm": 6.555146217346191,
      "learning_rate": 1.0892347600518807e-06,
      "loss": 83.3366,
      "step": 4200
    },
    {
      "epoch": 16.34341463414634,
      "eval_loss": 1.3012182712554932,
      "eval_perplexity": 3.673769474029541,
      "eval_runtime": 47.1321,
      "eval_samples_per_second": 42.073,
      "eval_steps_per_second": 5.262,
      "step": 4200
    },
    {
      "epoch": 16.538536585365854,
      "grad_norm": 5.898374557495117,
      "learning_rate": 1.1022049286640729e-06,
      "loss": 83.3617,
      "step": 4250
    },
    {
      "epoch": 16.733658536585367,
      "grad_norm": 7.373749256134033,
      "learning_rate": 1.1151750972762648e-06,
      "loss": 83.2975,
      "step": 4300
    },
    {
      "epoch": 16.92878048780488,
      "grad_norm": 8.080221176147461,
      "learning_rate": 1.1281452658884567e-06,
      "loss": 83.3624,
      "step": 4350
    },
    {
      "epoch": 17.120975609756098,
      "grad_norm": 5.987873554229736,
      "learning_rate": 1.1411154345006486e-06,
      "loss": 81.992,
      "step": 4400
    },
    {
      "epoch": 17.120975609756098,
      "eval_loss": 1.2981172800064087,
      "eval_perplexity": 3.6623950004577637,
      "eval_runtime": 46.8564,
      "eval_samples_per_second": 42.321,
      "eval_steps_per_second": 5.293,
      "step": 4400
    },
    {
      "epoch": 17.31609756097561,
      "grad_norm": 10.897383689880371,
      "learning_rate": 1.1540856031128405e-06,
      "loss": 83.2519,
      "step": 4450
    },
    {
      "epoch": 17.511219512195122,
      "grad_norm": 8.18626594543457,
      "learning_rate": 1.1670557717250324e-06,
      "loss": 83.1927,
      "step": 4500
    },
    {
      "epoch": 17.706341463414635,
      "grad_norm": 8.43591594696045,
      "learning_rate": 1.1800259403372246e-06,
      "loss": 83.1903,
      "step": 4550
    },
    {
      "epoch": 17.901463414634147,
      "grad_norm": 7.243773937225342,
      "learning_rate": 1.1929961089494165e-06,
      "loss": 83.108,
      "step": 4600
    },
    {
      "epoch": 17.901463414634147,
      "eval_loss": 1.2963671684265137,
      "eval_perplexity": 3.6559908390045166,
      "eval_runtime": 46.4367,
      "eval_samples_per_second": 42.703,
      "eval_steps_per_second": 5.341,
      "step": 4600
    },
    {
      "epoch": 18.093658536585366,
      "grad_norm": 7.964147090911865,
      "learning_rate": 1.2059662775616084e-06,
      "loss": 81.8778,
      "step": 4650
    },
    {
      "epoch": 18.28878048780488,
      "grad_norm": 9.85496711730957,
      "learning_rate": 1.2189364461738003e-06,
      "loss": 83.084,
      "step": 4700
    },
    {
      "epoch": 18.48390243902439,
      "grad_norm": 8.569170951843262,
      "learning_rate": 1.2319066147859922e-06,
      "loss": 83.0897,
      "step": 4750
    },
    {
      "epoch": 18.679024390243903,
      "grad_norm": 4.829867362976074,
      "learning_rate": 1.2448767833981841e-06,
      "loss": 83.1224,
      "step": 4800
    },
    {
      "epoch": 18.679024390243903,
      "eval_loss": 1.295111060142517,
      "eval_perplexity": 3.6514015197753906,
      "eval_runtime": 46.8314,
      "eval_samples_per_second": 42.343,
      "eval_steps_per_second": 5.296,
      "step": 4800
    },
    {
      "epoch": 18.874146341463415,
      "grad_norm": 5.661312580108643,
      "learning_rate": 1.257846952010376e-06,
      "loss": 83.0098,
      "step": 4850
    },
    {
      "epoch": 19.066341463414634,
      "grad_norm": 11.196198463439941,
      "learning_rate": 1.270817120622568e-06,
      "loss": 81.7335,
      "step": 4900
    },
    {
      "epoch": 19.261463414634147,
      "grad_norm": 4.4880452156066895,
      "learning_rate": 1.2837872892347603e-06,
      "loss": 82.9421,
      "step": 4950
    },
    {
      "epoch": 19.45658536585366,
      "grad_norm": 4.932310581207275,
      "learning_rate": 1.2967574578469522e-06,
      "loss": 82.9784,
      "step": 5000
    },
    {
      "epoch": 19.45658536585366,
      "eval_loss": 1.294049859046936,
      "eval_perplexity": 3.647528648376465,
      "eval_runtime": 46.3751,
      "eval_samples_per_second": 42.76,
      "eval_steps_per_second": 5.348,
      "step": 5000
    },
    {
      "epoch": 19.65170731707317,
      "grad_norm": 7.899780750274658,
      "learning_rate": 1.3097276264591441e-06,
      "loss": 83.0995,
      "step": 5050
    },
    {
      "epoch": 19.846829268292684,
      "grad_norm": 8.462109565734863,
      "learning_rate": 1.322697795071336e-06,
      "loss": 82.9343,
      "step": 5100
    },
    {
      "epoch": 20.039024390243902,
      "grad_norm": 5.395748138427734,
      "learning_rate": 1.335667963683528e-06,
      "loss": 81.6897,
      "step": 5150
    },
    {
      "epoch": 20.234146341463415,
      "grad_norm": 5.493910312652588,
      "learning_rate": 1.3486381322957199e-06,
      "loss": 82.8684,
      "step": 5200
    },
    {
      "epoch": 20.234146341463415,
      "eval_loss": 1.2930622100830078,
      "eval_perplexity": 3.643928050994873,
      "eval_runtime": 46.5409,
      "eval_samples_per_second": 42.608,
      "eval_steps_per_second": 5.329,
      "step": 5200
    },
    {
      "epoch": 20.429268292682927,
      "grad_norm": 9.999631881713867,
      "learning_rate": 1.361608300907912e-06,
      "loss": 82.8979,
      "step": 5250
    },
    {
      "epoch": 20.62439024390244,
      "grad_norm": 6.214622497558594,
      "learning_rate": 1.374578469520104e-06,
      "loss": 82.8567,
      "step": 5300
    },
    {
      "epoch": 20.819512195121952,
      "grad_norm": 6.904379844665527,
      "learning_rate": 1.3875486381322958e-06,
      "loss": 82.8986,
      "step": 5350
    },
    {
      "epoch": 21.01170731707317,
      "grad_norm": 8.023032188415527,
      "learning_rate": 1.4005188067444878e-06,
      "loss": 81.7075,
      "step": 5400
    },
    {
      "epoch": 21.01170731707317,
      "eval_loss": 1.2923247814178467,
      "eval_perplexity": 3.6412417888641357,
      "eval_runtime": 46.9045,
      "eval_samples_per_second": 42.277,
      "eval_steps_per_second": 5.287,
      "step": 5400
    },
    {
      "epoch": 21.206829268292683,
      "grad_norm": 6.122704982757568,
      "learning_rate": 1.4134889753566797e-06,
      "loss": 82.839,
      "step": 5450
    },
    {
      "epoch": 21.401951219512195,
      "grad_norm": 10.71738052368164,
      "learning_rate": 1.4264591439688716e-06,
      "loss": 82.8074,
      "step": 5500
    },
    {
      "epoch": 21.597073170731708,
      "grad_norm": 8.736265182495117,
      "learning_rate": 1.4394293125810637e-06,
      "loss": 82.8625,
      "step": 5550
    },
    {
      "epoch": 21.79219512195122,
      "grad_norm": 8.737895011901855,
      "learning_rate": 1.4523994811932556e-06,
      "loss": 82.8713,
      "step": 5600
    },
    {
      "epoch": 21.79219512195122,
      "eval_loss": 1.2916216850280762,
      "eval_perplexity": 3.6386826038360596,
      "eval_runtime": 46.6705,
      "eval_samples_per_second": 42.489,
      "eval_steps_per_second": 5.314,
      "step": 5600
    },
    {
      "epoch": 21.987317073170733,
      "grad_norm": 6.189862251281738,
      "learning_rate": 1.4653696498054475e-06,
      "loss": 82.8137,
      "step": 5650
    },
    {
      "epoch": 22.17951219512195,
      "grad_norm": 7.030555248260498,
      "learning_rate": 1.4783398184176395e-06,
      "loss": 81.6148,
      "step": 5700
    },
    {
      "epoch": 22.374634146341464,
      "grad_norm": 8.850667953491211,
      "learning_rate": 1.4913099870298314e-06,
      "loss": 82.7685,
      "step": 5750
    },
    {
      "epoch": 22.569756097560976,
      "grad_norm": 5.857985496520996,
      "learning_rate": 1.5042801556420233e-06,
      "loss": 82.765,
      "step": 5800
    },
    {
      "epoch": 22.569756097560976,
      "eval_loss": 1.2909449338912964,
      "eval_perplexity": 3.636220932006836,
      "eval_runtime": 47.6111,
      "eval_samples_per_second": 41.65,
      "eval_steps_per_second": 5.209,
      "step": 5800
    },
    {
      "epoch": 22.76487804878049,
      "grad_norm": 8.174302101135254,
      "learning_rate": 1.5172503242542156e-06,
      "loss": 82.7889,
      "step": 5850
    },
    {
      "epoch": 22.96,
      "grad_norm": 5.131411075592041,
      "learning_rate": 1.5302204928664075e-06,
      "loss": 82.7555,
      "step": 5900
    },
    {
      "epoch": 23.15219512195122,
      "grad_norm": 6.882120132446289,
      "learning_rate": 1.5431906614785995e-06,
      "loss": 81.519,
      "step": 5950
    },
    {
      "epoch": 23.347317073170732,
      "grad_norm": 8.782923698425293,
      "learning_rate": 1.5561608300907914e-06,
      "loss": 82.7517,
      "step": 6000
    },
    {
      "epoch": 23.347317073170732,
      "eval_loss": 1.290360927581787,
      "eval_perplexity": 3.6340980529785156,
      "eval_runtime": 46.5149,
      "eval_samples_per_second": 42.631,
      "eval_steps_per_second": 5.332,
      "step": 6000
    },
    {
      "epoch": 23.542439024390244,
      "grad_norm": 7.806629657745361,
      "learning_rate": 1.5691309987029833e-06,
      "loss": 82.6401,
      "step": 6050
    },
    {
      "epoch": 23.737560975609757,
      "grad_norm": 6.867400169372559,
      "learning_rate": 1.5821011673151752e-06,
      "loss": 82.7502,
      "step": 6100
    },
    {
      "epoch": 23.93268292682927,
      "grad_norm": 6.350253105163574,
      "learning_rate": 1.5950713359273673e-06,
      "loss": 82.744,
      "step": 6150
    },
    {
      "epoch": 24.124878048780488,
      "grad_norm": 6.586564540863037,
      "learning_rate": 1.6080415045395592e-06,
      "loss": 81.3605,
      "step": 6200
    },
    {
      "epoch": 24.124878048780488,
      "eval_loss": 1.2896921634674072,
      "eval_perplexity": 3.6316683292388916,
      "eval_runtime": 46.4845,
      "eval_samples_per_second": 42.659,
      "eval_steps_per_second": 5.335,
      "step": 6200
    },
    {
      "epoch": 24.32,
      "grad_norm": 5.722955703735352,
      "learning_rate": 1.6210116731517512e-06,
      "loss": 82.6341,
      "step": 6250
    },
    {
      "epoch": 24.515121951219513,
      "grad_norm": 3.9386165142059326,
      "learning_rate": 1.633981841763943e-06,
      "loss": 82.7041,
      "step": 6300
    },
    {
      "epoch": 24.710243902439025,
      "grad_norm": 4.977619647979736,
      "learning_rate": 1.646952010376135e-06,
      "loss": 82.6303,
      "step": 6350
    },
    {
      "epoch": 24.905365853658537,
      "grad_norm": 6.084348201751709,
      "learning_rate": 1.659922178988327e-06,
      "loss": 82.8269,
      "step": 6400
    },
    {
      "epoch": 24.905365853658537,
      "eval_loss": 1.289063811302185,
      "eval_perplexity": 3.629387140274048,
      "eval_runtime": 47.4925,
      "eval_samples_per_second": 41.754,
      "eval_steps_per_second": 5.222,
      "step": 6400
    },
    {
      "epoch": 25.097560975609756,
      "grad_norm": 6.545688629150391,
      "learning_rate": 1.672892347600519e-06,
      "loss": 81.4182,
      "step": 6450
    },
    {
      "epoch": 25.29268292682927,
      "grad_norm": 7.9983720779418945,
      "learning_rate": 1.685862516212711e-06,
      "loss": 82.6104,
      "step": 6500
    },
    {
      "epoch": 25.48780487804878,
      "grad_norm": 5.627593040466309,
      "learning_rate": 1.6988326848249029e-06,
      "loss": 82.6909,
      "step": 6550
    },
    {
      "epoch": 25.682926829268293,
      "grad_norm": 9.477391242980957,
      "learning_rate": 1.7118028534370948e-06,
      "loss": 82.612,
      "step": 6600
    },
    {
      "epoch": 25.682926829268293,
      "eval_loss": 1.2885241508483887,
      "eval_perplexity": 3.6274290084838867,
      "eval_runtime": 46.5085,
      "eval_samples_per_second": 42.637,
      "eval_steps_per_second": 5.332,
      "step": 6600
    },
    {
      "epoch": 25.878048780487806,
      "grad_norm": 5.31033992767334,
      "learning_rate": 1.7247730220492867e-06,
      "loss": 82.5657,
      "step": 6650
    },
    {
      "epoch": 26.070243902439024,
      "grad_norm": 6.507229328155518,
      "learning_rate": 1.7377431906614786e-06,
      "loss": 81.3469,
      "step": 6700
    },
    {
      "epoch": 26.265365853658537,
      "grad_norm": 4.360378265380859,
      "learning_rate": 1.7507133592736707e-06,
      "loss": 82.5699,
      "step": 6750
    },
    {
      "epoch": 26.46048780487805,
      "grad_norm": 6.009718418121338,
      "learning_rate": 1.7636835278858626e-06,
      "loss": 82.6632,
      "step": 6800
    },
    {
      "epoch": 26.46048780487805,
      "eval_loss": 1.2880187034606934,
      "eval_perplexity": 3.625596046447754,
      "eval_runtime": 46.6226,
      "eval_samples_per_second": 42.533,
      "eval_steps_per_second": 5.319,
      "step": 6800
    },
    {
      "epoch": 26.65560975609756,
      "grad_norm": 8.402948379516602,
      "learning_rate": 1.7766536964980546e-06,
      "loss": 82.6008,
      "step": 6850
    },
    {
      "epoch": 26.850731707317074,
      "grad_norm": 6.664887428283691,
      "learning_rate": 1.7896238651102465e-06,
      "loss": 82.4734,
      "step": 6900
    },
    {
      "epoch": 27.042926829268293,
      "grad_norm": 7.9240498542785645,
      "learning_rate": 1.8025940337224384e-06,
      "loss": 81.2922,
      "step": 6950
    },
    {
      "epoch": 27.238048780487805,
      "grad_norm": 6.801504611968994,
      "learning_rate": 1.8155642023346303e-06,
      "loss": 82.5363,
      "step": 7000
    },
    {
      "epoch": 27.238048780487805,
      "eval_loss": 1.2875317335128784,
      "eval_perplexity": 3.623831033706665,
      "eval_runtime": 47.3072,
      "eval_samples_per_second": 41.918,
      "eval_steps_per_second": 5.242,
      "step": 7000
    },
    {
      "epoch": 27.433170731707317,
      "grad_norm": 5.0604753494262695,
      "learning_rate": 1.8285343709468226e-06,
      "loss": 82.6394,
      "step": 7050
    },
    {
      "epoch": 27.62829268292683,
      "grad_norm": 5.91637659072876,
      "learning_rate": 1.8415045395590146e-06,
      "loss": 82.5796,
      "step": 7100
    },
    {
      "epoch": 27.823414634146342,
      "grad_norm": 8.112119674682617,
      "learning_rate": 1.8544747081712065e-06,
      "loss": 82.4316,
      "step": 7150
    },
    {
      "epoch": 28.01560975609756,
      "grad_norm": 5.506821632385254,
      "learning_rate": 1.8674448767833984e-06,
      "loss": 81.2973,
      "step": 7200
    },
    {
      "epoch": 28.01560975609756,
      "eval_loss": 1.2870211601257324,
      "eval_perplexity": 3.621981143951416,
      "eval_runtime": 47.3994,
      "eval_samples_per_second": 41.836,
      "eval_steps_per_second": 5.232,
      "step": 7200
    },
    {
      "epoch": 28.210731707317073,
      "grad_norm": 8.071704864501953,
      "learning_rate": 1.8804150453955903e-06,
      "loss": 82.5349,
      "step": 7250
    },
    {
      "epoch": 28.405853658536586,
      "grad_norm": 5.975597381591797,
      "learning_rate": 1.8933852140077822e-06,
      "loss": 82.5233,
      "step": 7300
    },
    {
      "epoch": 28.600975609756098,
      "grad_norm": 4.912400245666504,
      "learning_rate": 1.9063553826199743e-06,
      "loss": 82.4776,
      "step": 7350
    },
    {
      "epoch": 28.79609756097561,
      "grad_norm": 8.339591026306152,
      "learning_rate": 1.9193255512321663e-06,
      "loss": 82.3677,
      "step": 7400
    },
    {
      "epoch": 28.79609756097561,
      "eval_loss": 1.2865544557571411,
      "eval_perplexity": 3.620291233062744,
      "eval_runtime": 46.6999,
      "eval_samples_per_second": 42.463,
      "eval_steps_per_second": 5.311,
      "step": 7400
    },
    {
      "epoch": 28.991219512195123,
      "grad_norm": 6.257800579071045,
      "learning_rate": 1.932295719844358e-06,
      "loss": 82.5612,
      "step": 7450
    },
    {
      "epoch": 29.18341463414634,
      "grad_norm": 7.392521858215332,
      "learning_rate": 1.94526588845655e-06,
      "loss": 81.2408,
      "step": 7500
    },
    {
      "epoch": 29.378536585365854,
      "grad_norm": 4.6816935539245605,
      "learning_rate": 1.9582360570687418e-06,
      "loss": 82.3998,
      "step": 7550
    },
    {
      "epoch": 29.573658536585366,
      "grad_norm": 6.472294330596924,
      "learning_rate": 1.971206225680934e-06,
      "loss": 82.4126,
      "step": 7600
    },
    {
      "epoch": 29.573658536585366,
      "eval_loss": 1.2861188650131226,
      "eval_perplexity": 3.6187145709991455,
      "eval_runtime": 46.7489,
      "eval_samples_per_second": 42.418,
      "eval_steps_per_second": 5.305,
      "step": 7600
    },
    {
      "epoch": 29.76878048780488,
      "grad_norm": 6.398114204406738,
      "learning_rate": 1.984176394293126e-06,
      "loss": 82.511,
      "step": 7650
    },
    {
      "epoch": 29.96390243902439,
      "grad_norm": 5.75391149520874,
      "learning_rate": 1.997146562905318e-06,
      "loss": 82.4443,
      "step": 7700
    },
    {
      "epoch": 30.15609756097561,
      "grad_norm": 7.423117160797119,
      "learning_rate": 2.01011673151751e-06,
      "loss": 81.1121,
      "step": 7750
    },
    {
      "epoch": 30.351219512195122,
      "grad_norm": 11.260043144226074,
      "learning_rate": 2.023086900129702e-06,
      "loss": 82.3812,
      "step": 7800
    },
    {
      "epoch": 30.351219512195122,
      "eval_loss": 1.285657286643982,
      "eval_perplexity": 3.617044687271118,
      "eval_runtime": 46.2633,
      "eval_samples_per_second": 42.863,
      "eval_steps_per_second": 5.361,
      "step": 7800
    },
    {
      "epoch": 30.546341463414635,
      "grad_norm": 7.854236125946045,
      "learning_rate": 2.0360570687418937e-06,
      "loss": 82.4739,
      "step": 7850
    },
    {
      "epoch": 30.741463414634147,
      "grad_norm": 11.212300300598145,
      "learning_rate": 2.049027237354086e-06,
      "loss": 82.4151,
      "step": 7900
    },
    {
      "epoch": 30.93658536585366,
      "grad_norm": 12.16933536529541,
      "learning_rate": 2.061997405966278e-06,
      "loss": 82.4224,
      "step": 7950
    },
    {
      "epoch": 31.128780487804878,
      "grad_norm": 11.715889930725098,
      "learning_rate": 2.0749675745784697e-06,
      "loss": 81.1773,
      "step": 8000
    },
    {
      "epoch": 31.128780487804878,
      "eval_loss": 1.2852119207382202,
      "eval_perplexity": 3.615434169769287,
      "eval_runtime": 48.463,
      "eval_samples_per_second": 40.918,
      "eval_steps_per_second": 5.117,
      "step": 8000
    },
    {
      "epoch": 31.32390243902439,
      "grad_norm": 7.306983947753906,
      "learning_rate": 2.0879377431906618e-06,
      "loss": 82.4286,
      "step": 8050
    },
    {
      "epoch": 31.519024390243903,
      "grad_norm": 6.068855285644531,
      "learning_rate": 2.1009079118028535e-06,
      "loss": 82.3974,
      "step": 8100
    },
    {
      "epoch": 31.714146341463415,
      "grad_norm": 7.560631275177002,
      "learning_rate": 2.1138780804150456e-06,
      "loss": 82.3592,
      "step": 8150
    },
    {
      "epoch": 31.909268292682928,
      "grad_norm": 11.395291328430176,
      "learning_rate": 2.1268482490272373e-06,
      "loss": 82.2936,
      "step": 8200
    },
    {
      "epoch": 31.909268292682928,
      "eval_loss": 1.2846966981887817,
      "eval_perplexity": 3.613571882247925,
      "eval_runtime": 46.9564,
      "eval_samples_per_second": 42.231,
      "eval_steps_per_second": 5.281,
      "step": 8200
    },
    {
      "epoch": 32.101463414634146,
      "grad_norm": 6.736449241638184,
      "learning_rate": 2.1398184176394294e-06,
      "loss": 81.0567,
      "step": 8250
    },
    {
      "epoch": 32.29658536585366,
      "grad_norm": 7.983084201812744,
      "learning_rate": 2.1527885862516216e-06,
      "loss": 82.2463,
      "step": 8300
    },
    {
      "epoch": 32.49170731707317,
      "grad_norm": 5.664475917816162,
      "learning_rate": 2.1657587548638133e-06,
      "loss": 82.2862,
      "step": 8350
    },
    {
      "epoch": 32.68682926829268,
      "grad_norm": 10.956154823303223,
      "learning_rate": 2.1787289234760054e-06,
      "loss": 82.3287,
      "step": 8400
    },
    {
      "epoch": 32.68682926829268,
      "eval_loss": 1.2841408252716064,
      "eval_perplexity": 3.6115636825561523,
      "eval_runtime": 46.9526,
      "eval_samples_per_second": 42.234,
      "eval_steps_per_second": 5.282,
      "step": 8400
    },
    {
      "epoch": 32.881951219512196,
      "grad_norm": 7.619876861572266,
      "learning_rate": 2.191699092088197e-06,
      "loss": 82.3431,
      "step": 8450
    },
    {
      "epoch": 33.07414634146341,
      "grad_norm": 15.04808235168457,
      "learning_rate": 2.2046692607003892e-06,
      "loss": 81.1055,
      "step": 8500
    },
    {
      "epoch": 33.26926829268292,
      "grad_norm": 7.056735515594482,
      "learning_rate": 2.2176394293125814e-06,
      "loss": 82.2555,
      "step": 8550
    },
    {
      "epoch": 33.464390243902436,
      "grad_norm": 8.435641288757324,
      "learning_rate": 2.2306095979247735e-06,
      "loss": 82.313,
      "step": 8600
    },
    {
      "epoch": 33.464390243902436,
      "eval_loss": 1.283582091331482,
      "eval_perplexity": 3.609546422958374,
      "eval_runtime": 46.6172,
      "eval_samples_per_second": 42.538,
      "eval_steps_per_second": 5.32,
      "step": 8600
    },
    {
      "epoch": 33.65951219512195,
      "grad_norm": 7.362847328186035,
      "learning_rate": 2.243579766536965e-06,
      "loss": 82.2409,
      "step": 8650
    },
    {
      "epoch": 33.85463414634146,
      "grad_norm": 7.0938310623168945,
      "learning_rate": 2.2565499351491573e-06,
      "loss": 82.2133,
      "step": 8700
    },
    {
      "epoch": 34.04682926829268,
      "grad_norm": 6.569704532623291,
      "learning_rate": 2.269520103761349e-06,
      "loss": 81.0095,
      "step": 8750
    },
    {
      "epoch": 34.241951219512195,
      "grad_norm": 9.857244491577148,
      "learning_rate": 2.2824902723735407e-06,
      "loss": 82.2172,
      "step": 8800
    },
    {
      "epoch": 34.241951219512195,
      "eval_loss": 1.2829787731170654,
      "eval_perplexity": 3.6073691844940186,
      "eval_runtime": 46.683,
      "eval_samples_per_second": 42.478,
      "eval_steps_per_second": 5.312,
      "step": 8800
    },
    {
      "epoch": 34.43707317073171,
      "grad_norm": 11.174607276916504,
      "learning_rate": 2.2954604409857333e-06,
      "loss": 82.2028,
      "step": 8850
    },
    {
      "epoch": 34.63219512195122,
      "grad_norm": 8.239313125610352,
      "learning_rate": 2.308430609597925e-06,
      "loss": 82.1855,
      "step": 8900
    },
    {
      "epoch": 34.82731707317073,
      "grad_norm": 10.68331241607666,
      "learning_rate": 2.321400778210117e-06,
      "loss": 82.2689,
      "step": 8950
    },
    {
      "epoch": 35.01951219512195,
      "grad_norm": 8.158864974975586,
      "learning_rate": 2.334370946822309e-06,
      "loss": 80.8936,
      "step": 9000
    },
    {
      "epoch": 35.01951219512195,
      "eval_loss": 1.2822238206863403,
      "eval_perplexity": 3.604646921157837,
      "eval_runtime": 46.501,
      "eval_samples_per_second": 42.644,
      "eval_steps_per_second": 5.333,
      "step": 9000
    },
    {
      "epoch": 35.21463414634146,
      "grad_norm": 10.145269393920898,
      "learning_rate": 2.347341115434501e-06,
      "loss": 82.1431,
      "step": 9050
    },
    {
      "epoch": 35.40975609756097,
      "grad_norm": 21.176401138305664,
      "learning_rate": 2.3603112840466926e-06,
      "loss": 82.2306,
      "step": 9100
    },
    {
      "epoch": 35.604878048780485,
      "grad_norm": 12.092711448669434,
      "learning_rate": 2.3732814526588848e-06,
      "loss": 82.1439,
      "step": 9150
    },
    {
      "epoch": 35.8,
      "grad_norm": 10.36771297454834,
      "learning_rate": 2.386251621271077e-06,
      "loss": 82.1482,
      "step": 9200
    },
    {
      "epoch": 35.8,
      "eval_loss": 1.2813822031021118,
      "eval_perplexity": 3.601614475250244,
      "eval_runtime": 46.8436,
      "eval_samples_per_second": 42.332,
      "eval_steps_per_second": 5.294,
      "step": 9200
    },
    {
      "epoch": 35.99512195121951,
      "grad_norm": 12.070416450500488,
      "learning_rate": 2.3992217898832686e-06,
      "loss": 81.9834,
      "step": 9250
    },
    {
      "epoch": 36.18731707317073,
      "grad_norm": 14.370905876159668,
      "learning_rate": 2.4121919584954607e-06,
      "loss": 80.8861,
      "step": 9300
    },
    {
      "epoch": 36.382439024390244,
      "grad_norm": 10.84345531463623,
      "learning_rate": 2.4251621271076524e-06,
      "loss": 82.0289,
      "step": 9350
    },
    {
      "epoch": 36.57756097560976,
      "grad_norm": 20.74272346496582,
      "learning_rate": 2.4381322957198445e-06,
      "loss": 81.9656,
      "step": 9400
    },
    {
      "epoch": 36.57756097560976,
      "eval_loss": 1.2807178497314453,
      "eval_perplexity": 3.599222421646118,
      "eval_runtime": 46.5699,
      "eval_samples_per_second": 42.581,
      "eval_steps_per_second": 5.325,
      "step": 9400
    },
    {
      "epoch": 36.77268292682927,
      "grad_norm": 15.841682434082031,
      "learning_rate": 2.4511024643320367e-06,
      "loss": 82.0516,
      "step": 9450
    },
    {
      "epoch": 36.96780487804878,
      "grad_norm": 9.845006942749023,
      "learning_rate": 2.4640726329442284e-06,
      "loss": 82.1058,
      "step": 9500
    },
    {
      "epoch": 37.16,
      "grad_norm": 12.61862564086914,
      "learning_rate": 2.4770428015564205e-06,
      "loss": 80.9314,
      "step": 9550
    },
    {
      "epoch": 37.35512195121951,
      "grad_norm": 10.7813081741333,
      "learning_rate": 2.490012970168612e-06,
      "loss": 81.8963,
      "step": 9600
    },
    {
      "epoch": 37.35512195121951,
      "eval_loss": 1.2799220085144043,
      "eval_perplexity": 3.5963592529296875,
      "eval_runtime": 46.9092,
      "eval_samples_per_second": 42.273,
      "eval_steps_per_second": 5.287,
      "step": 9600
    },
    {
      "epoch": 37.55024390243902,
      "grad_norm": 7.879450798034668,
      "learning_rate": 2.5029831387808047e-06,
      "loss": 82.0107,
      "step": 9650
    },
    {
      "epoch": 37.745365853658534,
      "grad_norm": 9.317995071411133,
      "learning_rate": 2.5159533073929965e-06,
      "loss": 81.937,
      "step": 9700
    },
    {
      "epoch": 37.940487804878046,
      "grad_norm": 11.960807800292969,
      "learning_rate": 2.5289234760051886e-06,
      "loss": 82.0126,
      "step": 9750
    },
    {
      "epoch": 38.13268292682927,
      "grad_norm": 9.198863983154297,
      "learning_rate": 2.5418936446173803e-06,
      "loss": 80.5207,
      "step": 9800
    },
    {
      "epoch": 38.13268292682927,
      "eval_loss": 1.2795848846435547,
      "eval_perplexity": 3.595146894454956,
      "eval_runtime": 46.6044,
      "eval_samples_per_second": 42.55,
      "eval_steps_per_second": 5.321,
      "step": 9800
    },
    {
      "epoch": 38.32780487804878,
      "grad_norm": 18.611108779907227,
      "learning_rate": 2.5548638132295724e-06,
      "loss": 81.9928,
      "step": 9850
    },
    {
      "epoch": 38.52292682926829,
      "grad_norm": 10.099822998046875,
      "learning_rate": 2.567833981841764e-06,
      "loss": 81.9924,
      "step": 9900
    },
    {
      "epoch": 38.718048780487806,
      "grad_norm": 15.344515800476074,
      "learning_rate": 2.5808041504539562e-06,
      "loss": 81.9273,
      "step": 9950
    },
    {
      "epoch": 38.91317073170732,
      "grad_norm": 18.16099739074707,
      "learning_rate": 2.593774319066148e-06,
      "loss": 81.8875,
      "step": 10000
    },
    {
      "epoch": 38.91317073170732,
      "eval_loss": 1.2787160873413086,
      "eval_perplexity": 3.592024803161621,
      "eval_runtime": 46.3922,
      "eval_samples_per_second": 42.744,
      "eval_steps_per_second": 5.346,
      "step": 10000
    },
    {
      "epoch": 39.10536585365853,
      "grad_norm": 24.73385238647461,
      "learning_rate": 2.60674448767834e-06,
      "loss": 80.6306,
      "step": 10050
    },
    {
      "epoch": 39.300487804878045,
      "grad_norm": 9.88043212890625,
      "learning_rate": 2.6197146562905318e-06,
      "loss": 82.0354,
      "step": 10100
    },
    {
      "epoch": 39.49560975609756,
      "grad_norm": 14.759455680847168,
      "learning_rate": 2.632684824902724e-06,
      "loss": 81.6516,
      "step": 10150
    },
    {
      "epoch": 39.69073170731707,
      "grad_norm": 14.209807395935059,
      "learning_rate": 2.645654993514916e-06,
      "loss": 81.7336,
      "step": 10200
    },
    {
      "epoch": 39.69073170731707,
      "eval_loss": 1.2778698205947876,
      "eval_perplexity": 3.588986396789551,
      "eval_runtime": 46.263,
      "eval_samples_per_second": 42.864,
      "eval_steps_per_second": 5.361,
      "step": 10200
    },
    {
      "epoch": 39.88585365853658,
      "grad_norm": 18.374351501464844,
      "learning_rate": 2.658625162127108e-06,
      "loss": 81.9684,
      "step": 10250
    },
    {
      "epoch": 40.078048780487805,
      "grad_norm": 14.319310188293457,
      "learning_rate": 2.6715953307393e-06,
      "loss": 80.7043,
      "step": 10300
    },
    {
      "epoch": 40.27317073170732,
      "grad_norm": 23.40023422241211,
      "learning_rate": 2.684565499351492e-06,
      "loss": 81.8641,
      "step": 10350
    },
    {
      "epoch": 40.46829268292683,
      "grad_norm": 18.167898178100586,
      "learning_rate": 2.6975356679636837e-06,
      "loss": 81.8502,
      "step": 10400
    },
    {
      "epoch": 40.46829268292683,
      "eval_loss": 1.2773650884628296,
      "eval_perplexity": 3.5871753692626953,
      "eval_runtime": 46.0506,
      "eval_samples_per_second": 43.061,
      "eval_steps_per_second": 5.385,
      "step": 10400
    },
    {
      "epoch": 40.66341463414634,
      "grad_norm": 19.793737411499023,
      "learning_rate": 2.710505836575876e-06,
      "loss": 81.8622,
      "step": 10450
    },
    {
      "epoch": 40.858536585365854,
      "grad_norm": 16.031322479248047,
      "learning_rate": 2.7234760051880675e-06,
      "loss": 81.6872,
      "step": 10500
    },
    {
      "epoch": 41.05073170731707,
      "grad_norm": 12.416406631469727,
      "learning_rate": 2.7364461738002596e-06,
      "loss": 80.4387,
      "step": 10550
    },
    {
      "epoch": 41.24585365853658,
      "grad_norm": 10.888899803161621,
      "learning_rate": 2.7494163424124513e-06,
      "loss": 81.7322,
      "step": 10600
    },
    {
      "epoch": 41.24585365853658,
      "eval_loss": 1.2767505645751953,
      "eval_perplexity": 3.5849716663360596,
      "eval_runtime": 45.8952,
      "eval_samples_per_second": 43.207,
      "eval_steps_per_second": 5.404,
      "step": 10600
    },
    {
      "epoch": 41.440975609756094,
      "grad_norm": 11.243260383605957,
      "learning_rate": 2.7623865110246435e-06,
      "loss": 81.8093,
      "step": 10650
    },
    {
      "epoch": 41.63609756097561,
      "grad_norm": 20.834047317504883,
      "learning_rate": 2.775356679636835e-06,
      "loss": 81.59,
      "step": 10700
    },
    {
      "epoch": 41.83121951219512,
      "grad_norm": 11.886606216430664,
      "learning_rate": 2.7883268482490273e-06,
      "loss": 81.816,
      "step": 10750
    },
    {
      "epoch": 42.02341463414634,
      "grad_norm": 19.417530059814453,
      "learning_rate": 2.80129701686122e-06,
      "loss": 80.4539,
      "step": 10800
    },
    {
      "epoch": 42.02341463414634,
      "eval_loss": 1.2762917280197144,
      "eval_perplexity": 3.583327054977417,
      "eval_runtime": 46.4541,
      "eval_samples_per_second": 42.687,
      "eval_steps_per_second": 5.339,
      "step": 10800
    },
    {
      "epoch": 42.218536585365854,
      "grad_norm": 18.27802276611328,
      "learning_rate": 2.8142671854734116e-06,
      "loss": 81.6554,
      "step": 10850
    },
    {
      "epoch": 42.413658536585366,
      "grad_norm": 22.110504150390625,
      "learning_rate": 2.8272373540856037e-06,
      "loss": 81.7258,
      "step": 10900
    },
    {
      "epoch": 42.60878048780488,
      "grad_norm": 22.007007598876953,
      "learning_rate": 2.8402075226977954e-06,
      "loss": 81.6253,
      "step": 10950
    },
    {
      "epoch": 42.80390243902439,
      "grad_norm": 20.845827102661133,
      "learning_rate": 2.8531776913099875e-06,
      "loss": 81.7644,
      "step": 11000
    },
    {
      "epoch": 42.80390243902439,
      "eval_loss": 1.275590181350708,
      "eval_perplexity": 3.5808141231536865,
      "eval_runtime": 46.44,
      "eval_samples_per_second": 42.7,
      "eval_steps_per_second": 5.34,
      "step": 11000
    },
    {
      "epoch": 42.9990243902439,
      "grad_norm": 17.639739990234375,
      "learning_rate": 2.866147859922179e-06,
      "loss": 81.6781,
      "step": 11050
    },
    {
      "epoch": 43.19121951219512,
      "grad_norm": 17.06352996826172,
      "learning_rate": 2.8791180285343713e-06,
      "loss": 80.3111,
      "step": 11100
    },
    {
      "epoch": 43.38634146341463,
      "grad_norm": 23.57854461669922,
      "learning_rate": 2.892088197146563e-06,
      "loss": 81.6029,
      "step": 11150
    },
    {
      "epoch": 43.58146341463414,
      "grad_norm": 23.455123901367188,
      "learning_rate": 2.905058365758755e-06,
      "loss": 81.6493,
      "step": 11200
    },
    {
      "epoch": 43.58146341463414,
      "eval_loss": 1.2750275135040283,
      "eval_perplexity": 3.5787999629974365,
      "eval_runtime": 46.4891,
      "eval_samples_per_second": 42.655,
      "eval_steps_per_second": 5.335,
      "step": 11200
    },
    {
      "epoch": 43.776585365853656,
      "grad_norm": 43.299381256103516,
      "learning_rate": 2.918028534370947e-06,
      "loss": 81.5879,
      "step": 11250
    },
    {
      "epoch": 43.97170731707317,
      "grad_norm": 18.457134246826172,
      "learning_rate": 2.930998702983139e-06,
      "loss": 81.6561,
      "step": 11300
    },
    {
      "epoch": 44.16390243902439,
      "grad_norm": 25.69742202758789,
      "learning_rate": 2.9439688715953307e-06,
      "loss": 80.3525,
      "step": 11350
    },
    {
      "epoch": 44.3590243902439,
      "grad_norm": 14.845590591430664,
      "learning_rate": 2.9569390402075232e-06,
      "loss": 81.5639,
      "step": 11400
    },
    {
      "epoch": 44.3590243902439,
      "eval_loss": 1.2745603322982788,
      "eval_perplexity": 3.5771284103393555,
      "eval_runtime": 46.6669,
      "eval_samples_per_second": 42.493,
      "eval_steps_per_second": 5.314,
      "step": 11400
    },
    {
      "epoch": 44.554146341463415,
      "grad_norm": 15.410818099975586,
      "learning_rate": 2.969909208819715e-06,
      "loss": 81.5397,
      "step": 11450
    },
    {
      "epoch": 44.74926829268293,
      "grad_norm": 21.196989059448242,
      "learning_rate": 2.982879377431907e-06,
      "loss": 81.5991,
      "step": 11500
    },
    {
      "epoch": 44.94439024390244,
      "grad_norm": 35.03132629394531,
      "learning_rate": 2.9958495460440988e-06,
      "loss": 81.5408,
      "step": 11550
    },
    {
      "epoch": 45.136585365853655,
      "grad_norm": 28.685531616210938,
      "learning_rate": 3.008819714656291e-06,
      "loss": 80.2566,
      "step": 11600
    },
    {
      "epoch": 45.136585365853655,
      "eval_loss": 1.2739524841308594,
      "eval_perplexity": 3.5749545097351074,
      "eval_runtime": 46.4017,
      "eval_samples_per_second": 42.736,
      "eval_steps_per_second": 5.345,
      "step": 11600
    },
    {
      "epoch": 45.33170731707317,
      "grad_norm": 24.748416900634766,
      "learning_rate": 3.0217898832684826e-06,
      "loss": 81.6363,
      "step": 11650
    },
    {
      "epoch": 45.52682926829268,
      "grad_norm": 18.225099563598633,
      "learning_rate": 3.0347600518806747e-06,
      "loss": 81.5151,
      "step": 11700
    },
    {
      "epoch": 45.72195121951219,
      "grad_norm": 25.687816619873047,
      "learning_rate": 3.0477302204928664e-06,
      "loss": 81.4209,
      "step": 11750
    },
    {
      "epoch": 45.917073170731705,
      "grad_norm": 26.970895767211914,
      "learning_rate": 3.0607003891050586e-06,
      "loss": 81.4748,
      "step": 11800
    },
    {
      "epoch": 45.917073170731705,
      "eval_loss": 1.273669719696045,
      "eval_perplexity": 3.573943853378296,
      "eval_runtime": 46.3134,
      "eval_samples_per_second": 42.817,
      "eval_steps_per_second": 5.355,
      "step": 11800
    },
    {
      "epoch": 46.10926829268293,
      "grad_norm": 18.5040283203125,
      "learning_rate": 3.0736705577172503e-06,
      "loss": 80.1373,
      "step": 11850
    },
    {
      "epoch": 46.30439024390244,
      "grad_norm": 17.678571701049805,
      "learning_rate": 3.0866407263294424e-06,
      "loss": 81.5353,
      "step": 11900
    },
    {
      "epoch": 46.49951219512195,
      "grad_norm": 24.62421417236328,
      "learning_rate": 3.099610894941634e-06,
      "loss": 81.5438,
      "step": 11950
    },
    {
      "epoch": 46.694634146341464,
      "grad_norm": 24.771581649780273,
      "learning_rate": 3.1125810635538267e-06,
      "loss": 81.3989,
      "step": 12000
    },
    {
      "epoch": 46.694634146341464,
      "eval_loss": 1.2731049060821533,
      "eval_perplexity": 3.5719258785247803,
      "eval_runtime": 46.6544,
      "eval_samples_per_second": 42.504,
      "eval_steps_per_second": 5.316,
      "step": 12000
    },
    {
      "epoch": 46.889756097560976,
      "grad_norm": 17.56694221496582,
      "learning_rate": 3.1255512321660188e-06,
      "loss": 81.3133,
      "step": 12050
    },
    {
      "epoch": 47.08195121951219,
      "grad_norm": 20.33832359313965,
      "learning_rate": 3.1385214007782105e-06,
      "loss": 80.2467,
      "step": 12100
    },
    {
      "epoch": 47.277073170731704,
      "grad_norm": 31.80974578857422,
      "learning_rate": 3.1514915693904026e-06,
      "loss": 81.4383,
      "step": 12150
    },
    {
      "epoch": 47.472195121951216,
      "grad_norm": 15.991684913635254,
      "learning_rate": 3.1644617380025943e-06,
      "loss": 81.3443,
      "step": 12200
    },
    {
      "epoch": 47.472195121951216,
      "eval_loss": 1.2725409269332886,
      "eval_perplexity": 3.5699119567871094,
      "eval_runtime": 45.9195,
      "eval_samples_per_second": 43.184,
      "eval_steps_per_second": 5.401,
      "step": 12200
    },
    {
      "epoch": 47.66731707317073,
      "grad_norm": 21.94366455078125,
      "learning_rate": 3.1774319066147864e-06,
      "loss": 81.2986,
      "step": 12250
    },
    {
      "epoch": 47.86243902439024,
      "grad_norm": 20.944995880126953,
      "learning_rate": 3.190402075226978e-06,
      "loss": 81.3157,
      "step": 12300
    },
    {
      "epoch": 48.05463414634146,
      "grad_norm": 24.603673934936523,
      "learning_rate": 3.2033722438391703e-06,
      "loss": 80.1341,
      "step": 12350
    },
    {
      "epoch": 48.249756097560976,
      "grad_norm": 21.535818099975586,
      "learning_rate": 3.216342412451362e-06,
      "loss": 81.2218,
      "step": 12400
    },
    {
      "epoch": 48.249756097560976,
      "eval_loss": 1.2721748352050781,
      "eval_perplexity": 3.5686051845550537,
      "eval_runtime": 46.2261,
      "eval_samples_per_second": 42.898,
      "eval_steps_per_second": 5.365,
      "step": 12400
    },
    {
      "epoch": 48.44487804878049,
      "grad_norm": 19.146528244018555,
      "learning_rate": 3.229312581063554e-06,
      "loss": 81.213,
      "step": 12450
    },
    {
      "epoch": 48.64,
      "grad_norm": 28.79192352294922,
      "learning_rate": 3.242282749675746e-06,
      "loss": 81.2611,
      "step": 12500
    },
    {
      "epoch": 48.83512195121951,
      "grad_norm": 18.43653106689453,
      "learning_rate": 3.255252918287938e-06,
      "loss": 81.4571,
      "step": 12550
    },
    {
      "epoch": 49.02731707317073,
      "grad_norm": 38.7769775390625,
      "learning_rate": 3.26822308690013e-06,
      "loss": 80.0801,
      "step": 12600
    },
    {
      "epoch": 49.02731707317073,
      "eval_loss": 1.2714645862579346,
      "eval_perplexity": 3.5660715103149414,
      "eval_runtime": 46.1317,
      "eval_samples_per_second": 42.986,
      "eval_steps_per_second": 5.376,
      "step": 12600
    },
    {
      "epoch": 49.22243902439024,
      "grad_norm": 28.704864501953125,
      "learning_rate": 3.281193255512322e-06,
      "loss": 81.2976,
      "step": 12650
    },
    {
      "epoch": 49.41756097560975,
      "grad_norm": 34.55477523803711,
      "learning_rate": 3.294163424124514e-06,
      "loss": 81.2539,
      "step": 12700
    },
    {
      "epoch": 49.612682926829265,
      "grad_norm": 33.56720733642578,
      "learning_rate": 3.307133592736706e-06,
      "loss": 81.2537,
      "step": 12750
    },
    {
      "epoch": 49.80780487804878,
      "grad_norm": 25.98894691467285,
      "learning_rate": 3.3201037613488977e-06,
      "loss": 81.2323,
      "step": 12800
    },
    {
      "epoch": 49.80780487804878,
      "eval_loss": 1.2708081007003784,
      "eval_perplexity": 3.5637311935424805,
      "eval_runtime": 45.8667,
      "eval_samples_per_second": 43.234,
      "eval_steps_per_second": 5.407,
      "step": 12800
    },
    {
      "epoch": 50.0,
      "grad_norm": 11.941808700561523,
      "learning_rate": 3.33307392996109e-06,
      "loss": 79.938,
      "step": 12850
    },
    {
      "epoch": 50.19512195121951,
      "grad_norm": 23.147912979125977,
      "learning_rate": 3.3460440985732815e-06,
      "loss": 81.1354,
      "step": 12900
    },
    {
      "epoch": 50.390243902439025,
      "grad_norm": 26.439516067504883,
      "learning_rate": 3.3590142671854737e-06,
      "loss": 81.3304,
      "step": 12950
    },
    {
      "epoch": 50.58536585365854,
      "grad_norm": 26.67255401611328,
      "learning_rate": 3.3719844357976654e-06,
      "loss": 81.2213,
      "step": 13000
    },
    {
      "epoch": 50.58536585365854,
      "eval_loss": 1.2702009677886963,
      "eval_perplexity": 3.561568260192871,
      "eval_runtime": 46.1598,
      "eval_samples_per_second": 42.96,
      "eval_steps_per_second": 5.373,
      "step": 13000
    },
    {
      "epoch": 50.78048780487805,
      "grad_norm": 35.04331588745117,
      "learning_rate": 3.3849546044098575e-06,
      "loss": 81.0874,
      "step": 13050
    },
    {
      "epoch": 50.97560975609756,
      "grad_norm": 21.486684799194336,
      "learning_rate": 3.397924773022049e-06,
      "loss": 81.1284,
      "step": 13100
    },
    {
      "epoch": 51.16780487804878,
      "grad_norm": 33.55108642578125,
      "learning_rate": 3.4108949416342413e-06,
      "loss": 79.7304,
      "step": 13150
    },
    {
      "epoch": 51.36292682926829,
      "grad_norm": 28.119997024536133,
      "learning_rate": 3.423865110246434e-06,
      "loss": 81.0161,
      "step": 13200
    },
    {
      "epoch": 51.36292682926829,
      "eval_loss": 1.2697807550430298,
      "eval_perplexity": 3.5600719451904297,
      "eval_runtime": 46.3791,
      "eval_samples_per_second": 42.756,
      "eval_steps_per_second": 5.347,
      "step": 13200
    },
    {
      "epoch": 51.5580487804878,
      "grad_norm": 25.460660934448242,
      "learning_rate": 3.4368352788586256e-06,
      "loss": 81.0799,
      "step": 13250
    },
    {
      "epoch": 51.753170731707314,
      "grad_norm": 57.79084014892578,
      "learning_rate": 3.4498054474708177e-06,
      "loss": 81.0305,
      "step": 13300
    },
    {
      "epoch": 51.94829268292683,
      "grad_norm": 17.795372009277344,
      "learning_rate": 3.4627756160830094e-06,
      "loss": 81.156,
      "step": 13350
    },
    {
      "epoch": 52.14048780487805,
      "grad_norm": 25.139158248901367,
      "learning_rate": 3.4757457846952015e-06,
      "loss": 80.0375,
      "step": 13400
    },
    {
      "epoch": 52.14048780487805,
      "eval_loss": 1.2694849967956543,
      "eval_perplexity": 3.559019088745117,
      "eval_runtime": 46.2505,
      "eval_samples_per_second": 42.875,
      "eval_steps_per_second": 5.362,
      "step": 13400
    },
    {
      "epoch": 52.33560975609756,
      "grad_norm": 22.931726455688477,
      "learning_rate": 3.4887159533073932e-06,
      "loss": 81.0145,
      "step": 13450
    },
    {
      "epoch": 52.530731707317074,
      "grad_norm": 43.32676315307617,
      "learning_rate": 3.5016861219195854e-06,
      "loss": 81.0375,
      "step": 13500
    },
    {
      "epoch": 52.725853658536586,
      "grad_norm": 34.691043853759766,
      "learning_rate": 3.514656290531777e-06,
      "loss": 80.9203,
      "step": 13550
    },
    {
      "epoch": 52.9209756097561,
      "grad_norm": 46.737022399902344,
      "learning_rate": 3.527626459143969e-06,
      "loss": 81.0127,
      "step": 13600
    },
    {
      "epoch": 52.9209756097561,
      "eval_loss": 1.268815040588379,
      "eval_perplexity": 3.556635618209839,
      "eval_runtime": 45.7981,
      "eval_samples_per_second": 43.299,
      "eval_steps_per_second": 5.415,
      "step": 13600
    },
    {
      "epoch": 53.113170731707314,
      "grad_norm": 65.05512237548828,
      "learning_rate": 3.540596627756161e-06,
      "loss": 79.6826,
      "step": 13650
    },
    {
      "epoch": 53.308292682926826,
      "grad_norm": 29.276451110839844,
      "learning_rate": 3.553566796368353e-06,
      "loss": 81.0141,
      "step": 13700
    },
    {
      "epoch": 53.50341463414634,
      "grad_norm": 31.55052947998047,
      "learning_rate": 3.5665369649805447e-06,
      "loss": 81.0657,
      "step": 13750
    },
    {
      "epoch": 53.69853658536585,
      "grad_norm": 27.511171340942383,
      "learning_rate": 3.5795071335927373e-06,
      "loss": 80.8529,
      "step": 13800
    },
    {
      "epoch": 53.69853658536585,
      "eval_loss": 1.268166184425354,
      "eval_perplexity": 3.554328680038452,
      "eval_runtime": 46.0232,
      "eval_samples_per_second": 43.087,
      "eval_steps_per_second": 5.389,
      "step": 13800
    },
    {
      "epoch": 53.89365853658536,
      "grad_norm": 24.61992645263672,
      "learning_rate": 3.592477302204929e-06,
      "loss": 80.9096,
      "step": 13850
    },
    {
      "epoch": 54.085853658536585,
      "grad_norm": 36.46946334838867,
      "learning_rate": 3.605447470817121e-06,
      "loss": 79.6297,
      "step": 13900
    },
    {
      "epoch": 54.2809756097561,
      "grad_norm": 29.217355728149414,
      "learning_rate": 3.618417639429313e-06,
      "loss": 80.7666,
      "step": 13950
    },
    {
      "epoch": 54.47609756097561,
      "grad_norm": 36.222930908203125,
      "learning_rate": 3.631387808041505e-06,
      "loss": 80.9002,
      "step": 14000
    },
    {
      "epoch": 54.47609756097561,
      "eval_loss": 1.2674996852874756,
      "eval_perplexity": 3.5519604682922363,
      "eval_runtime": 46.1126,
      "eval_samples_per_second": 43.003,
      "eval_steps_per_second": 5.378,
      "step": 14000
    },
    {
      "epoch": 54.67121951219512,
      "grad_norm": 18.430397033691406,
      "learning_rate": 3.6443579766536966e-06,
      "loss": 80.9541,
      "step": 14050
    },
    {
      "epoch": 54.866341463414635,
      "grad_norm": 31.30219841003418,
      "learning_rate": 3.6573281452658888e-06,
      "loss": 80.7977,
      "step": 14100
    },
    {
      "epoch": 55.05853658536585,
      "grad_norm": 29.366779327392578,
      "learning_rate": 3.6702983138780805e-06,
      "loss": 79.6467,
      "step": 14150
    },
    {
      "epoch": 55.25365853658536,
      "grad_norm": 44.95875930786133,
      "learning_rate": 3.6832684824902726e-06,
      "loss": 80.7953,
      "step": 14200
    },
    {
      "epoch": 55.25365853658536,
      "eval_loss": 1.2669122219085693,
      "eval_perplexity": 3.5498743057250977,
      "eval_runtime": 46.6089,
      "eval_samples_per_second": 42.545,
      "eval_steps_per_second": 5.321,
      "step": 14200
    },
    {
      "epoch": 55.448780487804875,
      "grad_norm": 40.90207290649414,
      "learning_rate": 3.6962386511024643e-06,
      "loss": 80.9166,
      "step": 14250
    },
    {
      "epoch": 55.64390243902439,
      "grad_norm": 20.830089569091797,
      "learning_rate": 3.7092088197146564e-06,
      "loss": 80.6987,
      "step": 14300
    },
    {
      "epoch": 55.8390243902439,
      "grad_norm": 24.027450561523438,
      "learning_rate": 3.722178988326848e-06,
      "loss": 80.7466,
      "step": 14350
    },
    {
      "epoch": 56.03121951219512,
      "grad_norm": 31.850849151611328,
      "learning_rate": 3.7351491569390407e-06,
      "loss": 79.4873,
      "step": 14400
    },
    {
      "epoch": 56.03121951219512,
      "eval_loss": 1.2662204504013062,
      "eval_perplexity": 3.547419548034668,
      "eval_runtime": 46.4616,
      "eval_samples_per_second": 42.68,
      "eval_steps_per_second": 5.338,
      "step": 14400
    },
    {
      "epoch": 56.226341463414634,
      "grad_norm": 27.239803314208984,
      "learning_rate": 3.748119325551233e-06,
      "loss": 80.8266,
      "step": 14450
    },
    {
      "epoch": 56.42146341463415,
      "grad_norm": 32.107154846191406,
      "learning_rate": 3.7610894941634245e-06,
      "loss": 80.6063,
      "step": 14500
    },
    {
      "epoch": 56.61658536585366,
      "grad_norm": 61.5618896484375,
      "learning_rate": 3.7740596627756166e-06,
      "loss": 80.8627,
      "step": 14550
    },
    {
      "epoch": 56.81170731707317,
      "grad_norm": 60.204593658447266,
      "learning_rate": 3.7870298313878083e-06,
      "loss": 80.6681,
      "step": 14600
    },
    {
      "epoch": 56.81170731707317,
      "eval_loss": 1.2663650512695312,
      "eval_perplexity": 3.5479326248168945,
      "eval_runtime": 45.8293,
      "eval_samples_per_second": 43.269,
      "eval_steps_per_second": 5.411,
      "step": 14600
    },
    {
      "epoch": 57.00390243902439,
      "grad_norm": 21.54239273071289,
      "learning_rate": 3.8000000000000005e-06,
      "loss": 79.3987,
      "step": 14650
    },
    {
      "epoch": 57.1990243902439,
      "grad_norm": 38.08658981323242,
      "learning_rate": 3.812970168612192e-06,
      "loss": 80.5852,
      "step": 14700
    },
    {
      "epoch": 57.39414634146341,
      "grad_norm": 27.632965087890625,
      "learning_rate": 3.825940337224384e-06,
      "loss": 80.7486,
      "step": 14750
    },
    {
      "epoch": 57.589268292682924,
      "grad_norm": 41.36558532714844,
      "learning_rate": 3.838910505836576e-06,
      "loss": 80.5359,
      "step": 14800
    },
    {
      "epoch": 57.589268292682924,
      "eval_loss": 1.265275478363037,
      "eval_perplexity": 3.5440688133239746,
      "eval_runtime": 46.1232,
      "eval_samples_per_second": 42.994,
      "eval_steps_per_second": 5.377,
      "step": 14800
    },
    {
      "epoch": 57.784390243902436,
      "grad_norm": 28.359516143798828,
      "learning_rate": 3.851880674448768e-06,
      "loss": 80.6306,
      "step": 14850
    },
    {
      "epoch": 57.97951219512195,
      "grad_norm": 43.72461700439453,
      "learning_rate": 3.86485084306096e-06,
      "loss": 80.6871,
      "step": 14900
    },
    {
      "epoch": 58.17170731707317,
      "grad_norm": 38.20333480834961,
      "learning_rate": 3.877821011673152e-06,
      "loss": 79.3433,
      "step": 14950
    },
    {
      "epoch": 58.36682926829268,
      "grad_norm": 24.780502319335938,
      "learning_rate": 3.890791180285344e-06,
      "loss": 80.4489,
      "step": 15000
    },
    {
      "epoch": 58.36682926829268,
      "eval_loss": 1.2645378112792969,
      "eval_perplexity": 3.5414555072784424,
      "eval_runtime": 45.7003,
      "eval_samples_per_second": 43.391,
      "eval_steps_per_second": 5.427,
      "step": 15000
    },
    {
      "epoch": 58.561951219512196,
      "grad_norm": 31.283416748046875,
      "learning_rate": 3.903761348897536e-06,
      "loss": 80.5275,
      "step": 15050
    },
    {
      "epoch": 58.75707317073171,
      "grad_norm": 24.840042114257812,
      "learning_rate": 3.916731517509728e-06,
      "loss": 80.449,
      "step": 15100
    },
    {
      "epoch": 58.95219512195122,
      "grad_norm": 52.582088470458984,
      "learning_rate": 3.9297016861219205e-06,
      "loss": 80.7032,
      "step": 15150
    },
    {
      "epoch": 59.144390243902436,
      "grad_norm": 44.77614212036133,
      "learning_rate": 3.942671854734112e-06,
      "loss": 79.3394,
      "step": 15200
    },
    {
      "epoch": 59.144390243902436,
      "eval_loss": 1.2638232707977295,
      "eval_perplexity": 3.538925886154175,
      "eval_runtime": 46.2911,
      "eval_samples_per_second": 42.838,
      "eval_steps_per_second": 5.357,
      "step": 15200
    },
    {
      "epoch": 59.33951219512195,
      "grad_norm": 66.53433990478516,
      "learning_rate": 3.955642023346304e-06,
      "loss": 80.4614,
      "step": 15250
    },
    {
      "epoch": 59.53463414634146,
      "grad_norm": 27.32443618774414,
      "learning_rate": 3.968612191958496e-06,
      "loss": 80.6652,
      "step": 15300
    },
    {
      "epoch": 59.72975609756097,
      "grad_norm": 33.74604797363281,
      "learning_rate": 3.981582360570688e-06,
      "loss": 80.4599,
      "step": 15350
    },
    {
      "epoch": 59.924878048780485,
      "grad_norm": 59.50687026977539,
      "learning_rate": 3.994552529182879e-06,
      "loss": 80.4746,
      "step": 15400
    },
    {
      "epoch": 59.924878048780485,
      "eval_loss": 1.263890266418457,
      "eval_perplexity": 3.539163112640381,
      "eval_runtime": 46.3867,
      "eval_samples_per_second": 42.749,
      "eval_steps_per_second": 5.346,
      "step": 15400
    },
    {
      "epoch": 60.11707317073171,
      "grad_norm": 66.43708038330078,
      "learning_rate": 4.0075226977950715e-06,
      "loss": 79.2616,
      "step": 15450
    },
    {
      "epoch": 60.31219512195122,
      "grad_norm": 36.97115707397461,
      "learning_rate": 4.020492866407264e-06,
      "loss": 80.2647,
      "step": 15500
    },
    {
      "epoch": 60.50731707317073,
      "grad_norm": 31.98956871032715,
      "learning_rate": 4.033463035019455e-06,
      "loss": 80.3672,
      "step": 15550
    },
    {
      "epoch": 60.702439024390245,
      "grad_norm": 48.1707649230957,
      "learning_rate": 4.046433203631647e-06,
      "loss": 80.4237,
      "step": 15600
    },
    {
      "epoch": 60.702439024390245,
      "eval_loss": 1.2629022598266602,
      "eval_perplexity": 3.535668134689331,
      "eval_runtime": 46.2458,
      "eval_samples_per_second": 42.88,
      "eval_steps_per_second": 5.363,
      "step": 15600
    },
    {
      "epoch": 60.89756097560976,
      "grad_norm": 47.16300964355469,
      "learning_rate": 4.05940337224384e-06,
      "loss": 80.5625,
      "step": 15650
    },
    {
      "epoch": 61.08975609756097,
      "grad_norm": 54.1176872253418,
      "learning_rate": 4.072373540856031e-06,
      "loss": 79.1633,
      "step": 15700
    },
    {
      "epoch": 61.284878048780485,
      "grad_norm": 42.8754997253418,
      "learning_rate": 4.0853437094682234e-06,
      "loss": 80.3205,
      "step": 15750
    },
    {
      "epoch": 61.48,
      "grad_norm": 46.93606185913086,
      "learning_rate": 4.0983138780804156e-06,
      "loss": 80.3351,
      "step": 15800
    },
    {
      "epoch": 61.48,
      "eval_loss": 1.262278437614441,
      "eval_perplexity": 3.5334630012512207,
      "eval_runtime": 46.0486,
      "eval_samples_per_second": 43.063,
      "eval_steps_per_second": 5.386,
      "step": 15800
    },
    {
      "epoch": 61.67512195121951,
      "grad_norm": 30.507545471191406,
      "learning_rate": 4.111284046692608e-06,
      "loss": 80.3714,
      "step": 15850
    },
    {
      "epoch": 61.87024390243902,
      "grad_norm": 34.56854248046875,
      "learning_rate": 4.124254215304799e-06,
      "loss": 80.1938,
      "step": 15900
    },
    {
      "epoch": 62.062439024390244,
      "grad_norm": 35.62372970581055,
      "learning_rate": 4.137224383916991e-06,
      "loss": 79.085,
      "step": 15950
    },
    {
      "epoch": 62.257560975609756,
      "grad_norm": 39.19121170043945,
      "learning_rate": 4.150194552529183e-06,
      "loss": 80.4429,
      "step": 16000
    },
    {
      "epoch": 62.257560975609756,
      "eval_loss": 1.261771321296692,
      "eval_perplexity": 3.5316717624664307,
      "eval_runtime": 45.5894,
      "eval_samples_per_second": 43.497,
      "eval_steps_per_second": 5.44,
      "step": 16000
    },
    {
      "epoch": 62.45268292682927,
      "grad_norm": 35.57392883300781,
      "learning_rate": 4.163164721141375e-06,
      "loss": 80.2708,
      "step": 16050
    },
    {
      "epoch": 62.64780487804878,
      "grad_norm": 62.32598876953125,
      "learning_rate": 4.176134889753567e-06,
      "loss": 80.0801,
      "step": 16100
    },
    {
      "epoch": 62.84292682926829,
      "grad_norm": 44.28409957885742,
      "learning_rate": 4.189105058365759e-06,
      "loss": 80.2573,
      "step": 16150
    },
    {
      "epoch": 63.03512195121951,
      "grad_norm": 55.378013610839844,
      "learning_rate": 4.202075226977951e-06,
      "loss": 79.0296,
      "step": 16200
    },
    {
      "epoch": 63.03512195121951,
      "eval_loss": 1.2609777450561523,
      "eval_perplexity": 3.528870105743408,
      "eval_runtime": 45.6964,
      "eval_samples_per_second": 43.395,
      "eval_steps_per_second": 5.427,
      "step": 16200
    },
    {
      "epoch": 63.23024390243902,
      "grad_norm": 70.60749053955078,
      "learning_rate": 4.215045395590143e-06,
      "loss": 80.1455,
      "step": 16250
    },
    {
      "epoch": 63.42536585365853,
      "grad_norm": 36.62373352050781,
      "learning_rate": 4.228015564202335e-06,
      "loss": 80.2325,
      "step": 16300
    },
    {
      "epoch": 63.620487804878046,
      "grad_norm": 55.55228042602539,
      "learning_rate": 4.240985732814527e-06,
      "loss": 79.9629,
      "step": 16350
    },
    {
      "epoch": 63.81560975609756,
      "grad_norm": 38.99797058105469,
      "learning_rate": 4.253955901426719e-06,
      "loss": 80.3845,
      "step": 16400
    },
    {
      "epoch": 63.81560975609756,
      "eval_loss": 1.2606676816940308,
      "eval_perplexity": 3.5277762413024902,
      "eval_runtime": 45.7925,
      "eval_samples_per_second": 43.304,
      "eval_steps_per_second": 5.416,
      "step": 16400
    },
    {
      "epoch": 64.00780487804877,
      "grad_norm": 34.801666259765625,
      "learning_rate": 4.266926070038911e-06,
      "loss": 78.9208,
      "step": 16450
    },
    {
      "epoch": 64.20292682926829,
      "grad_norm": 41.06031036376953,
      "learning_rate": 4.279896238651103e-06,
      "loss": 80.2131,
      "step": 16500
    },
    {
      "epoch": 64.3980487804878,
      "grad_norm": 64.51519012451172,
      "learning_rate": 4.292866407263295e-06,
      "loss": 80.1721,
      "step": 16550
    },
    {
      "epoch": 64.59317073170732,
      "grad_norm": 40.8251953125,
      "learning_rate": 4.305836575875487e-06,
      "loss": 80.0203,
      "step": 16600
    },
    {
      "epoch": 64.59317073170732,
      "eval_loss": 1.2602237462997437,
      "eval_perplexity": 3.526210308074951,
      "eval_runtime": 46.3711,
      "eval_samples_per_second": 42.764,
      "eval_steps_per_second": 5.348,
      "step": 16600
    },
    {
      "epoch": 64.78829268292682,
      "grad_norm": 85.82621765136719,
      "learning_rate": 4.318806744487678e-06,
      "loss": 79.876,
      "step": 16650
    },
    {
      "epoch": 64.98341463414634,
      "grad_norm": 43.15752410888672,
      "learning_rate": 4.3317769130998705e-06,
      "loss": 80.2508,
      "step": 16700
    },
    {
      "epoch": 65.17560975609756,
      "grad_norm": 48.65204620361328,
      "learning_rate": 4.344747081712063e-06,
      "loss": 78.8846,
      "step": 16750
    },
    {
      "epoch": 65.37073170731708,
      "grad_norm": 34.65209197998047,
      "learning_rate": 4.357717250324255e-06,
      "loss": 79.8135,
      "step": 16800
    },
    {
      "epoch": 65.37073170731708,
      "eval_loss": 1.2594647407531738,
      "eval_perplexity": 3.5235350131988525,
      "eval_runtime": 45.9411,
      "eval_samples_per_second": 43.164,
      "eval_steps_per_second": 5.398,
      "step": 16800
    },
    {
      "epoch": 65.56585365853658,
      "grad_norm": 41.59480285644531,
      "learning_rate": 4.370687418936447e-06,
      "loss": 79.9492,
      "step": 16850
    },
    {
      "epoch": 65.7609756097561,
      "grad_norm": 40.456424713134766,
      "learning_rate": 4.383657587548639e-06,
      "loss": 80.2194,
      "step": 16900
    },
    {
      "epoch": 65.9560975609756,
      "grad_norm": 57.77902603149414,
      "learning_rate": 4.39662775616083e-06,
      "loss": 80.154,
      "step": 16950
    },
    {
      "epoch": 66.14829268292682,
      "grad_norm": 44.0405158996582,
      "learning_rate": 4.409597924773022e-06,
      "loss": 78.7498,
      "step": 17000
    },
    {
      "epoch": 66.14829268292682,
      "eval_loss": 1.2586969137191772,
      "eval_perplexity": 3.5208306312561035,
      "eval_runtime": 46.1125,
      "eval_samples_per_second": 43.004,
      "eval_steps_per_second": 5.378,
      "step": 17000
    },
    {
      "epoch": 66.34341463414634,
      "grad_norm": 78.47004699707031,
      "learning_rate": 4.4225680933852145e-06,
      "loss": 80.0514,
      "step": 17050
    },
    {
      "epoch": 66.53853658536585,
      "grad_norm": 30.395883560180664,
      "learning_rate": 4.435538261997407e-06,
      "loss": 79.8182,
      "step": 17100
    },
    {
      "epoch": 66.73365853658537,
      "grad_norm": 41.93042755126953,
      "learning_rate": 4.448508430609598e-06,
      "loss": 80.0295,
      "step": 17150
    },
    {
      "epoch": 66.92878048780487,
      "grad_norm": 48.88070297241211,
      "learning_rate": 4.46147859922179e-06,
      "loss": 79.9517,
      "step": 17200
    },
    {
      "epoch": 66.92878048780487,
      "eval_loss": 1.2586774826049805,
      "eval_perplexity": 3.5207622051239014,
      "eval_runtime": 46.0393,
      "eval_samples_per_second": 43.072,
      "eval_steps_per_second": 5.387,
      "step": 17200
    },
    {
      "epoch": 67.1209756097561,
      "grad_norm": 55.5092658996582,
      "learning_rate": 4.474448767833982e-06,
      "loss": 78.5646,
      "step": 17250
    },
    {
      "epoch": 67.3160975609756,
      "grad_norm": 61.474937438964844,
      "learning_rate": 4.487418936446174e-06,
      "loss": 79.9189,
      "step": 17300
    },
    {
      "epoch": 67.51121951219513,
      "grad_norm": 29.194047927856445,
      "learning_rate": 4.5003891050583656e-06,
      "loss": 79.782,
      "step": 17350
    },
    {
      "epoch": 67.70634146341463,
      "grad_norm": 38.63207244873047,
      "learning_rate": 4.513359273670558e-06,
      "loss": 79.9116,
      "step": 17400
    },
    {
      "epoch": 67.70634146341463,
      "eval_loss": 1.2575451135635376,
      "eval_perplexity": 3.516777515411377,
      "eval_runtime": 46.5299,
      "eval_samples_per_second": 42.618,
      "eval_steps_per_second": 5.33,
      "step": 17400
    },
    {
      "epoch": 67.90146341463415,
      "grad_norm": 38.96307373046875,
      "learning_rate": 4.526329442282751e-06,
      "loss": 79.9023,
      "step": 17450
    },
    {
      "epoch": 68.09365853658537,
      "grad_norm": 37.689247131347656,
      "learning_rate": 4.539299610894942e-06,
      "loss": 78.6945,
      "step": 17500
    },
    {
      "epoch": 68.28878048780487,
      "grad_norm": 58.821311950683594,
      "learning_rate": 4.552269779507134e-06,
      "loss": 80.0617,
      "step": 17550
    },
    {
      "epoch": 68.48390243902439,
      "grad_norm": 50.66943359375,
      "learning_rate": 4.565239948119326e-06,
      "loss": 79.7421,
      "step": 17600
    },
    {
      "epoch": 68.48390243902439,
      "eval_loss": 1.2571204900741577,
      "eval_perplexity": 3.515284538269043,
      "eval_runtime": 46.0687,
      "eval_samples_per_second": 43.044,
      "eval_steps_per_second": 5.383,
      "step": 17600
    },
    {
      "epoch": 68.6790243902439,
      "grad_norm": 32.46078109741211,
      "learning_rate": 4.578210116731518e-06,
      "loss": 79.5455,
      "step": 17650
    },
    {
      "epoch": 68.87414634146342,
      "grad_norm": 53.44896697998047,
      "learning_rate": 4.59118028534371e-06,
      "loss": 79.6984,
      "step": 17700
    },
    {
      "epoch": 69.06634146341463,
      "grad_norm": 59.86668395996094,
      "learning_rate": 4.604150453955902e-06,
      "loss": 78.6785,
      "step": 17750
    },
    {
      "epoch": 69.26146341463415,
      "grad_norm": 53.604270935058594,
      "learning_rate": 4.617120622568094e-06,
      "loss": 79.6526,
      "step": 17800
    },
    {
      "epoch": 69.26146341463415,
      "eval_loss": 1.2564263343811035,
      "eval_perplexity": 3.512845277786255,
      "eval_runtime": 46.056,
      "eval_samples_per_second": 43.056,
      "eval_steps_per_second": 5.385,
      "step": 17800
    },
    {
      "epoch": 69.45658536585366,
      "grad_norm": 56.02439880371094,
      "learning_rate": 4.630090791180286e-06,
      "loss": 79.7153,
      "step": 17850
    },
    {
      "epoch": 69.65170731707317,
      "grad_norm": 62.417694091796875,
      "learning_rate": 4.643060959792477e-06,
      "loss": 79.6955,
      "step": 17900
    },
    {
      "epoch": 69.84682926829268,
      "grad_norm": 51.143890380859375,
      "learning_rate": 4.656031128404669e-06,
      "loss": 79.8766,
      "step": 17950
    },
    {
      "epoch": 70.0390243902439,
      "grad_norm": 59.23521423339844,
      "learning_rate": 4.6690012970168615e-06,
      "loss": 78.6408,
      "step": 18000
    },
    {
      "epoch": 70.0390243902439,
      "eval_loss": 1.2562140226364136,
      "eval_perplexity": 3.512099504470825,
      "eval_runtime": 46.1264,
      "eval_samples_per_second": 42.991,
      "eval_steps_per_second": 5.377,
      "step": 18000
    },
    {
      "epoch": 70.23414634146341,
      "grad_norm": 65.03706359863281,
      "learning_rate": 4.681971465629054e-06,
      "loss": 79.8824,
      "step": 18050
    },
    {
      "epoch": 70.42926829268292,
      "grad_norm": 54.928890228271484,
      "learning_rate": 4.694941634241246e-06,
      "loss": 79.4748,
      "step": 18100
    },
    {
      "epoch": 70.62439024390244,
      "grad_norm": 57.88855743408203,
      "learning_rate": 4.707911802853438e-06,
      "loss": 79.508,
      "step": 18150
    },
    {
      "epoch": 70.81951219512194,
      "grad_norm": 86.23046112060547,
      "learning_rate": 4.720881971465629e-06,
      "loss": 79.5186,
      "step": 18200
    },
    {
      "epoch": 70.81951219512194,
      "eval_loss": 1.2553139925003052,
      "eval_perplexity": 3.5089399814605713,
      "eval_runtime": 45.8311,
      "eval_samples_per_second": 43.268,
      "eval_steps_per_second": 5.411,
      "step": 18200
    },
    {
      "epoch": 71.01170731707317,
      "grad_norm": 96.12167358398438,
      "learning_rate": 4.733852140077821e-06,
      "loss": 78.5783,
      "step": 18250
    },
    {
      "epoch": 71.20682926829268,
      "grad_norm": 44.40259552001953,
      "learning_rate": 4.746822308690013e-06,
      "loss": 79.8531,
      "step": 18300
    },
    {
      "epoch": 71.4019512195122,
      "grad_norm": 60.54746627807617,
      "learning_rate": 4.7597924773022055e-06,
      "loss": 79.4628,
      "step": 18350
    },
    {
      "epoch": 71.5970731707317,
      "grad_norm": 36.67414093017578,
      "learning_rate": 4.772762645914397e-06,
      "loss": 79.3205,
      "step": 18400
    },
    {
      "epoch": 71.5970731707317,
      "eval_loss": 1.2546981573104858,
      "eval_perplexity": 3.506779670715332,
      "eval_runtime": 45.6274,
      "eval_samples_per_second": 43.461,
      "eval_steps_per_second": 5.435,
      "step": 18400
    },
    {
      "epoch": 71.79219512195122,
      "grad_norm": 68.39741516113281,
      "learning_rate": 4.785732814526589e-06,
      "loss": 79.5823,
      "step": 18450
    },
    {
      "epoch": 71.98731707317073,
      "grad_norm": 73.28401947021484,
      "learning_rate": 4.798702983138781e-06,
      "loss": 79.6927,
      "step": 18500
    },
    {
      "epoch": 72.17951219512194,
      "grad_norm": 49.49235534667969,
      "learning_rate": 4.811673151750973e-06,
      "loss": 78.3892,
      "step": 18550
    },
    {
      "epoch": 72.37463414634146,
      "grad_norm": 56.872474670410156,
      "learning_rate": 4.8246433203631645e-06,
      "loss": 79.4548,
      "step": 18600
    },
    {
      "epoch": 72.37463414634146,
      "eval_loss": 1.2540613412857056,
      "eval_perplexity": 3.504547357559204,
      "eval_runtime": 46.1523,
      "eval_samples_per_second": 42.966,
      "eval_steps_per_second": 5.374,
      "step": 18600
    },
    {
      "epoch": 72.56975609756097,
      "grad_norm": 61.542388916015625,
      "learning_rate": 4.8376134889753575e-06,
      "loss": 79.7205,
      "step": 18650
    },
    {
      "epoch": 72.76487804878049,
      "grad_norm": 57.866424560546875,
      "learning_rate": 4.85058365758755e-06,
      "loss": 79.4805,
      "step": 18700
    },
    {
      "epoch": 72.96,
      "grad_norm": 76.9302749633789,
      "learning_rate": 4.863553826199741e-06,
      "loss": 79.4251,
      "step": 18750
    },
    {
      "epoch": 73.15219512195122,
      "grad_norm": 69.43395233154297,
      "learning_rate": 4.876523994811933e-06,
      "loss": 78.1933,
      "step": 18800
    },
    {
      "epoch": 73.15219512195122,
      "eval_loss": 1.2542139291763306,
      "eval_perplexity": 3.505082130432129,
      "eval_runtime": 45.8517,
      "eval_samples_per_second": 43.248,
      "eval_steps_per_second": 5.409,
      "step": 18800
    },
    {
      "epoch": 73.34731707317073,
      "grad_norm": 51.62270736694336,
      "learning_rate": 4.889494163424125e-06,
      "loss": 79.276,
      "step": 18850
    },
    {
      "epoch": 73.54243902439025,
      "grad_norm": 77.11093139648438,
      "learning_rate": 4.902464332036317e-06,
      "loss": 79.7416,
      "step": 18900
    },
    {
      "epoch": 73.73756097560975,
      "grad_norm": 53.33780288696289,
      "learning_rate": 4.9154345006485085e-06,
      "loss": 79.3954,
      "step": 18950
    },
    {
      "epoch": 73.93268292682927,
      "grad_norm": 64.54133605957031,
      "learning_rate": 4.928404669260701e-06,
      "loss": 79.5091,
      "step": 19000
    },
    {
      "epoch": 73.93268292682927,
      "eval_loss": 1.2530254125595093,
      "eval_perplexity": 3.5009186267852783,
      "eval_runtime": 46.1532,
      "eval_samples_per_second": 42.966,
      "eval_steps_per_second": 5.373,
      "step": 19000
    },
    {
      "epoch": 74.12487804878049,
      "grad_norm": 60.19660949707031,
      "learning_rate": 4.941374837872893e-06,
      "loss": 78.1222,
      "step": 19050
    },
    {
      "epoch": 74.32,
      "grad_norm": 54.17720413208008,
      "learning_rate": 4.954345006485085e-06,
      "loss": 79.1721,
      "step": 19100
    },
    {
      "epoch": 74.51512195121951,
      "grad_norm": 58.56651306152344,
      "learning_rate": 4.967315175097276e-06,
      "loss": 79.5401,
      "step": 19150
    },
    {
      "epoch": 74.71024390243902,
      "grad_norm": 90.5818862915039,
      "learning_rate": 4.980285343709468e-06,
      "loss": 79.3977,
      "step": 19200
    },
    {
      "epoch": 74.71024390243902,
      "eval_loss": 1.252933382987976,
      "eval_perplexity": 3.500596523284912,
      "eval_runtime": 46.2277,
      "eval_samples_per_second": 42.896,
      "eval_steps_per_second": 5.365,
      "step": 19200
    },
    {
      "epoch": 74.90536585365854,
      "grad_norm": 87.41036224365234,
      "learning_rate": 4.9932555123216604e-06,
      "loss": 79.3959,
      "step": 19250
    },
    {
      "epoch": 75.09756097560975,
      "grad_norm": 43.23686981201172,
      "learning_rate": 5.0062256809338526e-06,
      "loss": 78.1625,
      "step": 19300
    },
    {
      "epoch": 75.29268292682927,
      "grad_norm": 59.48529815673828,
      "learning_rate": 5.019195849546045e-06,
      "loss": 79.293,
      "step": 19350
    },
    {
      "epoch": 75.48780487804878,
      "grad_norm": 40.32875061035156,
      "learning_rate": 5.032166018158237e-06,
      "loss": 79.4408,
      "step": 19400
    },
    {
      "epoch": 75.48780487804878,
      "eval_loss": 1.2522435188293457,
      "eval_perplexity": 3.4981822967529297,
      "eval_runtime": 46.3752,
      "eval_samples_per_second": 42.76,
      "eval_steps_per_second": 5.348,
      "step": 19400
    },
    {
      "epoch": 75.6829268292683,
      "grad_norm": 53.630760192871094,
      "learning_rate": 5.045136186770428e-06,
      "loss": 79.1963,
      "step": 19450
    },
    {
      "epoch": 75.8780487804878,
      "grad_norm": 68.47099304199219,
      "learning_rate": 5.05810635538262e-06,
      "loss": 79.2942,
      "step": 19500
    },
    {
      "epoch": 76.07024390243902,
      "grad_norm": 51.35224914550781,
      "learning_rate": 5.071076523994812e-06,
      "loss": 78.3237,
      "step": 19550
    },
    {
      "epoch": 76.26536585365854,
      "grad_norm": 56.05230712890625,
      "learning_rate": 5.0840466926070045e-06,
      "loss": 79.3084,
      "step": 19600
    },
    {
      "epoch": 76.26536585365854,
      "eval_loss": 1.2510722875595093,
      "eval_perplexity": 3.4940876960754395,
      "eval_runtime": 46.2147,
      "eval_samples_per_second": 42.908,
      "eval_steps_per_second": 5.366,
      "step": 19600
    },
    {
      "epoch": 76.46048780487804,
      "grad_norm": 68.63664245605469,
      "learning_rate": 5.097016861219196e-06,
      "loss": 79.1481,
      "step": 19650
    },
    {
      "epoch": 76.65560975609756,
      "grad_norm": 67.20011901855469,
      "learning_rate": 5.109987029831388e-06,
      "loss": 79.2552,
      "step": 19700
    },
    {
      "epoch": 76.85073170731707,
      "grad_norm": 45.42533493041992,
      "learning_rate": 5.12295719844358e-06,
      "loss": 79.2308,
      "step": 19750
    },
    {
      "epoch": 77.0429268292683,
      "grad_norm": 94.80575561523438,
      "learning_rate": 5.135927367055772e-06,
      "loss": 77.9437,
      "step": 19800
    },
    {
      "epoch": 77.0429268292683,
      "eval_loss": 1.251154899597168,
      "eval_perplexity": 3.4943761825561523,
      "eval_runtime": 46.9616,
      "eval_samples_per_second": 42.226,
      "eval_steps_per_second": 5.281,
      "step": 19800
    },
    {
      "epoch": 77.2380487804878,
      "grad_norm": 60.486351013183594,
      "learning_rate": 5.148897535667963e-06,
      "loss": 78.8965,
      "step": 19850
    },
    {
      "epoch": 77.43317073170732,
      "grad_norm": 90.41155242919922,
      "learning_rate": 5.1618677042801555e-06,
      "loss": 79.4059,
      "step": 19900
    },
    {
      "epoch": 77.62829268292683,
      "grad_norm": 81.02830505371094,
      "learning_rate": 5.174837872892348e-06,
      "loss": 79.2938,
      "step": 19950
    },
    {
      "epoch": 77.82341463414635,
      "grad_norm": 70.73715209960938,
      "learning_rate": 5.18780804150454e-06,
      "loss": 79.3244,
      "step": 20000
    },
    {
      "epoch": 77.82341463414635,
      "eval_loss": 1.2500674724578857,
      "eval_perplexity": 3.4905784130096436,
      "eval_runtime": 46.2992,
      "eval_samples_per_second": 42.83,
      "eval_steps_per_second": 5.356,
      "step": 20000
    },
    {
      "epoch": 78.01560975609756,
      "grad_norm": 69.39656066894531,
      "learning_rate": 5.200778210116731e-06,
      "loss": 77.8923,
      "step": 20050
    },
    {
      "epoch": 78.21073170731707,
      "grad_norm": 92.11715698242188,
      "learning_rate": 5.213748378728923e-06,
      "loss": 78.9901,
      "step": 20100
    },
    {
      "epoch": 78.40585365853659,
      "grad_norm": 52.95503616333008,
      "learning_rate": 5.226718547341116e-06,
      "loss": 79.3129,
      "step": 20150
    },
    {
      "epoch": 78.60097560975609,
      "grad_norm": 67.30686950683594,
      "learning_rate": 5.239688715953308e-06,
      "loss": 79.1909,
      "step": 20200
    },
    {
      "epoch": 78.60097560975609,
      "eval_loss": 1.2496662139892578,
      "eval_perplexity": 3.48917818069458,
      "eval_runtime": 45.8542,
      "eval_samples_per_second": 43.246,
      "eval_steps_per_second": 5.408,
      "step": 20200
    },
    {
      "epoch": 78.79609756097561,
      "grad_norm": 71.74366760253906,
      "learning_rate": 5.2526588845655004e-06,
      "loss": 79.2341,
      "step": 20250
    },
    {
      "epoch": 78.99121951219512,
      "grad_norm": 57.8742790222168,
      "learning_rate": 5.2656290531776926e-06,
      "loss": 78.9032,
      "step": 20300
    },
    {
      "epoch": 79.18341463414635,
      "grad_norm": 50.242515563964844,
      "learning_rate": 5.278599221789884e-06,
      "loss": 77.8782,
      "step": 20350
    },
    {
      "epoch": 79.37853658536585,
      "grad_norm": 86.31529998779297,
      "learning_rate": 5.291569390402076e-06,
      "loss": 79.1394,
      "step": 20400
    },
    {
      "epoch": 79.37853658536585,
      "eval_loss": 1.249241590499878,
      "eval_perplexity": 3.487696886062622,
      "eval_runtime": 46.3746,
      "eval_samples_per_second": 42.76,
      "eval_steps_per_second": 5.348,
      "step": 20400
    },
    {
      "epoch": 79.57365853658537,
      "grad_norm": 67.37433624267578,
      "learning_rate": 5.304539559014268e-06,
      "loss": 78.966,
      "step": 20450
    },
    {
      "epoch": 79.76878048780488,
      "grad_norm": 41.11604309082031,
      "learning_rate": 5.317509727626459e-06,
      "loss": 79.0986,
      "step": 20500
    },
    {
      "epoch": 79.9639024390244,
      "grad_norm": 74.38232421875,
      "learning_rate": 5.3304798962386515e-06,
      "loss": 79.0235,
      "step": 20550
    },
    {
      "epoch": 80.15609756097561,
      "grad_norm": 72.22561645507812,
      "learning_rate": 5.343450064850844e-06,
      "loss": 77.866,
      "step": 20600
    },
    {
      "epoch": 80.15609756097561,
      "eval_loss": 1.2498152256011963,
      "eval_perplexity": 3.4896981716156006,
      "eval_runtime": 46.456,
      "eval_samples_per_second": 42.686,
      "eval_steps_per_second": 5.338,
      "step": 20600
    },
    {
      "epoch": 80.35121951219512,
      "grad_norm": 98.46707916259766,
      "learning_rate": 5.356420233463036e-06,
      "loss": 78.9882,
      "step": 20650
    },
    {
      "epoch": 80.54634146341463,
      "grad_norm": 68.49076080322266,
      "learning_rate": 5.369390402075227e-06,
      "loss": 78.9092,
      "step": 20700
    },
    {
      "epoch": 80.74146341463414,
      "grad_norm": 57.837127685546875,
      "learning_rate": 5.382360570687419e-06,
      "loss": 79.1788,
      "step": 20750
    },
    {
      "epoch": 80.93658536585366,
      "grad_norm": 98.67790222167969,
      "learning_rate": 5.395330739299611e-06,
      "loss": 78.9024,
      "step": 20800
    },
    {
      "epoch": 80.93658536585366,
      "eval_loss": 1.2481915950775146,
      "eval_perplexity": 3.484036684036255,
      "eval_runtime": 45.762,
      "eval_samples_per_second": 43.333,
      "eval_steps_per_second": 5.419,
      "step": 20800
    },
    {
      "epoch": 81.12878048780487,
      "grad_norm": 95.32164764404297,
      "learning_rate": 5.408300907911803e-06,
      "loss": 77.8229,
      "step": 20850
    },
    {
      "epoch": 81.3239024390244,
      "grad_norm": 70.3943099975586,
      "learning_rate": 5.421271076523995e-06,
      "loss": 78.9431,
      "step": 20900
    },
    {
      "epoch": 81.5190243902439,
      "grad_norm": 47.92431640625,
      "learning_rate": 5.434241245136187e-06,
      "loss": 79.1262,
      "step": 20950
    },
    {
      "epoch": 81.71414634146342,
      "grad_norm": 37.1824836730957,
      "learning_rate": 5.447211413748379e-06,
      "loss": 79.0078,
      "step": 21000
    },
    {
      "epoch": 81.71414634146342,
      "eval_loss": 1.2484261989593506,
      "eval_perplexity": 3.484854221343994,
      "eval_runtime": 45.8008,
      "eval_samples_per_second": 43.296,
      "eval_steps_per_second": 5.415,
      "step": 21000
    },
    {
      "epoch": 81.90926829268292,
      "grad_norm": 73.22100067138672,
      "learning_rate": 5.460181582360571e-06,
      "loss": 78.817,
      "step": 21050
    },
    {
      "epoch": 82.10146341463414,
      "grad_norm": 52.58655548095703,
      "learning_rate": 5.473151750972762e-06,
      "loss": 77.5791,
      "step": 21100
    },
    {
      "epoch": 82.29658536585366,
      "grad_norm": 57.66385269165039,
      "learning_rate": 5.4861219195849545e-06,
      "loss": 79.1198,
      "step": 21150
    },
    {
      "epoch": 82.49170731707316,
      "grad_norm": 61.58435821533203,
      "learning_rate": 5.499092088197147e-06,
      "loss": 78.7237,
      "step": 21200
    },
    {
      "epoch": 82.49170731707316,
      "eval_loss": 1.2468689680099487,
      "eval_perplexity": 3.479431629180908,
      "eval_runtime": 46.5961,
      "eval_samples_per_second": 42.557,
      "eval_steps_per_second": 5.322,
      "step": 21200
    },
    {
      "epoch": 82.68682926829268,
      "grad_norm": 61.21829605102539,
      "learning_rate": 5.512062256809339e-06,
      "loss": 78.8764,
      "step": 21250
    },
    {
      "epoch": 82.88195121951219,
      "grad_norm": 63.95830535888672,
      "learning_rate": 5.52503242542153e-06,
      "loss": 78.7911,
      "step": 21300
    },
    {
      "epoch": 83.07414634146342,
      "grad_norm": 61.290313720703125,
      "learning_rate": 5.538002594033724e-06,
      "loss": 77.6157,
      "step": 21350
    },
    {
      "epoch": 83.26926829268292,
      "grad_norm": 87.71065521240234,
      "learning_rate": 5.550972762645915e-06,
      "loss": 78.8764,
      "step": 21400
    },
    {
      "epoch": 83.26926829268292,
      "eval_loss": 1.2462363243103027,
      "eval_perplexity": 3.477231025695801,
      "eval_runtime": 46.3315,
      "eval_samples_per_second": 42.8,
      "eval_steps_per_second": 5.353,
      "step": 21400
    },
    {
      "epoch": 83.46439024390244,
      "grad_norm": 75.7412338256836,
      "learning_rate": 5.563942931258107e-06,
      "loss": 79.1859,
      "step": 21450
    },
    {
      "epoch": 83.65951219512195,
      "grad_norm": 51.17167282104492,
      "learning_rate": 5.576913099870299e-06,
      "loss": 78.6811,
      "step": 21500
    },
    {
      "epoch": 83.85463414634147,
      "grad_norm": 106.70230102539062,
      "learning_rate": 5.5898832684824915e-06,
      "loss": 78.6316,
      "step": 21550
    },
    {
      "epoch": 84.04682926829268,
      "grad_norm": 46.53221130371094,
      "learning_rate": 5.602853437094683e-06,
      "loss": 77.495,
      "step": 21600
    },
    {
      "epoch": 84.04682926829268,
      "eval_loss": 1.2461163997650146,
      "eval_perplexity": 3.476814031600952,
      "eval_runtime": 45.5877,
      "eval_samples_per_second": 43.499,
      "eval_steps_per_second": 5.44,
      "step": 21600
    },
    {
      "epoch": 84.24195121951219,
      "grad_norm": 46.99778366088867,
      "learning_rate": 5.615823605706875e-06,
      "loss": 78.8491,
      "step": 21650
    },
    {
      "epoch": 84.43707317073171,
      "grad_norm": 96.654541015625,
      "learning_rate": 5.628793774319067e-06,
      "loss": 78.5618,
      "step": 21700
    },
    {
      "epoch": 84.63219512195121,
      "grad_norm": 104.57510375976562,
      "learning_rate": 5.641763942931259e-06,
      "loss": 78.8697,
      "step": 21750
    },
    {
      "epoch": 84.82731707317073,
      "grad_norm": 59.22598648071289,
      "learning_rate": 5.65473411154345e-06,
      "loss": 78.8491,
      "step": 21800
    },
    {
      "epoch": 84.82731707317073,
      "eval_loss": 1.2456294298171997,
      "eval_perplexity": 3.47512149810791,
      "eval_runtime": 45.9365,
      "eval_samples_per_second": 43.168,
      "eval_steps_per_second": 5.399,
      "step": 21800
    },
    {
      "epoch": 85.01951219512195,
      "grad_norm": 48.3032112121582,
      "learning_rate": 5.6677042801556425e-06,
      "loss": 77.4658,
      "step": 21850
    },
    {
      "epoch": 85.21463414634147,
      "grad_norm": 51.56279754638672,
      "learning_rate": 5.680674448767835e-06,
      "loss": 78.5443,
      "step": 21900
    },
    {
      "epoch": 85.40975609756097,
      "grad_norm": 68.8518295288086,
      "learning_rate": 5.693644617380027e-06,
      "loss": 78.6229,
      "step": 21950
    },
    {
      "epoch": 85.60487804878049,
      "grad_norm": 63.338985443115234,
      "learning_rate": 5.706614785992218e-06,
      "loss": 78.8859,
      "step": 22000
    },
    {
      "epoch": 85.60487804878049,
      "eval_loss": 1.2450569868087769,
      "eval_perplexity": 3.473132610321045,
      "eval_runtime": 46.0116,
      "eval_samples_per_second": 43.098,
      "eval_steps_per_second": 5.39,
      "step": 22000
    },
    {
      "epoch": 85.8,
      "grad_norm": 63.14470291137695,
      "learning_rate": 5.71958495460441e-06,
      "loss": 78.5751,
      "step": 22050
    },
    {
      "epoch": 85.99512195121952,
      "grad_norm": 62.1981086730957,
      "learning_rate": 5.732555123216602e-06,
      "loss": 78.8108,
      "step": 22100
    },
    {
      "epoch": 86.18731707317073,
      "grad_norm": 76.43157958984375,
      "learning_rate": 5.7455252918287945e-06,
      "loss": 77.702,
      "step": 22150
    },
    {
      "epoch": 86.38243902439024,
      "grad_norm": 67.49580383300781,
      "learning_rate": 5.758495460440986e-06,
      "loss": 78.4739,
      "step": 22200
    },
    {
      "epoch": 86.38243902439024,
      "eval_loss": 1.2454811334609985,
      "eval_perplexity": 3.4746060371398926,
      "eval_runtime": 47.5691,
      "eval_samples_per_second": 41.687,
      "eval_steps_per_second": 5.213,
      "step": 22200
    },
    {
      "epoch": 86.57756097560976,
      "grad_norm": 51.70243453979492,
      "learning_rate": 5.771465629053178e-06,
      "loss": 78.6923,
      "step": 22250
    },
    {
      "epoch": 86.77268292682926,
      "grad_norm": 60.83537673950195,
      "learning_rate": 5.78443579766537e-06,
      "loss": 78.4488,
      "step": 22300
    },
    {
      "epoch": 86.96780487804878,
      "grad_norm": 42.40756607055664,
      "learning_rate": 5.797405966277561e-06,
      "loss": 78.7539,
      "step": 22350
    },
    {
      "epoch": 87.16,
      "grad_norm": 76.59654998779297,
      "learning_rate": 5.810376134889753e-06,
      "loss": 77.2693,
      "step": 22400
    },
    {
      "epoch": 87.16,
      "eval_loss": 1.243693232536316,
      "eval_perplexity": 3.4683995246887207,
      "eval_runtime": 46.6339,
      "eval_samples_per_second": 42.523,
      "eval_steps_per_second": 5.318,
      "step": 22400
    },
    {
      "epoch": 87.35512195121952,
      "grad_norm": 60.776268005371094,
      "learning_rate": 5.8233463035019455e-06,
      "loss": 78.8167,
      "step": 22450
    },
    {
      "epoch": 87.55024390243902,
      "grad_norm": 85.40499114990234,
      "learning_rate": 5.836316472114138e-06,
      "loss": 78.6113,
      "step": 22500
    },
    {
      "epoch": 87.74536585365854,
      "grad_norm": 70.99576568603516,
      "learning_rate": 5.849286640726331e-06,
      "loss": 78.4052,
      "step": 22550
    },
    {
      "epoch": 87.94048780487805,
      "grad_norm": 64.30674743652344,
      "learning_rate": 5.862256809338523e-06,
      "loss": 78.6875,
      "step": 22600
    },
    {
      "epoch": 87.94048780487805,
      "eval_loss": 1.2429207563400269,
      "eval_perplexity": 3.4657211303710938,
      "eval_runtime": 46.5911,
      "eval_samples_per_second": 42.562,
      "eval_steps_per_second": 5.323,
      "step": 22600
    },
    {
      "epoch": 88.13268292682926,
      "grad_norm": 69.74058532714844,
      "learning_rate": 5.875226977950714e-06,
      "loss": 77.0641,
      "step": 22650
    },
    {
      "epoch": 88.32780487804878,
      "grad_norm": 93.30747985839844,
      "learning_rate": 5.888197146562906e-06,
      "loss": 78.1293,
      "step": 22700
    },
    {
      "epoch": 88.52292682926829,
      "grad_norm": 80.5257797241211,
      "learning_rate": 5.901167315175098e-06,
      "loss": 78.8547,
      "step": 22750
    },
    {
      "epoch": 88.7180487804878,
      "grad_norm": 69.82645416259766,
      "learning_rate": 5.91413748378729e-06,
      "loss": 78.6566,
      "step": 22800
    },
    {
      "epoch": 88.7180487804878,
      "eval_loss": 1.2423949241638184,
      "eval_perplexity": 3.4638993740081787,
      "eval_runtime": 46.2083,
      "eval_samples_per_second": 42.914,
      "eval_steps_per_second": 5.367,
      "step": 22800
    },
    {
      "epoch": 88.91317073170731,
      "grad_norm": 83.17684936523438,
      "learning_rate": 5.927107652399482e-06,
      "loss": 78.686,
      "step": 22850
    },
    {
      "epoch": 89.10536585365854,
      "grad_norm": 96.81497955322266,
      "learning_rate": 5.940077821011674e-06,
      "loss": 77.0611,
      "step": 22900
    },
    {
      "epoch": 89.30048780487805,
      "grad_norm": 76.14828491210938,
      "learning_rate": 5.953047989623866e-06,
      "loss": 78.4928,
      "step": 22950
    },
    {
      "epoch": 89.49560975609756,
      "grad_norm": 63.30647277832031,
      "learning_rate": 5.966018158236058e-06,
      "loss": 78.1847,
      "step": 23000
    },
    {
      "epoch": 89.49560975609756,
      "eval_loss": 1.2417935132980347,
      "eval_perplexity": 3.4618167877197266,
      "eval_runtime": 47.8025,
      "eval_samples_per_second": 41.483,
      "eval_steps_per_second": 5.188,
      "step": 23000
    },
    {
      "epoch": 89.69073170731707,
      "grad_norm": 95.15013122558594,
      "learning_rate": 5.978988326848249e-06,
      "loss": 78.5992,
      "step": 23050
    },
    {
      "epoch": 89.88585365853659,
      "grad_norm": 67.39320373535156,
      "learning_rate": 5.9919584954604415e-06,
      "loss": 78.697,
      "step": 23100
    },
    {
      "epoch": 90.0780487804878,
      "grad_norm": 70.14248657226562,
      "learning_rate": 6.004928664072634e-06,
      "loss": 77.5175,
      "step": 23150
    },
    {
      "epoch": 90.27317073170731,
      "grad_norm": 62.61317825317383,
      "learning_rate": 6.017898832684826e-06,
      "loss": 78.6858,
      "step": 23200
    },
    {
      "epoch": 90.27317073170731,
      "eval_loss": 1.241649866104126,
      "eval_perplexity": 3.4613194465637207,
      "eval_runtime": 46.5124,
      "eval_samples_per_second": 42.634,
      "eval_steps_per_second": 5.332,
      "step": 23200
    },
    {
      "epoch": 90.46829268292683,
      "grad_norm": 65.44196319580078,
      "learning_rate": 6.030869001297017e-06,
      "loss": 78.8707,
      "step": 23250
    },
    {
      "epoch": 90.66341463414633,
      "grad_norm": 90.13536071777344,
      "learning_rate": 6.043839169909209e-06,
      "loss": 78.241,
      "step": 23300
    },
    {
      "epoch": 90.85853658536585,
      "grad_norm": 68.19771575927734,
      "learning_rate": 6.056809338521401e-06,
      "loss": 78.4355,
      "step": 23350
    },
    {
      "epoch": 91.05073170731707,
      "grad_norm": 50.45315933227539,
      "learning_rate": 6.069779507133593e-06,
      "loss": 76.9972,
      "step": 23400
    },
    {
      "epoch": 91.05073170731707,
      "eval_loss": 1.2424092292785645,
      "eval_perplexity": 3.463948965072632,
      "eval_runtime": 92.2687,
      "eval_samples_per_second": 21.492,
      "eval_steps_per_second": 2.688,
      "step": 23400
    },
    {
      "epoch": 91.24585365853659,
      "grad_norm": 71.63521575927734,
      "learning_rate": 6.082749675745785e-06,
      "loss": 78.2304,
      "step": 23450
    },
    {
      "epoch": 91.4409756097561,
      "grad_norm": 111.98943328857422,
      "learning_rate": 6.095719844357977e-06,
      "loss": 78.5306,
      "step": 23500
    },
    {
      "epoch": 91.63609756097561,
      "grad_norm": 54.204288482666016,
      "learning_rate": 6.108690012970169e-06,
      "loss": 78.252,
      "step": 23550
    },
    {
      "epoch": 91.83121951219512,
      "grad_norm": 64.7399673461914,
      "learning_rate": 6.121660181582361e-06,
      "loss": 78.279,
      "step": 23600
    },
    {
      "epoch": 91.83121951219512,
      "eval_loss": 1.2404879331588745,
      "eval_perplexity": 3.4572999477386475,
      "eval_runtime": 38.0166,
      "eval_samples_per_second": 52.161,
      "eval_steps_per_second": 6.523,
      "step": 23600
    },
    {
      "epoch": 92.02341463414633,
      "grad_norm": 75.55311584472656,
      "learning_rate": 6.134630350194552e-06,
      "loss": 77.3939,
      "step": 23650
    },
    {
      "epoch": 92.21853658536585,
      "grad_norm": 86.86980438232422,
      "learning_rate": 6.1476005188067445e-06,
      "loss": 78.0154,
      "step": 23700
    },
    {
      "epoch": 92.41365853658536,
      "grad_norm": 71.59056854248047,
      "learning_rate": 6.1605706874189374e-06,
      "loss": 78.1405,
      "step": 23750
    },
    {
      "epoch": 92.60878048780488,
      "grad_norm": 67.49481964111328,
      "learning_rate": 6.1735408560311296e-06,
      "loss": 78.4874,
      "step": 23800
    },
    {
      "epoch": 92.60878048780488,
      "eval_loss": 1.2404507398605347,
      "eval_perplexity": 3.4571714401245117,
      "eval_runtime": 36.8837,
      "eval_samples_per_second": 53.764,
      "eval_steps_per_second": 6.724,
      "step": 23800
    },
    {
      "epoch": 92.80390243902438,
      "grad_norm": 60.54117965698242,
      "learning_rate": 6.186511024643322e-06,
      "loss": 78.3443,
      "step": 23850
    },
    {
      "epoch": 92.9990243902439,
      "grad_norm": 78.62271118164062,
      "learning_rate": 6.199481193255513e-06,
      "loss": 78.2606,
      "step": 23900
    },
    {
      "epoch": 93.19121951219512,
      "grad_norm": 65.8193588256836,
      "learning_rate": 6.212451361867705e-06,
      "loss": 76.642,
      "step": 23950
    },
    {
      "epoch": 93.38634146341464,
      "grad_norm": 58.84801483154297,
      "learning_rate": 6.225421530479897e-06,
      "loss": 78.3094,
      "step": 24000
    },
    {
      "epoch": 93.38634146341464,
      "eval_loss": 1.2389031648635864,
      "eval_perplexity": 3.4518253803253174,
      "eval_runtime": 43.5978,
      "eval_samples_per_second": 45.484,
      "eval_steps_per_second": 5.688,
      "step": 24000
    },
    {
      "epoch": 93.58146341463414,
      "grad_norm": 79.6810531616211,
      "learning_rate": 6.238391699092089e-06,
      "loss": 78.1908,
      "step": 24050
    },
    {
      "epoch": 93.77658536585366,
      "grad_norm": 70.87911224365234,
      "learning_rate": 6.251361867704281e-06,
      "loss": 78.1589,
      "step": 24100
    },
    {
      "epoch": 93.97170731707317,
      "grad_norm": 68.36620330810547,
      "learning_rate": 6.264332036316473e-06,
      "loss": 78.6265,
      "step": 24150
    },
    {
      "epoch": 94.16390243902438,
      "grad_norm": 86.37166595458984,
      "learning_rate": 6.277302204928665e-06,
      "loss": 76.9668,
      "step": 24200
    },
    {
      "epoch": 94.16390243902438,
      "eval_loss": 1.2384629249572754,
      "eval_perplexity": 3.450305938720703,
      "eval_runtime": 38.1628,
      "eval_samples_per_second": 51.962,
      "eval_steps_per_second": 6.498,
      "step": 24200
    },
    {
      "epoch": 94.3590243902439,
      "grad_norm": 68.61181640625,
      "learning_rate": 6.290272373540857e-06,
      "loss": 78.2576,
      "step": 24250
    },
    {
      "epoch": 94.55414634146341,
      "grad_norm": 80.7701416015625,
      "learning_rate": 6.303242542153048e-06,
      "loss": 78.0041,
      "step": 24300
    },
    {
      "epoch": 94.74926829268293,
      "grad_norm": 65.18714904785156,
      "learning_rate": 6.31621271076524e-06,
      "loss": 78.346,
      "step": 24350
    },
    {
      "epoch": 94.94439024390243,
      "grad_norm": 63.40169143676758,
      "learning_rate": 6.3291828793774325e-06,
      "loss": 78.2412,
      "step": 24400
    },
    {
      "epoch": 94.94439024390243,
      "eval_loss": 1.2409751415252686,
      "eval_perplexity": 3.458984851837158,
      "eval_runtime": 38.3234,
      "eval_samples_per_second": 51.744,
      "eval_steps_per_second": 6.471,
      "step": 24400
    },
    {
      "epoch": 95.13658536585366,
      "grad_norm": 79.28260040283203,
      "learning_rate": 6.342153047989625e-06,
      "loss": 77.0157,
      "step": 24450
    },
    {
      "epoch": 95.33170731707317,
      "grad_norm": 68.767822265625,
      "learning_rate": 6.355123216601816e-06,
      "loss": 77.8862,
      "step": 24500
    },
    {
      "epoch": 95.52682926829269,
      "grad_norm": 52.07954025268555,
      "learning_rate": 6.368093385214008e-06,
      "loss": 78.3293,
      "step": 24550
    },
    {
      "epoch": 95.72195121951219,
      "grad_norm": 46.266605377197266,
      "learning_rate": 6.3810635538262e-06,
      "loss": 78.1476,
      "step": 24600
    },
    {
      "epoch": 95.72195121951219,
      "eval_loss": 1.2376927137374878,
      "eval_perplexity": 3.4476494789123535,
      "eval_runtime": 34.1972,
      "eval_samples_per_second": 57.987,
      "eval_steps_per_second": 7.252,
      "step": 24600
    },
    {
      "epoch": 95.91707317073171,
      "grad_norm": 72.93976593017578,
      "learning_rate": 6.394033722438392e-06,
      "loss": 77.8923,
      "step": 24650
    },
    {
      "epoch": 96.10926829268293,
      "grad_norm": 62.77949523925781,
      "learning_rate": 6.407003891050584e-06,
      "loss": 76.8728,
      "step": 24700
    },
    {
      "epoch": 96.30439024390243,
      "grad_norm": 73.60626983642578,
      "learning_rate": 6.419974059662776e-06,
      "loss": 77.9549,
      "step": 24750
    },
    {
      "epoch": 96.49951219512195,
      "grad_norm": 42.42365646362305,
      "learning_rate": 6.432944228274968e-06,
      "loss": 78.4506,
      "step": 24800
    },
    {
      "epoch": 96.49951219512195,
      "eval_loss": 1.2371190786361694,
      "eval_perplexity": 3.4456725120544434,
      "eval_runtime": 31.8367,
      "eval_samples_per_second": 62.287,
      "eval_steps_per_second": 7.79,
      "step": 24800
    },
    {
      "epoch": 96.69463414634146,
      "grad_norm": 43.791465759277344,
      "learning_rate": 6.44591439688716e-06,
      "loss": 78.0196,
      "step": 24850
    },
    {
      "epoch": 96.88975609756098,
      "grad_norm": 60.73607635498047,
      "learning_rate": 6.458884565499351e-06,
      "loss": 77.5827,
      "step": 24900
    },
    {
      "epoch": 97.08195121951219,
      "grad_norm": 80.2353286743164,
      "learning_rate": 6.471854734111544e-06,
      "loss": 76.8313,
      "step": 24950
    },
    {
      "epoch": 97.27707317073171,
      "grad_norm": 64.68698120117188,
      "learning_rate": 6.484824902723736e-06,
      "loss": 78.1334,
      "step": 25000
    },
    {
      "epoch": 97.27707317073171,
      "eval_loss": 1.2356007099151611,
      "eval_perplexity": 3.4404447078704834,
      "eval_runtime": 30.672,
      "eval_samples_per_second": 64.652,
      "eval_steps_per_second": 8.086,
      "step": 25000
    },
    {
      "epoch": 97.47219512195122,
      "grad_norm": 76.82411193847656,
      "learning_rate": 6.4977950713359285e-06,
      "loss": 78.0162,
      "step": 25050
    },
    {
      "epoch": 97.66731707317074,
      "grad_norm": 69.63505554199219,
      "learning_rate": 6.510765239948121e-06,
      "loss": 77.8433,
      "step": 25100
    },
    {
      "epoch": 97.86243902439024,
      "grad_norm": 95.65419006347656,
      "learning_rate": 6.523735408560312e-06,
      "loss": 77.8445,
      "step": 25150
    },
    {
      "epoch": 98.05463414634146,
      "grad_norm": 71.70441436767578,
      "learning_rate": 6.536705577172504e-06,
      "loss": 76.7828,
      "step": 25200
    },
    {
      "epoch": 98.05463414634146,
      "eval_loss": 1.2351019382476807,
      "eval_perplexity": 3.4387290477752686,
      "eval_runtime": 33.3187,
      "eval_samples_per_second": 59.516,
      "eval_steps_per_second": 7.443,
      "step": 25200
    },
    {
      "epoch": 98.24975609756098,
      "grad_norm": 86.3576431274414,
      "learning_rate": 6.549675745784696e-06,
      "loss": 77.885,
      "step": 25250
    },
    {
      "epoch": 98.44487804878048,
      "grad_norm": 42.415618896484375,
      "learning_rate": 6.562645914396888e-06,
      "loss": 78.4322,
      "step": 25300
    },
    {
      "epoch": 98.64,
      "grad_norm": 77.31004333496094,
      "learning_rate": 6.5756160830090795e-06,
      "loss": 77.6024,
      "step": 25350
    },
    {
      "epoch": 98.8351219512195,
      "grad_norm": 78.47438049316406,
      "learning_rate": 6.588586251621272e-06,
      "loss": 77.698,
      "step": 25400
    },
    {
      "epoch": 98.8351219512195,
      "eval_loss": 1.2355594635009766,
      "eval_perplexity": 3.440302610397339,
      "eval_runtime": 33.9274,
      "eval_samples_per_second": 58.448,
      "eval_steps_per_second": 7.31,
      "step": 25400
    },
    {
      "epoch": 99.02731707317074,
      "grad_norm": 74.22371673583984,
      "learning_rate": 6.601556420233464e-06,
      "loss": 76.8277,
      "step": 25450
    },
    {
      "epoch": 99.22243902439024,
      "grad_norm": 72.92420196533203,
      "learning_rate": 6.614526588845656e-06,
      "loss": 77.6526,
      "step": 25500
    },
    {
      "epoch": 99.41756097560976,
      "grad_norm": 84.74749755859375,
      "learning_rate": 6.627496757457847e-06,
      "loss": 77.8998,
      "step": 25550
    },
    {
      "epoch": 99.61268292682927,
      "grad_norm": 70.24440002441406,
      "learning_rate": 6.640466926070039e-06,
      "loss": 77.8617,
      "step": 25600
    },
    {
      "epoch": 99.61268292682927,
      "eval_loss": 1.2337783575057983,
      "eval_perplexity": 3.434180498123169,
      "eval_runtime": 31.4299,
      "eval_samples_per_second": 63.093,
      "eval_steps_per_second": 7.891,
      "step": 25600
    },
    {
      "epoch": 99.80780487804878,
      "grad_norm": 75.43832397460938,
      "learning_rate": 6.6534370946822315e-06,
      "loss": 78.1386,
      "step": 25650
    },
    {
      "epoch": 100.0,
      "grad_norm": 21.106531143188477,
      "learning_rate": 6.666407263294424e-06,
      "loss": 76.5244,
      "step": 25700
    },
    {
      "epoch": 100.1951219512195,
      "grad_norm": 44.405757904052734,
      "learning_rate": 6.679377431906615e-06,
      "loss": 77.5917,
      "step": 25750
    },
    {
      "epoch": 100.39024390243902,
      "grad_norm": 68.60009002685547,
      "learning_rate": 6.692347600518807e-06,
      "loss": 77.7279,
      "step": 25800
    },
    {
      "epoch": 100.39024390243902,
      "eval_loss": 1.233510136604309,
      "eval_perplexity": 3.4332597255706787,
      "eval_runtime": 32.3104,
      "eval_samples_per_second": 61.373,
      "eval_steps_per_second": 7.676,
      "step": 25800
    },
    {
      "epoch": 100.58536585365853,
      "grad_norm": 81.56681060791016,
      "learning_rate": 6.705317769130999e-06,
      "loss": 77.7618,
      "step": 25850
    },
    {
      "epoch": 100.78048780487805,
      "grad_norm": 72.35240173339844,
      "learning_rate": 6.718287937743191e-06,
      "loss": 77.838,
      "step": 25900
    },
    {
      "epoch": 100.97560975609755,
      "grad_norm": 75.54377746582031,
      "learning_rate": 6.7312581063553825e-06,
      "loss": 77.9782,
      "step": 25950
    },
    {
      "epoch": 101.16780487804878,
      "grad_norm": 69.7529296875,
      "learning_rate": 6.744228274967575e-06,
      "loss": 76.2915,
      "step": 26000
    },
    {
      "epoch": 101.16780487804878,
      "eval_loss": 1.232583999633789,
      "eval_perplexity": 3.430081367492676,
      "eval_runtime": 31.2912,
      "eval_samples_per_second": 63.372,
      "eval_steps_per_second": 7.926,
      "step": 26000
    },
    {
      "epoch": 101.36292682926829,
      "grad_norm": 91.7532958984375,
      "learning_rate": 6.757198443579767e-06,
      "loss": 77.8434,
      "step": 26050
    },
    {
      "epoch": 101.55804878048781,
      "grad_norm": 59.13935470581055,
      "learning_rate": 6.770168612191959e-06,
      "loss": 77.7974,
      "step": 26100
    },
    {
      "epoch": 101.75317073170731,
      "grad_norm": 58.193721771240234,
      "learning_rate": 6.783138780804152e-06,
      "loss": 77.7184,
      "step": 26150
    },
    {
      "epoch": 101.94829268292683,
      "grad_norm": 59.22282791137695,
      "learning_rate": 6.796108949416343e-06,
      "loss": 77.8363,
      "step": 26200
    },
    {
      "epoch": 101.94829268292683,
      "eval_loss": 1.2315009832382202,
      "eval_perplexity": 3.426368474960327,
      "eval_runtime": 32.2561,
      "eval_samples_per_second": 61.477,
      "eval_steps_per_second": 7.688,
      "step": 26200
    },
    {
      "epoch": 102.14048780487805,
      "grad_norm": 40.980712890625,
      "learning_rate": 6.809079118028535e-06,
      "loss": 76.2555,
      "step": 26250
    },
    {
      "epoch": 102.33560975609755,
      "grad_norm": 45.74602127075195,
      "learning_rate": 6.822049286640727e-06,
      "loss": 77.7011,
      "step": 26300
    },
    {
      "epoch": 102.53073170731707,
      "grad_norm": 50.89128112792969,
      "learning_rate": 6.8350194552529195e-06,
      "loss": 77.5924,
      "step": 26350
    },
    {
      "epoch": 102.72585365853658,
      "grad_norm": 60.660945892333984,
      "learning_rate": 6.847989623865111e-06,
      "loss": 77.6643,
      "step": 26400
    },
    {
      "epoch": 102.72585365853658,
      "eval_loss": 1.231170892715454,
      "eval_perplexity": 3.4252376556396484,
      "eval_runtime": 33.5629,
      "eval_samples_per_second": 59.083,
      "eval_steps_per_second": 7.389,
      "step": 26400
    },
    {
      "epoch": 102.9209756097561,
      "grad_norm": 54.07129669189453,
      "learning_rate": 6.860959792477303e-06,
      "loss": 77.6965,
      "step": 26450
    },
    {
      "epoch": 103.11317073170731,
      "grad_norm": 87.73284149169922,
      "learning_rate": 6.873929961089495e-06,
      "loss": 76.2266,
      "step": 26500
    },
    {
      "epoch": 103.30829268292683,
      "grad_norm": 89.60846710205078,
      "learning_rate": 6.886900129701687e-06,
      "loss": 77.7181,
      "step": 26550
    },
    {
      "epoch": 103.50341463414634,
      "grad_norm": 80.98521423339844,
      "learning_rate": 6.8998702983138785e-06,
      "loss": 77.2219,
      "step": 26600
    },
    {
      "epoch": 103.50341463414634,
      "eval_loss": 1.2304104566574097,
      "eval_perplexity": 3.4226341247558594,
      "eval_runtime": 32.9286,
      "eval_samples_per_second": 60.221,
      "eval_steps_per_second": 7.531,
      "step": 26600
    },
    {
      "epoch": 103.69853658536586,
      "grad_norm": 68.38719940185547,
      "learning_rate": 6.912840466926071e-06,
      "loss": 77.612,
      "step": 26650
    },
    {
      "epoch": 103.89365853658536,
      "grad_norm": 76.11117553710938,
      "learning_rate": 6.925810635538263e-06,
      "loss": 77.9644,
      "step": 26700
    },
    {
      "epoch": 104.08585365853658,
      "grad_norm": 52.47669982910156,
      "learning_rate": 6.938780804150455e-06,
      "loss": 76.7392,
      "step": 26750
    },
    {
      "epoch": 104.2809756097561,
      "grad_norm": 64.02957916259766,
      "learning_rate": 6.951750972762646e-06,
      "loss": 77.4663,
      "step": 26800
    },
    {
      "epoch": 104.2809756097561,
      "eval_loss": 1.2303622961044312,
      "eval_perplexity": 3.422469139099121,
      "eval_runtime": 32.7899,
      "eval_samples_per_second": 60.476,
      "eval_steps_per_second": 7.563,
      "step": 26800
    },
    {
      "epoch": 104.4760975609756,
      "grad_norm": 73.470947265625,
      "learning_rate": 6.964721141374838e-06,
      "loss": 77.497,
      "step": 26850
    },
    {
      "epoch": 104.67121951219512,
      "grad_norm": 64.98258209228516,
      "learning_rate": 6.97769130998703e-06,
      "loss": 77.3391,
      "step": 26900
    },
    {
      "epoch": 104.86634146341463,
      "grad_norm": 56.62209701538086,
      "learning_rate": 6.9906614785992225e-06,
      "loss": 77.6226,
      "step": 26950
    },
    {
      "epoch": 105.05853658536586,
      "grad_norm": 75.74324798583984,
      "learning_rate": 7.003631647211414e-06,
      "loss": 76.2465,
      "step": 27000
    },
    {
      "epoch": 105.05853658536586,
      "eval_loss": 1.2292454242706299,
      "eval_perplexity": 3.4186489582061768,
      "eval_runtime": 33.0327,
      "eval_samples_per_second": 60.032,
      "eval_steps_per_second": 7.508,
      "step": 27000
    },
    {
      "epoch": 105.25365853658536,
      "grad_norm": 58.163047790527344,
      "learning_rate": 7.016601815823606e-06,
      "loss": 77.5951,
      "step": 27050
    },
    {
      "epoch": 105.44878048780488,
      "grad_norm": 87.75840759277344,
      "learning_rate": 7.029571984435798e-06,
      "loss": 77.4897,
      "step": 27100
    },
    {
      "epoch": 105.64390243902439,
      "grad_norm": 58.89145278930664,
      "learning_rate": 7.04254215304799e-06,
      "loss": 77.3694,
      "step": 27150
    },
    {
      "epoch": 105.8390243902439,
      "grad_norm": 49.136131286621094,
      "learning_rate": 7.0555123216601815e-06,
      "loss": 77.4233,
      "step": 27200
    },
    {
      "epoch": 105.8390243902439,
      "eval_loss": 1.228753685951233,
      "eval_perplexity": 3.41696834564209,
      "eval_runtime": 35.8152,
      "eval_samples_per_second": 55.368,
      "eval_steps_per_second": 6.924,
      "step": 27200
    },
    {
      "epoch": 106.03121951219512,
      "grad_norm": 58.01835632324219,
      "learning_rate": 7.068482490272374e-06,
      "loss": 76.204,
      "step": 27250
    },
    {
      "epoch": 106.22634146341463,
      "grad_norm": 81.00401306152344,
      "learning_rate": 7.081452658884566e-06,
      "loss": 77.0215,
      "step": 27300
    },
    {
      "epoch": 106.42146341463415,
      "grad_norm": 62.732601165771484,
      "learning_rate": 7.094422827496759e-06,
      "loss": 77.4704,
      "step": 27350
    },
    {
      "epoch": 106.61658536585365,
      "grad_norm": 59.09022903442383,
      "learning_rate": 7.107392996108951e-06,
      "loss": 77.183,
      "step": 27400
    },
    {
      "epoch": 106.61658536585365,
      "eval_loss": 1.228158712387085,
      "eval_perplexity": 3.414935827255249,
      "eval_runtime": 33.3813,
      "eval_samples_per_second": 59.405,
      "eval_steps_per_second": 7.429,
      "step": 27400
    },
    {
      "epoch": 106.81170731707317,
      "grad_norm": 69.1354751586914,
      "learning_rate": 7.120363164721142e-06,
      "loss": 77.5494,
      "step": 27450
    },
    {
      "epoch": 107.00390243902439,
      "grad_norm": 74.77510833740234,
      "learning_rate": 7.133333333333334e-06,
      "loss": 76.3177,
      "step": 27500
    },
    {
      "epoch": 107.1990243902439,
      "grad_norm": 60.03248596191406,
      "learning_rate": 7.146303501945526e-06,
      "loss": 77.6393,
      "step": 27550
    },
    {
      "epoch": 107.39414634146341,
      "grad_norm": 93.42886352539062,
      "learning_rate": 7.1592736705577185e-06,
      "loss": 76.9739,
      "step": 27600
    },
    {
      "epoch": 107.39414634146341,
      "eval_loss": 1.2275390625,
      "eval_perplexity": 3.412820339202881,
      "eval_runtime": 32.9955,
      "eval_samples_per_second": 60.099,
      "eval_steps_per_second": 7.516,
      "step": 27600
    },
    {
      "epoch": 107.58926829268293,
      "grad_norm": 87.12285614013672,
      "learning_rate": 7.17224383916991e-06,
      "loss": 77.4554,
      "step": 27650
    },
    {
      "epoch": 107.78439024390244,
      "grad_norm": 84.97637176513672,
      "learning_rate": 7.185214007782102e-06,
      "loss": 77.3562,
      "step": 27700
    },
    {
      "epoch": 107.97951219512196,
      "grad_norm": 62.085105895996094,
      "learning_rate": 7.198184176394294e-06,
      "loss": 77.3937,
      "step": 27750
    },
    {
      "epoch": 108.17170731707317,
      "grad_norm": 56.98367691040039,
      "learning_rate": 7.211154345006486e-06,
      "loss": 76.0809,
      "step": 27800
    },
    {
      "epoch": 108.17170731707317,
      "eval_loss": 1.2262399196624756,
      "eval_perplexity": 3.4083895683288574,
      "eval_runtime": 33.9805,
      "eval_samples_per_second": 58.357,
      "eval_steps_per_second": 7.298,
      "step": 27800
    },
    {
      "epoch": 108.36682926829268,
      "grad_norm": 70.7317886352539,
      "learning_rate": 7.224124513618677e-06,
      "loss": 77.4584,
      "step": 27850
    },
    {
      "epoch": 108.5619512195122,
      "grad_norm": 62.099788665771484,
      "learning_rate": 7.2370946822308695e-06,
      "loss": 77.4244,
      "step": 27900
    },
    {
      "epoch": 108.7570731707317,
      "grad_norm": 60.02988052368164,
      "learning_rate": 7.250064850843062e-06,
      "loss": 76.9028,
      "step": 27950
    },
    {
      "epoch": 108.95219512195122,
      "grad_norm": 55.9889030456543,
      "learning_rate": 7.263035019455254e-06,
      "loss": 77.2185,
      "step": 28000
    },
    {
      "epoch": 108.95219512195122,
      "eval_loss": 1.2255589962005615,
      "eval_perplexity": 3.40606951713562,
      "eval_runtime": 33.8801,
      "eval_samples_per_second": 58.53,
      "eval_steps_per_second": 7.32,
      "step": 28000
    },
    {
      "epoch": 109.14439024390244,
      "grad_norm": 59.40928649902344,
      "learning_rate": 7.276005188067445e-06,
      "loss": 75.8938,
      "step": 28050
    },
    {
      "epoch": 109.33951219512196,
      "grad_norm": 45.77985763549805,
      "learning_rate": 7.288975356679637e-06,
      "loss": 76.7575,
      "step": 28100
    },
    {
      "epoch": 109.53463414634146,
      "grad_norm": 45.206260681152344,
      "learning_rate": 7.301945525291829e-06,
      "loss": 77.5549,
      "step": 28150
    },
    {
      "epoch": 109.72975609756098,
      "grad_norm": 71.35424041748047,
      "learning_rate": 7.3149156939040214e-06,
      "loss": 77.2213,
      "step": 28200
    },
    {
      "epoch": 109.72975609756098,
      "eval_loss": 1.2257291078567505,
      "eval_perplexity": 3.406648874282837,
      "eval_runtime": 33.4556,
      "eval_samples_per_second": 59.273,
      "eval_steps_per_second": 7.413,
      "step": 28200
    },
    {
      "epoch": 109.92487804878049,
      "grad_norm": 55.67252731323242,
      "learning_rate": 7.327885862516213e-06,
      "loss": 77.5328,
      "step": 28250
    },
    {
      "epoch": 110.1170731707317,
      "grad_norm": 57.64541244506836,
      "learning_rate": 7.340856031128405e-06,
      "loss": 75.7003,
      "step": 28300
    },
    {
      "epoch": 110.31219512195122,
      "grad_norm": 58.45163345336914,
      "learning_rate": 7.353826199740597e-06,
      "loss": 77.4431,
      "step": 28350
    },
    {
      "epoch": 110.50731707317073,
      "grad_norm": 66.95048522949219,
      "learning_rate": 7.366796368352789e-06,
      "loss": 77.3076,
      "step": 28400
    },
    {
      "epoch": 110.50731707317073,
      "eval_loss": 1.2246352434158325,
      "eval_perplexity": 3.4029245376586914,
      "eval_runtime": 33.8898,
      "eval_samples_per_second": 58.513,
      "eval_steps_per_second": 7.318,
      "step": 28400
    },
    {
      "epoch": 110.70243902439024,
      "grad_norm": 73.57721710205078,
      "learning_rate": 7.37976653696498e-06,
      "loss": 77.0593,
      "step": 28450
    },
    {
      "epoch": 110.89756097560975,
      "grad_norm": 84.19498443603516,
      "learning_rate": 7.3927367055771725e-06,
      "loss": 76.7261,
      "step": 28500
    },
    {
      "epoch": 111.08975609756098,
      "grad_norm": 66.51695251464844,
      "learning_rate": 7.405706874189365e-06,
      "loss": 76.07,
      "step": 28550
    },
    {
      "epoch": 111.28487804878048,
      "grad_norm": 77.91126251220703,
      "learning_rate": 7.418677042801558e-06,
      "loss": 76.8776,
      "step": 28600
    },
    {
      "epoch": 111.28487804878048,
      "eval_loss": 1.2233412265777588,
      "eval_perplexity": 3.398524045944214,
      "eval_runtime": 33.1271,
      "eval_samples_per_second": 59.86,
      "eval_steps_per_second": 7.486,
      "step": 28600
    },
    {
      "epoch": 111.48,
      "grad_norm": 80.19942474365234,
      "learning_rate": 7.43164721141375e-06,
      "loss": 76.9929,
      "step": 28650
    },
    {
      "epoch": 111.67512195121951,
      "grad_norm": 58.33890151977539,
      "learning_rate": 7.444617380025941e-06,
      "loss": 77.1227,
      "step": 28700
    },
    {
      "epoch": 111.87024390243903,
      "grad_norm": 76.8427963256836,
      "learning_rate": 7.457587548638133e-06,
      "loss": 77.4429,
      "step": 28750
    },
    {
      "epoch": 112.06243902439024,
      "grad_norm": 81.46607208251953,
      "learning_rate": 7.470557717250325e-06,
      "loss": 75.7572,
      "step": 28800
    },
    {
      "epoch": 112.06243902439024,
      "eval_loss": 1.2238212823867798,
      "eval_perplexity": 3.400155782699585,
      "eval_runtime": 33.1131,
      "eval_samples_per_second": 59.886,
      "eval_steps_per_second": 7.489,
      "step": 28800
    },
    {
      "epoch": 112.25756097560975,
      "grad_norm": 66.85723876953125,
      "learning_rate": 7.483527885862517e-06,
      "loss": 76.7284,
      "step": 28850
    },
    {
      "epoch": 112.45268292682927,
      "grad_norm": 54.860267639160156,
      "learning_rate": 7.496498054474709e-06,
      "loss": 76.9031,
      "step": 28900
    },
    {
      "epoch": 112.64780487804877,
      "grad_norm": 69.46495056152344,
      "learning_rate": 7.509468223086901e-06,
      "loss": 76.897,
      "step": 28950
    },
    {
      "epoch": 112.8429268292683,
      "grad_norm": 60.32668685913086,
      "learning_rate": 7.522438391699093e-06,
      "loss": 77.1712,
      "step": 29000
    },
    {
      "epoch": 112.8429268292683,
      "eval_loss": 1.222282886505127,
      "eval_perplexity": 3.3949291706085205,
      "eval_runtime": 32.8929,
      "eval_samples_per_second": 60.287,
      "eval_steps_per_second": 7.54,
      "step": 29000
    },
    {
      "epoch": 113.03512195121951,
      "grad_norm": 84.5885238647461,
      "learning_rate": 7.535408560311285e-06,
      "loss": 76.0691,
      "step": 29050
    },
    {
      "epoch": 113.23024390243903,
      "grad_norm": 67.7867431640625,
      "learning_rate": 7.548378728923476e-06,
      "loss": 76.9416,
      "step": 29100
    },
    {
      "epoch": 113.42536585365853,
      "grad_norm": 78.27384948730469,
      "learning_rate": 7.5613488975356685e-06,
      "loss": 77.1169,
      "step": 29150
    },
    {
      "epoch": 113.62048780487805,
      "grad_norm": 93.17973327636719,
      "learning_rate": 7.574319066147861e-06,
      "loss": 76.7378,
      "step": 29200
    },
    {
      "epoch": 113.62048780487805,
      "eval_loss": 1.2221721410751343,
      "eval_perplexity": 3.3945531845092773,
      "eval_runtime": 32.7503,
      "eval_samples_per_second": 60.549,
      "eval_steps_per_second": 7.572,
      "step": 29200
    },
    {
      "epoch": 113.81560975609756,
      "grad_norm": 55.12309646606445,
      "learning_rate": 7.587289234760053e-06,
      "loss": 76.6556,
      "step": 29250
    },
    {
      "epoch": 114.00780487804877,
      "grad_norm": 79.63546752929688,
      "learning_rate": 7.600259403372244e-06,
      "loss": 75.8255,
      "step": 29300
    },
    {
      "epoch": 114.20292682926829,
      "grad_norm": 93.59783172607422,
      "learning_rate": 7.613229571984436e-06,
      "loss": 76.8669,
      "step": 29350
    },
    {
      "epoch": 114.3980487804878,
      "grad_norm": 56.749488830566406,
      "learning_rate": 7.626199740596628e-06,
      "loss": 77.0888,
      "step": 29400
    },
    {
      "epoch": 114.3980487804878,
      "eval_loss": 1.2208455801010132,
      "eval_perplexity": 3.3900530338287354,
      "eval_runtime": 33.2307,
      "eval_samples_per_second": 59.674,
      "eval_steps_per_second": 7.463,
      "step": 29400
    },
    {
      "epoch": 114.59317073170732,
      "grad_norm": 58.12580871582031,
      "learning_rate": 7.63916990920882e-06,
      "loss": 76.9835,
      "step": 29450
    },
    {
      "epoch": 114.78829268292682,
      "grad_norm": 79.01248931884766,
      "learning_rate": 7.652140077821012e-06,
      "loss": 76.8015,
      "step": 29500
    },
    {
      "epoch": 114.98341463414634,
      "grad_norm": 78.36924743652344,
      "learning_rate": 7.665110246433204e-06,
      "loss": 76.6084,
      "step": 29550
    },
    {
      "epoch": 115.17560975609756,
      "grad_norm": 81.15123748779297,
      "learning_rate": 7.678080415045396e-06,
      "loss": 76.2005,
      "step": 29600
    },
    {
      "epoch": 115.17560975609756,
      "eval_loss": 1.2218048572540283,
      "eval_perplexity": 3.3933067321777344,
      "eval_runtime": 34.2033,
      "eval_samples_per_second": 57.977,
      "eval_steps_per_second": 7.251,
      "step": 29600
    },
    {
      "epoch": 115.37073170731708,
      "grad_norm": 74.3904037475586,
      "learning_rate": 7.691050583657588e-06,
      "loss": 76.5215,
      "step": 29650
    },
    {
      "epoch": 115.56585365853658,
      "grad_norm": 84.36856842041016,
      "learning_rate": 7.70402075226978e-06,
      "loss": 76.7794,
      "step": 29700
    },
    {
      "epoch": 115.7609756097561,
      "grad_norm": 69.50337982177734,
      "learning_rate": 7.716990920881972e-06,
      "loss": 76.8399,
      "step": 29750
    },
    {
      "epoch": 115.9560975609756,
      "grad_norm": 55.93000793457031,
      "learning_rate": 7.729961089494164e-06,
      "loss": 76.662,
      "step": 29800
    },
    {
      "epoch": 115.9560975609756,
      "eval_loss": 1.2194823026657104,
      "eval_perplexity": 3.385434627532959,
      "eval_runtime": 32.4571,
      "eval_samples_per_second": 61.096,
      "eval_steps_per_second": 7.641,
      "step": 29800
    },
    {
      "epoch": 116.14829268292682,
      "grad_norm": 93.03834533691406,
      "learning_rate": 7.742931258106357e-06,
      "loss": 75.6686,
      "step": 29850
    },
    {
      "epoch": 116.34341463414634,
      "grad_norm": 80.21048736572266,
      "learning_rate": 7.755901426718549e-06,
      "loss": 76.8268,
      "step": 29900
    },
    {
      "epoch": 116.53853658536585,
      "grad_norm": 33.97014617919922,
      "learning_rate": 7.76887159533074e-06,
      "loss": 76.8901,
      "step": 29950
    },
    {
      "epoch": 116.73365853658537,
      "grad_norm": 67.39973449707031,
      "learning_rate": 7.781841763942933e-06,
      "loss": 76.2558,
      "step": 30000
    },
    {
      "epoch": 116.73365853658537,
      "eval_loss": 1.2188262939453125,
      "eval_perplexity": 3.3832144737243652,
      "eval_runtime": 31.7889,
      "eval_samples_per_second": 62.38,
      "eval_steps_per_second": 7.801,
      "step": 30000
    },
    {
      "epoch": 116.92878048780487,
      "grad_norm": 69.78201293945312,
      "learning_rate": 7.794811932555123e-06,
      "loss": 76.8842,
      "step": 30050
    },
    {
      "epoch": 117.1209756097561,
      "grad_norm": 63.61412811279297,
      "learning_rate": 7.807782101167315e-06,
      "loss": 75.1869,
      "step": 30100
    },
    {
      "epoch": 117.3160975609756,
      "grad_norm": 50.725364685058594,
      "learning_rate": 7.820752269779508e-06,
      "loss": 76.7229,
      "step": 30150
    },
    {
      "epoch": 117.51121951219513,
      "grad_norm": 50.12179183959961,
      "learning_rate": 7.8337224383917e-06,
      "loss": 76.4078,
      "step": 30200
    },
    {
      "epoch": 117.51121951219513,
      "eval_loss": 1.2183136940002441,
      "eval_perplexity": 3.3814806938171387,
      "eval_runtime": 32.4534,
      "eval_samples_per_second": 61.103,
      "eval_steps_per_second": 7.642,
      "step": 30200
    },
    {
      "epoch": 117.70634146341463,
      "grad_norm": 55.67790603637695,
      "learning_rate": 7.846692607003892e-06,
      "loss": 76.6981,
      "step": 30250
    },
    {
      "epoch": 117.90146341463415,
      "grad_norm": 82.9999008178711,
      "learning_rate": 7.859662775616084e-06,
      "loss": 76.8088,
      "step": 30300
    },
    {
      "epoch": 118.09365853658537,
      "grad_norm": 53.0321044921875,
      "learning_rate": 7.872632944228276e-06,
      "loss": 75.6835,
      "step": 30350
    },
    {
      "epoch": 118.28878048780487,
      "grad_norm": 75.74787139892578,
      "learning_rate": 7.885603112840468e-06,
      "loss": 76.3489,
      "step": 30400
    },
    {
      "epoch": 118.28878048780487,
      "eval_loss": 1.2177801132202148,
      "eval_perplexity": 3.3796768188476562,
      "eval_runtime": 34.0401,
      "eval_samples_per_second": 58.255,
      "eval_steps_per_second": 7.286,
      "step": 30400
    },
    {
      "epoch": 118.48390243902439,
      "grad_norm": 40.64992141723633,
      "learning_rate": 7.898573281452659e-06,
      "loss": 76.8171,
      "step": 30450
    },
    {
      "epoch": 118.6790243902439,
      "grad_norm": 53.71099853515625,
      "learning_rate": 7.91154345006485e-06,
      "loss": 76.7792,
      "step": 30500
    },
    {
      "epoch": 118.87414634146342,
      "grad_norm": 110.9295654296875,
      "learning_rate": 7.924513618677043e-06,
      "loss": 76.6998,
      "step": 30550
    },
    {
      "epoch": 119.06634146341463,
      "grad_norm": 84.49185943603516,
      "learning_rate": 7.937483787289235e-06,
      "loss": 75.3809,
      "step": 30600
    },
    {
      "epoch": 119.06634146341463,
      "eval_loss": 1.2178568840026855,
      "eval_perplexity": 3.379936456680298,
      "eval_runtime": 33.036,
      "eval_samples_per_second": 60.025,
      "eval_steps_per_second": 7.507,
      "step": 30600
    },
    {
      "epoch": 119.26146341463415,
      "grad_norm": 70.87801361083984,
      "learning_rate": 7.950453955901427e-06,
      "loss": 76.8535,
      "step": 30650
    },
    {
      "epoch": 119.45658536585366,
      "grad_norm": 82.11611938476562,
      "learning_rate": 7.96342412451362e-06,
      "loss": 76.7323,
      "step": 30700
    },
    {
      "epoch": 119.65170731707317,
      "grad_norm": 46.3865852355957,
      "learning_rate": 7.976394293125811e-06,
      "loss": 76.4955,
      "step": 30750
    },
    {
      "epoch": 119.84682926829268,
      "grad_norm": 50.54000473022461,
      "learning_rate": 7.989364461738004e-06,
      "loss": 76.1311,
      "step": 30800
    },
    {
      "epoch": 119.84682926829268,
      "eval_loss": 1.2158373594284058,
      "eval_perplexity": 3.373117446899414,
      "eval_runtime": 36.0993,
      "eval_samples_per_second": 54.932,
      "eval_steps_per_second": 6.87,
      "step": 30800
    },
    {
      "epoch": 120.0390243902439,
      "grad_norm": 51.13536071777344,
      "learning_rate": 8.002334630350194e-06,
      "loss": 75.2003,
      "step": 30850
    },
    {
      "epoch": 120.23414634146341,
      "grad_norm": 64.12824249267578,
      "learning_rate": 8.015304798962386e-06,
      "loss": 76.3435,
      "step": 30900
    },
    {
      "epoch": 120.42926829268292,
      "grad_norm": 86.13548278808594,
      "learning_rate": 8.028274967574578e-06,
      "loss": 76.5441,
      "step": 30950
    },
    {
      "epoch": 120.62439024390244,
      "grad_norm": 65.93336486816406,
      "learning_rate": 8.041245136186772e-06,
      "loss": 76.693,
      "step": 31000
    },
    {
      "epoch": 120.62439024390244,
      "eval_loss": 1.2152416706085205,
      "eval_perplexity": 3.3711087703704834,
      "eval_runtime": 32.8696,
      "eval_samples_per_second": 60.329,
      "eval_steps_per_second": 7.545,
      "step": 31000
    },
    {
      "epoch": 120.81951219512194,
      "grad_norm": 63.572593688964844,
      "learning_rate": 8.054215304798964e-06,
      "loss": 76.3138,
      "step": 31050
    },
    {
      "epoch": 121.01170731707317,
      "grad_norm": 69.38111877441406,
      "learning_rate": 8.067185473411155e-06,
      "loss": 75.4093,
      "step": 31100
    },
    {
      "epoch": 121.20682926829268,
      "grad_norm": 62.630149841308594,
      "learning_rate": 8.080155642023347e-06,
      "loss": 76.5706,
      "step": 31150
    },
    {
      "epoch": 121.4019512195122,
      "grad_norm": 65.15371704101562,
      "learning_rate": 8.093125810635539e-06,
      "loss": 76.1441,
      "step": 31200
    },
    {
      "epoch": 121.4019512195122,
      "eval_loss": 1.214853048324585,
      "eval_perplexity": 3.3697988986968994,
      "eval_runtime": 32.7031,
      "eval_samples_per_second": 60.636,
      "eval_steps_per_second": 7.583,
      "step": 31200
    },
    {
      "epoch": 121.5970731707317,
      "grad_norm": 55.0949592590332,
      "learning_rate": 8.106095979247731e-06,
      "loss": 76.2261,
      "step": 31250
    },
    {
      "epoch": 121.79219512195122,
      "grad_norm": 76.57344055175781,
      "learning_rate": 8.119066147859923e-06,
      "loss": 76.821,
      "step": 31300
    },
    {
      "epoch": 121.98731707317073,
      "grad_norm": 79.36648559570312,
      "learning_rate": 8.132036316472115e-06,
      "loss": 76.3269,
      "step": 31350
    },
    {
      "epoch": 122.17951219512194,
      "grad_norm": 82.24600982666016,
      "learning_rate": 8.145006485084307e-06,
      "loss": 75.7244,
      "step": 31400
    },
    {
      "epoch": 122.17951219512194,
      "eval_loss": 1.214133620262146,
      "eval_perplexity": 3.367375373840332,
      "eval_runtime": 32.7197,
      "eval_samples_per_second": 60.606,
      "eval_steps_per_second": 7.58,
      "step": 31400
    },
    {
      "epoch": 122.37463414634146,
      "grad_norm": 53.279579162597656,
      "learning_rate": 8.1579766536965e-06,
      "loss": 76.1339,
      "step": 31450
    },
    {
      "epoch": 122.56975609756097,
      "grad_norm": 71.19982147216797,
      "learning_rate": 8.17094682230869e-06,
      "loss": 76.1067,
      "step": 31500
    },
    {
      "epoch": 122.76487804878049,
      "grad_norm": 41.124149322509766,
      "learning_rate": 8.183916990920882e-06,
      "loss": 76.015,
      "step": 31550
    },
    {
      "epoch": 122.96,
      "grad_norm": 58.52576446533203,
      "learning_rate": 8.196887159533074e-06,
      "loss": 76.9182,
      "step": 31600
    },
    {
      "epoch": 122.96,
      "eval_loss": 1.2135534286499023,
      "eval_perplexity": 3.365422248840332,
      "eval_runtime": 33.7477,
      "eval_samples_per_second": 58.76,
      "eval_steps_per_second": 7.349,
      "step": 31600
    },
    {
      "epoch": 123.15219512195122,
      "grad_norm": 67.08340454101562,
      "learning_rate": 8.209857328145266e-06,
      "loss": 74.907,
      "step": 31650
    },
    {
      "epoch": 123.34731707317073,
      "grad_norm": 67.17622375488281,
      "learning_rate": 8.222827496757458e-06,
      "loss": 76.5746,
      "step": 31700
    },
    {
      "epoch": 123.54243902439025,
      "grad_norm": 63.08903503417969,
      "learning_rate": 8.23579766536965e-06,
      "loss": 75.9292,
      "step": 31750
    },
    {
      "epoch": 123.73756097560975,
      "grad_norm": 68.91940307617188,
      "learning_rate": 8.248767833981843e-06,
      "loss": 76.3719,
      "step": 31800
    },
    {
      "epoch": 123.73756097560975,
      "eval_loss": 1.2123123407363892,
      "eval_perplexity": 3.361248016357422,
      "eval_runtime": 31.9226,
      "eval_samples_per_second": 62.119,
      "eval_steps_per_second": 7.769,
      "step": 31800
    },
    {
      "epoch": 123.93268292682927,
      "grad_norm": 90.73916625976562,
      "learning_rate": 8.261738002594035e-06,
      "loss": 76.4218,
      "step": 31850
    },
    {
      "epoch": 124.12487804878049,
      "grad_norm": 74.32209777832031,
      "learning_rate": 8.274708171206225e-06,
      "loss": 74.928,
      "step": 31900
    },
    {
      "epoch": 124.32,
      "grad_norm": 76.04503631591797,
      "learning_rate": 8.287678339818417e-06,
      "loss": 75.652,
      "step": 31950
    },
    {
      "epoch": 124.51512195121951,
      "grad_norm": 56.40437698364258,
      "learning_rate": 8.30064850843061e-06,
      "loss": 76.0815,
      "step": 32000
    },
    {
      "epoch": 124.51512195121951,
      "eval_loss": 1.2115757465362549,
      "eval_perplexity": 3.3587729930877686,
      "eval_runtime": 32.091,
      "eval_samples_per_second": 61.793,
      "eval_steps_per_second": 7.728,
      "step": 32000
    },
    {
      "epoch": 124.71024390243902,
      "grad_norm": 79.3938980102539,
      "learning_rate": 8.313618677042802e-06,
      "loss": 76.5998,
      "step": 32050
    },
    {
      "epoch": 124.90536585365854,
      "grad_norm": 45.023746490478516,
      "learning_rate": 8.326588845654994e-06,
      "loss": 76.2303,
      "step": 32100
    },
    {
      "epoch": 125.09756097560975,
      "grad_norm": 95.63916015625,
      "learning_rate": 8.339559014267186e-06,
      "loss": 75.2679,
      "step": 32150
    },
    {
      "epoch": 125.29268292682927,
      "grad_norm": 116.59077453613281,
      "learning_rate": 8.352529182879378e-06,
      "loss": 76.3945,
      "step": 32200
    },
    {
      "epoch": 125.29268292682927,
      "eval_loss": 1.2116408348083496,
      "eval_perplexity": 3.3589916229248047,
      "eval_runtime": 36.909,
      "eval_samples_per_second": 53.727,
      "eval_steps_per_second": 6.719,
      "step": 32200
    },
    {
      "epoch": 125.48780487804878,
      "grad_norm": 71.76261138916016,
      "learning_rate": 8.36549935149157e-06,
      "loss": 75.9454,
      "step": 32250
    },
    {
      "epoch": 125.6829268292683,
      "grad_norm": 58.901954650878906,
      "learning_rate": 8.378469520103762e-06,
      "loss": 76.1107,
      "step": 32300
    },
    {
      "epoch": 125.8780487804878,
      "grad_norm": 59.65183639526367,
      "learning_rate": 8.391439688715954e-06,
      "loss": 76.3408,
      "step": 32350
    },
    {
      "epoch": 126.07024390243902,
      "grad_norm": 71.57334899902344,
      "learning_rate": 8.404409857328147e-06,
      "loss": 75.1602,
      "step": 32400
    },
    {
      "epoch": 126.07024390243902,
      "eval_loss": 1.2108389139175415,
      "eval_perplexity": 3.3562991619110107,
      "eval_runtime": 33.3486,
      "eval_samples_per_second": 59.463,
      "eval_steps_per_second": 7.437,
      "step": 32400
    },
    {
      "epoch": 126.26536585365854,
      "grad_norm": 52.361602783203125,
      "learning_rate": 8.417380025940339e-06,
      "loss": 75.8571,
      "step": 32450
    },
    {
      "epoch": 126.46048780487804,
      "grad_norm": 67.28522491455078,
      "learning_rate": 8.43035019455253e-06,
      "loss": 76.2816,
      "step": 32500
    },
    {
      "epoch": 126.65560975609756,
      "grad_norm": 55.9750862121582,
      "learning_rate": 8.443320363164721e-06,
      "loss": 76.006,
      "step": 32550
    },
    {
      "epoch": 126.85073170731707,
      "grad_norm": 61.30789566040039,
      "learning_rate": 8.456290531776913e-06,
      "loss": 75.8604,
      "step": 32600
    },
    {
      "epoch": 126.85073170731707,
      "eval_loss": 1.2097859382629395,
      "eval_perplexity": 3.352766752243042,
      "eval_runtime": 34.1089,
      "eval_samples_per_second": 58.137,
      "eval_steps_per_second": 7.271,
      "step": 32600
    },
    {
      "epoch": 127.0429268292683,
      "grad_norm": 57.85205078125,
      "learning_rate": 8.469260700389105e-06,
      "loss": 75.0737,
      "step": 32650
    },
    {
      "epoch": 127.2380487804878,
      "grad_norm": 70.80636596679688,
      "learning_rate": 8.482230869001298e-06,
      "loss": 75.9106,
      "step": 32700
    },
    {
      "epoch": 127.43317073170732,
      "grad_norm": 76.0492172241211,
      "learning_rate": 8.49520103761349e-06,
      "loss": 75.8874,
      "step": 32750
    },
    {
      "epoch": 127.62829268292683,
      "grad_norm": 47.12430191040039,
      "learning_rate": 8.508171206225682e-06,
      "loss": 76.2251,
      "step": 32800
    },
    {
      "epoch": 127.62829268292683,
      "eval_loss": 1.2088342905044556,
      "eval_perplexity": 3.3495776653289795,
      "eval_runtime": 31.8166,
      "eval_samples_per_second": 62.326,
      "eval_steps_per_second": 7.795,
      "step": 32800
    },
    {
      "epoch": 127.82341463414635,
      "grad_norm": 90.07543182373047,
      "learning_rate": 8.521141374837874e-06,
      "loss": 76.1909,
      "step": 32850
    },
    {
      "epoch": 128.01560975609755,
      "grad_norm": 50.60216522216797,
      "learning_rate": 8.534111543450066e-06,
      "loss": 75.1629,
      "step": 32900
    },
    {
      "epoch": 128.21073170731708,
      "grad_norm": 54.088340759277344,
      "learning_rate": 8.547081712062257e-06,
      "loss": 75.8324,
      "step": 32950
    },
    {
      "epoch": 128.40585365853659,
      "grad_norm": 73.01464080810547,
      "learning_rate": 8.560051880674449e-06,
      "loss": 75.9903,
      "step": 33000
    },
    {
      "epoch": 128.40585365853659,
      "eval_loss": 1.208456039428711,
      "eval_perplexity": 3.348310947418213,
      "eval_runtime": 33.0858,
      "eval_samples_per_second": 59.935,
      "eval_steps_per_second": 7.496,
      "step": 33000
    },
    {
      "epoch": 128.6009756097561,
      "grad_norm": 70.2934341430664,
      "learning_rate": 8.57302204928664e-06,
      "loss": 75.737,
      "step": 33050
    },
    {
      "epoch": 128.7960975609756,
      "grad_norm": 68.60060119628906,
      "learning_rate": 8.585992217898833e-06,
      "loss": 76.0086,
      "step": 33100
    },
    {
      "epoch": 128.99121951219513,
      "grad_norm": 47.21454620361328,
      "learning_rate": 8.598962386511025e-06,
      "loss": 76.1759,
      "step": 33150
    },
    {
      "epoch": 129.18341463414635,
      "grad_norm": 81.11746978759766,
      "learning_rate": 8.611932555123217e-06,
      "loss": 75.2372,
      "step": 33200
    },
    {
      "epoch": 129.18341463414635,
      "eval_loss": 1.2075995206832886,
      "eval_perplexity": 3.345444440841675,
      "eval_runtime": 33.5815,
      "eval_samples_per_second": 59.05,
      "eval_steps_per_second": 7.385,
      "step": 33200
    },
    {
      "epoch": 129.37853658536585,
      "grad_norm": 51.25579833984375,
      "learning_rate": 8.62490272373541e-06,
      "loss": 75.6434,
      "step": 33250
    },
    {
      "epoch": 129.57365853658536,
      "grad_norm": 38.89411926269531,
      "learning_rate": 8.637872892347601e-06,
      "loss": 75.5714,
      "step": 33300
    },
    {
      "epoch": 129.7687804878049,
      "grad_norm": 72.7647705078125,
      "learning_rate": 8.650843060959792e-06,
      "loss": 75.8088,
      "step": 33350
    },
    {
      "epoch": 129.9639024390244,
      "grad_norm": 59.33364486694336,
      "learning_rate": 8.663813229571986e-06,
      "loss": 76.429,
      "step": 33400
    },
    {
      "epoch": 129.9639024390244,
      "eval_loss": 1.2070529460906982,
      "eval_perplexity": 3.343616247177124,
      "eval_runtime": 32.4831,
      "eval_samples_per_second": 61.047,
      "eval_steps_per_second": 7.635,
      "step": 33400
    },
    {
      "epoch": 130.1560975609756,
      "grad_norm": 38.37897491455078,
      "learning_rate": 8.676783398184178e-06,
      "loss": 75.0214,
      "step": 33450
    },
    {
      "epoch": 130.35121951219512,
      "grad_norm": 66.07197570800781,
      "learning_rate": 8.68975356679637e-06,
      "loss": 75.996,
      "step": 33500
    },
    {
      "epoch": 130.54634146341462,
      "grad_norm": 86.44100952148438,
      "learning_rate": 8.702723735408562e-06,
      "loss": 76.1165,
      "step": 33550
    },
    {
      "epoch": 130.74146341463415,
      "grad_norm": 48.731544494628906,
      "learning_rate": 8.715693904020754e-06,
      "loss": 75.6427,
      "step": 33600
    },
    {
      "epoch": 130.74146341463415,
      "eval_loss": 1.20652174949646,
      "eval_perplexity": 3.3418407440185547,
      "eval_runtime": 33.0735,
      "eval_samples_per_second": 59.957,
      "eval_steps_per_second": 7.498,
      "step": 33600
    },
    {
      "epoch": 130.93658536585366,
      "grad_norm": 83.46350860595703,
      "learning_rate": 8.728664072632945e-06,
      "loss": 75.9009,
      "step": 33650
    },
    {
      "epoch": 131.12878048780487,
      "grad_norm": 64.89693450927734,
      "learning_rate": 8.741634241245137e-06,
      "loss": 73.9929,
      "step": 33700
    },
    {
      "epoch": 131.32390243902438,
      "grad_norm": 51.38096237182617,
      "learning_rate": 8.754604409857329e-06,
      "loss": 75.9683,
      "step": 33750
    },
    {
      "epoch": 131.5190243902439,
      "grad_norm": 86.61460876464844,
      "learning_rate": 8.767574578469521e-06,
      "loss": 75.8976,
      "step": 33800
    },
    {
      "epoch": 131.5190243902439,
      "eval_loss": 1.2055505514144897,
      "eval_perplexity": 3.3385965824127197,
      "eval_runtime": 35.1027,
      "eval_samples_per_second": 56.491,
      "eval_steps_per_second": 7.065,
      "step": 33800
    },
    {
      "epoch": 131.71414634146342,
      "grad_norm": 77.5379867553711,
      "learning_rate": 8.780544747081713e-06,
      "loss": 75.7316,
      "step": 33850
    },
    {
      "epoch": 131.90926829268292,
      "grad_norm": 71.85955047607422,
      "learning_rate": 8.793514915693905e-06,
      "loss": 75.9869,
      "step": 33900
    },
    {
      "epoch": 132.10146341463414,
      "grad_norm": 63.6020393371582,
      "learning_rate": 8.806485084306097e-06,
      "loss": 74.7619,
      "step": 33950
    },
    {
      "epoch": 132.29658536585364,
      "grad_norm": 71.00436401367188,
      "learning_rate": 8.819455252918288e-06,
      "loss": 75.7188,
      "step": 34000
    },
    {
      "epoch": 132.29658536585364,
      "eval_loss": 1.2060410976409912,
      "eval_perplexity": 3.3402347564697266,
      "eval_runtime": 31.878,
      "eval_samples_per_second": 62.206,
      "eval_steps_per_second": 7.78,
      "step": 34000
    },
    {
      "epoch": 132.49170731707318,
      "grad_norm": 98.44277954101562,
      "learning_rate": 8.83242542153048e-06,
      "loss": 75.408,
      "step": 34050
    },
    {
      "epoch": 132.68682926829268,
      "grad_norm": 56.15290832519531,
      "learning_rate": 8.845395590142672e-06,
      "loss": 75.9431,
      "step": 34100
    },
    {
      "epoch": 132.8819512195122,
      "grad_norm": 59.021175384521484,
      "learning_rate": 8.858365758754864e-06,
      "loss": 75.5996,
      "step": 34150
    },
    {
      "epoch": 133.0741463414634,
      "grad_norm": 54.226715087890625,
      "learning_rate": 8.871335927367056e-06,
      "loss": 75.0591,
      "step": 34200
    },
    {
      "epoch": 133.0741463414634,
      "eval_loss": 1.2064251899719238,
      "eval_perplexity": 3.341517925262451,
      "eval_runtime": 34.955,
      "eval_samples_per_second": 56.73,
      "eval_steps_per_second": 7.095,
      "step": 34200
    },
    {
      "epoch": 133.26926829268294,
      "grad_norm": 82.64982604980469,
      "learning_rate": 8.884306095979248e-06,
      "loss": 75.3991,
      "step": 34250
    },
    {
      "epoch": 133.46439024390244,
      "grad_norm": 66.2894058227539,
      "learning_rate": 8.89727626459144e-06,
      "loss": 75.4678,
      "step": 34300
    },
    {
      "epoch": 133.65951219512195,
      "grad_norm": 74.13277435302734,
      "learning_rate": 8.910246433203633e-06,
      "loss": 75.747,
      "step": 34350
    },
    {
      "epoch": 133.85463414634145,
      "grad_norm": 45.74089431762695,
      "learning_rate": 8.923216601815823e-06,
      "loss": 76.036,
      "step": 34400
    },
    {
      "epoch": 133.85463414634145,
      "eval_loss": 1.2043672800064087,
      "eval_perplexity": 3.334648609161377,
      "eval_runtime": 32.9698,
      "eval_samples_per_second": 60.146,
      "eval_steps_per_second": 7.522,
      "step": 34400
    },
    {
      "epoch": 134.04682926829267,
      "grad_norm": 56.10232162475586,
      "learning_rate": 8.936186770428015e-06,
      "loss": 74.5029,
      "step": 34450
    },
    {
      "epoch": 134.2419512195122,
      "grad_norm": 56.106502532958984,
      "learning_rate": 8.949156939040207e-06,
      "loss": 75.474,
      "step": 34500
    },
    {
      "epoch": 134.4370731707317,
      "grad_norm": 72.81916809082031,
      "learning_rate": 8.9621271076524e-06,
      "loss": 75.7989,
      "step": 34550
    },
    {
      "epoch": 134.6321951219512,
      "grad_norm": 89.84574127197266,
      "learning_rate": 8.975097276264593e-06,
      "loss": 75.6769,
      "step": 34600
    },
    {
      "epoch": 134.6321951219512,
      "eval_loss": 1.204035758972168,
      "eval_perplexity": 3.333543300628662,
      "eval_runtime": 32.9377,
      "eval_samples_per_second": 60.205,
      "eval_steps_per_second": 7.529,
      "step": 34600
    },
    {
      "epoch": 134.82731707317072,
      "grad_norm": 66.88611602783203,
      "learning_rate": 8.988067444876785e-06,
      "loss": 75.6238,
      "step": 34650
    },
    {
      "epoch": 135.01951219512196,
      "grad_norm": 57.26847839355469,
      "learning_rate": 9.001037613488976e-06,
      "loss": 74.6099,
      "step": 34700
    },
    {
      "epoch": 135.21463414634147,
      "grad_norm": 62.380767822265625,
      "learning_rate": 9.014007782101168e-06,
      "loss": 75.7173,
      "step": 34750
    },
    {
      "epoch": 135.40975609756097,
      "grad_norm": 76.88412475585938,
      "learning_rate": 9.02697795071336e-06,
      "loss": 75.3733,
      "step": 34800
    },
    {
      "epoch": 135.40975609756097,
      "eval_loss": 1.2021373510360718,
      "eval_perplexity": 3.3272206783294678,
      "eval_runtime": 34.2509,
      "eval_samples_per_second": 57.896,
      "eval_steps_per_second": 7.241,
      "step": 34800
    },
    {
      "epoch": 135.60487804878048,
      "grad_norm": 69.05183410644531,
      "learning_rate": 9.039948119325552e-06,
      "loss": 75.693,
      "step": 34850
    },
    {
      "epoch": 135.8,
      "grad_norm": 49.828975677490234,
      "learning_rate": 9.052918287937744e-06,
      "loss": 75.4426,
      "step": 34900
    },
    {
      "epoch": 135.99512195121952,
      "grad_norm": 44.11347198486328,
      "learning_rate": 9.065888456549937e-06,
      "loss": 75.6717,
      "step": 34950
    },
    {
      "epoch": 136.18731707317073,
      "grad_norm": 75.53634643554688,
      "learning_rate": 9.078858625162129e-06,
      "loss": 74.5969,
      "step": 35000
    },
    {
      "epoch": 136.18731707317073,
      "eval_loss": 1.2022831439971924,
      "eval_perplexity": 3.3277058601379395,
      "eval_runtime": 33.5198,
      "eval_samples_per_second": 59.159,
      "eval_steps_per_second": 7.399,
      "step": 35000
    },
    {
      "epoch": 136.38243902439024,
      "grad_norm": 80.48794555664062,
      "learning_rate": 9.09182879377432e-06,
      "loss": 75.4891,
      "step": 35050
    },
    {
      "epoch": 136.57756097560974,
      "grad_norm": 86.08599853515625,
      "learning_rate": 9.104798962386511e-06,
      "loss": 75.4914,
      "step": 35100
    },
    {
      "epoch": 136.77268292682928,
      "grad_norm": 54.80598449707031,
      "learning_rate": 9.117769130998703e-06,
      "loss": 75.8353,
      "step": 35150
    },
    {
      "epoch": 136.96780487804878,
      "grad_norm": 48.15670394897461,
      "learning_rate": 9.130739299610895e-06,
      "loss": 75.367,
      "step": 35200
    },
    {
      "epoch": 136.96780487804878,
      "eval_loss": 1.2012407779693604,
      "eval_perplexity": 3.3242390155792236,
      "eval_runtime": 33.9172,
      "eval_samples_per_second": 58.466,
      "eval_steps_per_second": 7.312,
      "step": 35200
    },
    {
      "epoch": 137.16,
      "grad_norm": 45.58426284790039,
      "learning_rate": 9.143709468223088e-06,
      "loss": 74.0854,
      "step": 35250
    },
    {
      "epoch": 137.3551219512195,
      "grad_norm": 90.6800537109375,
      "learning_rate": 9.15667963683528e-06,
      "loss": 75.6804,
      "step": 35300
    },
    {
      "epoch": 137.55024390243904,
      "grad_norm": 71.54524230957031,
      "learning_rate": 9.169649805447472e-06,
      "loss": 75.6149,
      "step": 35350
    },
    {
      "epoch": 137.74536585365854,
      "grad_norm": 71.29147338867188,
      "learning_rate": 9.182619974059664e-06,
      "loss": 75.5851,
      "step": 35400
    },
    {
      "epoch": 137.74536585365854,
      "eval_loss": 1.200302243232727,
      "eval_perplexity": 3.321120500564575,
      "eval_runtime": 33.5711,
      "eval_samples_per_second": 59.069,
      "eval_steps_per_second": 7.387,
      "step": 35400
    },
    {
      "epoch": 137.94048780487805,
      "grad_norm": 67.52505493164062,
      "learning_rate": 9.195590142671856e-06,
      "loss": 75.1957,
      "step": 35450
    },
    {
      "epoch": 138.13268292682926,
      "grad_norm": 45.05027389526367,
      "learning_rate": 9.208560311284047e-06,
      "loss": 73.806,
      "step": 35500
    },
    {
      "epoch": 138.32780487804877,
      "grad_norm": 83.31101989746094,
      "learning_rate": 9.221530479896239e-06,
      "loss": 75.365,
      "step": 35550
    },
    {
      "epoch": 138.5229268292683,
      "grad_norm": 66.27106475830078,
      "learning_rate": 9.23450064850843e-06,
      "loss": 75.4708,
      "step": 35600
    },
    {
      "epoch": 138.5229268292683,
      "eval_loss": 1.2005572319030762,
      "eval_perplexity": 3.321967601776123,
      "eval_runtime": 33.9057,
      "eval_samples_per_second": 58.486,
      "eval_steps_per_second": 7.314,
      "step": 35600
    },
    {
      "epoch": 138.7180487804878,
      "grad_norm": 71.21336364746094,
      "learning_rate": 9.247470817120623e-06,
      "loss": 75.8235,
      "step": 35650
    },
    {
      "epoch": 138.9131707317073,
      "grad_norm": 51.261085510253906,
      "learning_rate": 9.260440985732815e-06,
      "loss": 75.4284,
      "step": 35700
    },
    {
      "epoch": 139.10536585365853,
      "grad_norm": 65.41361999511719,
      "learning_rate": 9.273411154345007e-06,
      "loss": 74.5501,
      "step": 35750
    },
    {
      "epoch": 139.30048780487806,
      "grad_norm": 43.80616760253906,
      "learning_rate": 9.2863813229572e-06,
      "loss": 75.7104,
      "step": 35800
    },
    {
      "epoch": 139.30048780487806,
      "eval_loss": 1.19959557056427,
      "eval_perplexity": 3.318774461746216,
      "eval_runtime": 34.1943,
      "eval_samples_per_second": 57.992,
      "eval_steps_per_second": 7.253,
      "step": 35800
    },
    {
      "epoch": 139.49560975609756,
      "grad_norm": 81.26734161376953,
      "learning_rate": 9.299351491569391e-06,
      "loss": 75.2708,
      "step": 35850
    },
    {
      "epoch": 139.69073170731707,
      "grad_norm": 69.29035949707031,
      "learning_rate": 9.312321660181584e-06,
      "loss": 75.2664,
      "step": 35900
    },
    {
      "epoch": 139.88585365853658,
      "grad_norm": 74.97532653808594,
      "learning_rate": 9.325291828793776e-06,
      "loss": 75.3031,
      "step": 35950
    },
    {
      "epoch": 140.0780487804878,
      "grad_norm": 52.20684814453125,
      "learning_rate": 9.338261997405968e-06,
      "loss": 73.9368,
      "step": 36000
    },
    {
      "epoch": 140.0780487804878,
      "eval_loss": 1.1990405321121216,
      "eval_perplexity": 3.3169329166412354,
      "eval_runtime": 32.7636,
      "eval_samples_per_second": 60.524,
      "eval_steps_per_second": 7.569,
      "step": 36000
    },
    {
      "epoch": 140.27317073170732,
      "grad_norm": 54.777732849121094,
      "learning_rate": 9.35123216601816e-06,
      "loss": 75.2139,
      "step": 36050
    },
    {
      "epoch": 140.46829268292683,
      "grad_norm": 73.30970764160156,
      "learning_rate": 9.364202334630352e-06,
      "loss": 75.1744,
      "step": 36100
    },
    {
      "epoch": 140.66341463414633,
      "grad_norm": 69.68060302734375,
      "learning_rate": 9.377172503242542e-06,
      "loss": 75.2051,
      "step": 36150
    },
    {
      "epoch": 140.85853658536584,
      "grad_norm": 77.59161376953125,
      "learning_rate": 9.390142671854735e-06,
      "loss": 75.9184,
      "step": 36200
    },
    {
      "epoch": 140.85853658536584,
      "eval_loss": 1.1987650394439697,
      "eval_perplexity": 3.316019296646118,
      "eval_runtime": 31.7996,
      "eval_samples_per_second": 62.359,
      "eval_steps_per_second": 7.799,
      "step": 36200
    },
    {
      "epoch": 141.05073170731708,
      "grad_norm": 41.58888244628906,
      "learning_rate": 9.403112840466927e-06,
      "loss": 73.7339,
      "step": 36250
    },
    {
      "epoch": 141.2458536585366,
      "grad_norm": 61.662506103515625,
      "learning_rate": 9.416083009079119e-06,
      "loss": 75.3056,
      "step": 36300
    },
    {
      "epoch": 141.4409756097561,
      "grad_norm": 43.71986389160156,
      "learning_rate": 9.429053177691311e-06,
      "loss": 75.2384,
      "step": 36350
    },
    {
      "epoch": 141.6360975609756,
      "grad_norm": 41.309242248535156,
      "learning_rate": 9.442023346303503e-06,
      "loss": 75.303,
      "step": 36400
    },
    {
      "epoch": 141.6360975609756,
      "eval_loss": 1.1976289749145508,
      "eval_perplexity": 3.3122541904449463,
      "eval_runtime": 34.4474,
      "eval_samples_per_second": 57.566,
      "eval_steps_per_second": 7.199,
      "step": 36400
    },
    {
      "epoch": 141.83121951219513,
      "grad_norm": 59.88702392578125,
      "learning_rate": 9.454993514915695e-06,
      "loss": 75.4883,
      "step": 36450
    },
    {
      "epoch": 142.02341463414635,
      "grad_norm": 50.58732986450195,
      "learning_rate": 9.467963683527887e-06,
      "loss": 74.0341,
      "step": 36500
    },
    {
      "epoch": 142.21853658536585,
      "grad_norm": 65.037109375,
      "learning_rate": 9.480933852140078e-06,
      "loss": 75.387,
      "step": 36550
    },
    {
      "epoch": 142.41365853658536,
      "grad_norm": 72.72785186767578,
      "learning_rate": 9.49390402075227e-06,
      "loss": 75.1023,
      "step": 36600
    },
    {
      "epoch": 142.41365853658536,
      "eval_loss": 1.1984457969665527,
      "eval_perplexity": 3.3149607181549072,
      "eval_runtime": 33.3165,
      "eval_samples_per_second": 59.52,
      "eval_steps_per_second": 7.444,
      "step": 36600
    },
    {
      "epoch": 142.60878048780486,
      "grad_norm": 67.27557373046875,
      "learning_rate": 9.506874189364462e-06,
      "loss": 75.0503,
      "step": 36650
    },
    {
      "epoch": 142.8039024390244,
      "grad_norm": 56.7576789855957,
      "learning_rate": 9.519844357976654e-06,
      "loss": 75.3736,
      "step": 36700
    },
    {
      "epoch": 142.9990243902439,
      "grad_norm": 51.66471481323242,
      "learning_rate": 9.532814526588846e-06,
      "loss": 75.0484,
      "step": 36750
    },
    {
      "epoch": 143.19121951219512,
      "grad_norm": 71.99359130859375,
      "learning_rate": 9.545784695201038e-06,
      "loss": 74.8432,
      "step": 36800
    },
    {
      "epoch": 143.19121951219512,
      "eval_loss": 1.1959505081176758,
      "eval_perplexity": 3.306699275970459,
      "eval_runtime": 32.703,
      "eval_samples_per_second": 60.637,
      "eval_steps_per_second": 7.583,
      "step": 36800
    },
    {
      "epoch": 143.38634146341462,
      "grad_norm": 74.6788558959961,
      "learning_rate": 9.55875486381323e-06,
      "loss": 74.5367,
      "step": 36850
    },
    {
      "epoch": 143.58146341463416,
      "grad_norm": 45.83584976196289,
      "learning_rate": 9.571725032425423e-06,
      "loss": 75.2478,
      "step": 36900
    },
    {
      "epoch": 143.77658536585366,
      "grad_norm": 56.058815002441406,
      "learning_rate": 9.584695201037613e-06,
      "loss": 75.296,
      "step": 36950
    },
    {
      "epoch": 143.97170731707317,
      "grad_norm": 62.41201400756836,
      "learning_rate": 9.597665369649807e-06,
      "loss": 74.6048,
      "step": 37000
    },
    {
      "epoch": 143.97170731707317,
      "eval_loss": 1.19489324092865,
      "eval_perplexity": 3.3032050132751465,
      "eval_runtime": 32.4139,
      "eval_samples_per_second": 61.177,
      "eval_steps_per_second": 7.651,
      "step": 37000
    },
    {
      "epoch": 144.16390243902438,
      "grad_norm": 67.35426330566406,
      "learning_rate": 9.610635538261999e-06,
      "loss": 73.8054,
      "step": 37050
    },
    {
      "epoch": 144.3590243902439,
      "grad_norm": 58.206993103027344,
      "learning_rate": 9.623605706874191e-06,
      "loss": 75.3349,
      "step": 37100
    },
    {
      "epoch": 144.55414634146342,
      "grad_norm": 100.56554412841797,
      "learning_rate": 9.636575875486383e-06,
      "loss": 75.2285,
      "step": 37150
    },
    {
      "epoch": 144.74926829268293,
      "grad_norm": 62.74109649658203,
      "learning_rate": 9.649546044098574e-06,
      "loss": 75.0637,
      "step": 37200
    },
    {
      "epoch": 144.74926829268293,
      "eval_loss": 1.1950002908706665,
      "eval_perplexity": 3.303558826446533,
      "eval_runtime": 33.3186,
      "eval_samples_per_second": 59.516,
      "eval_steps_per_second": 7.443,
      "step": 37200
    },
    {
      "epoch": 144.94439024390243,
      "grad_norm": 58.457698822021484,
      "learning_rate": 9.662516212710766e-06,
      "loss": 75.0395,
      "step": 37250
    },
    {
      "epoch": 145.13658536585365,
      "grad_norm": 44.417884826660156,
      "learning_rate": 9.675486381322958e-06,
      "loss": 73.6001,
      "step": 37300
    },
    {
      "epoch": 145.33170731707318,
      "grad_norm": 76.74977111816406,
      "learning_rate": 9.68845654993515e-06,
      "loss": 75.2532,
      "step": 37350
    },
    {
      "epoch": 145.5268292682927,
      "grad_norm": 58.34923553466797,
      "learning_rate": 9.701426718547342e-06,
      "loss": 75.5329,
      "step": 37400
    },
    {
      "epoch": 145.5268292682927,
      "eval_loss": 1.1936415433883667,
      "eval_perplexity": 3.2990729808807373,
      "eval_runtime": 34.3665,
      "eval_samples_per_second": 57.702,
      "eval_steps_per_second": 7.216,
      "step": 37400
    },
    {
      "epoch": 145.7219512195122,
      "grad_norm": 73.609130859375,
      "learning_rate": 9.714396887159534e-06,
      "loss": 74.8045,
      "step": 37450
    },
    {
      "epoch": 145.9170731707317,
      "grad_norm": 76.67505645751953,
      "learning_rate": 9.727367055771726e-06,
      "loss": 74.945,
      "step": 37500
    },
    {
      "epoch": 146.1092682926829,
      "grad_norm": 57.50325393676758,
      "learning_rate": 9.740337224383919e-06,
      "loss": 73.586,
      "step": 37550
    },
    {
      "epoch": 146.30439024390245,
      "grad_norm": 61.30116653442383,
      "learning_rate": 9.753307392996109e-06,
      "loss": 74.8572,
      "step": 37600
    },
    {
      "epoch": 146.30439024390245,
      "eval_loss": 1.1928751468658447,
      "eval_perplexity": 3.2965457439422607,
      "eval_runtime": 35.1321,
      "eval_samples_per_second": 56.444,
      "eval_steps_per_second": 7.059,
      "step": 37600
    },
    {
      "epoch": 146.49951219512195,
      "grad_norm": 73.67691802978516,
      "learning_rate": 9.766277561608301e-06,
      "loss": 74.9008,
      "step": 37650
    },
    {
      "epoch": 146.69463414634146,
      "grad_norm": 78.16860961914062,
      "learning_rate": 9.779247730220493e-06,
      "loss": 75.188,
      "step": 37700
    },
    {
      "epoch": 146.88975609756096,
      "grad_norm": 66.43135833740234,
      "learning_rate": 9.792217898832685e-06,
      "loss": 74.3831,
      "step": 37750
    },
    {
      "epoch": 147.0819512195122,
      "grad_norm": 74.62745666503906,
      "learning_rate": 9.805188067444878e-06,
      "loss": 74.7052,
      "step": 37800
    },
    {
      "epoch": 147.0819512195122,
      "eval_loss": 1.1931179761886597,
      "eval_perplexity": 3.297346353530884,
      "eval_runtime": 32.6413,
      "eval_samples_per_second": 60.751,
      "eval_steps_per_second": 7.598,
      "step": 37800
    },
    {
      "epoch": 147.2770731707317,
      "grad_norm": 50.29746627807617,
      "learning_rate": 9.81815823605707e-06,
      "loss": 74.8164,
      "step": 37850
    },
    {
      "epoch": 147.47219512195122,
      "grad_norm": 52.71171951293945,
      "learning_rate": 9.831128404669262e-06,
      "loss": 75.217,
      "step": 37900
    },
    {
      "epoch": 147.66731707317072,
      "grad_norm": 56.09222412109375,
      "learning_rate": 9.844098573281454e-06,
      "loss": 74.7423,
      "step": 37950
    },
    {
      "epoch": 147.86243902439026,
      "grad_norm": 58.77735900878906,
      "learning_rate": 9.857068741893644e-06,
      "loss": 74.4403,
      "step": 38000
    },
    {
      "epoch": 147.86243902439026,
      "eval_loss": 1.192169189453125,
      "eval_perplexity": 3.2942192554473877,
      "eval_runtime": 34.4656,
      "eval_samples_per_second": 57.536,
      "eval_steps_per_second": 7.196,
      "step": 38000
    },
    {
      "epoch": 148.05463414634147,
      "grad_norm": 67.772705078125,
      "learning_rate": 9.870038910505836e-06,
      "loss": 74.2346,
      "step": 38050
    },
    {
      "epoch": 148.24975609756098,
      "grad_norm": 56.224857330322266,
      "learning_rate": 9.883009079118029e-06,
      "loss": 74.5572,
      "step": 38100
    },
    {
      "epoch": 148.44487804878048,
      "grad_norm": 53.71895980834961,
      "learning_rate": 9.89597924773022e-06,
      "loss": 74.9233,
      "step": 38150
    },
    {
      "epoch": 148.64,
      "grad_norm": 58.67127227783203,
      "learning_rate": 9.908949416342415e-06,
      "loss": 74.9777,
      "step": 38200
    },
    {
      "epoch": 148.64,
      "eval_loss": 1.1923474073410034,
      "eval_perplexity": 3.294806480407715,
      "eval_runtime": 33.0101,
      "eval_samples_per_second": 60.073,
      "eval_steps_per_second": 7.513,
      "step": 38200
    },
    {
      "epoch": 148.83512195121952,
      "grad_norm": 77.46047973632812,
      "learning_rate": 9.921919584954605e-06,
      "loss": 74.9914,
      "step": 38250
    },
    {
      "epoch": 149.02731707317074,
      "grad_norm": 52.9017333984375,
      "learning_rate": 9.934889753566797e-06,
      "loss": 73.7936,
      "step": 38300
    },
    {
      "epoch": 149.22243902439024,
      "grad_norm": 61.42400360107422,
      "learning_rate": 9.94785992217899e-06,
      "loss": 74.7459,
      "step": 38350
    },
    {
      "epoch": 149.41756097560975,
      "grad_norm": 49.23053741455078,
      "learning_rate": 9.960830090791181e-06,
      "loss": 74.7336,
      "step": 38400
    },
    {
      "epoch": 149.41756097560975,
      "eval_loss": 1.1901696920394897,
      "eval_perplexity": 3.2876391410827637,
      "eval_runtime": 32.2817,
      "eval_samples_per_second": 61.428,
      "eval_steps_per_second": 7.682,
      "step": 38400
    },
    {
      "epoch": 149.61268292682928,
      "grad_norm": 46.5675048828125,
      "learning_rate": 9.973800259403374e-06,
      "loss": 74.6593,
      "step": 38450
    },
    {
      "epoch": 149.80780487804878,
      "grad_norm": 53.99067687988281,
      "learning_rate": 9.986770428015566e-06,
      "loss": 75.1225,
      "step": 38500
    },
    {
      "epoch": 150.0,
      "grad_norm": 11.030900001525879,
      "learning_rate": 9.999740596627758e-06,
      "loss": 73.5435,
      "step": 38550
    },
    {
      "epoch": 150.1951219512195,
      "grad_norm": 62.74857711791992,
      "learning_rate": 1.0012710765239948e-05,
      "loss": 74.7123,
      "step": 38600
    },
    {
      "epoch": 150.1951219512195,
      "eval_loss": 1.1896499395370483,
      "eval_perplexity": 3.285930633544922,
      "eval_runtime": 34.2301,
      "eval_samples_per_second": 57.931,
      "eval_steps_per_second": 7.245,
      "step": 38600
    },
    {
      "epoch": 150.390243902439,
      "grad_norm": 69.34379577636719,
      "learning_rate": 1.002568093385214e-05,
      "loss": 75.2007,
      "step": 38650
    },
    {
      "epoch": 150.58536585365854,
      "grad_norm": 48.33938217163086,
      "learning_rate": 1.0038651102464332e-05,
      "loss": 74.7113,
      "step": 38700
    },
    {
      "epoch": 150.78048780487805,
      "grad_norm": 75.58338165283203,
      "learning_rate": 1.0051621271076525e-05,
      "loss": 74.6157,
      "step": 38750
    },
    {
      "epoch": 150.97560975609755,
      "grad_norm": 48.849761962890625,
      "learning_rate": 1.0064591439688718e-05,
      "loss": 74.5412,
      "step": 38800
    },
    {
      "epoch": 150.97560975609755,
      "eval_loss": 1.1888867616653442,
      "eval_perplexity": 3.283423900604248,
      "eval_runtime": 35.1563,
      "eval_samples_per_second": 56.405,
      "eval_steps_per_second": 7.054,
      "step": 38800
    },
    {
      "epoch": 151.16780487804877,
      "grad_norm": 60.40437316894531,
      "learning_rate": 1.007756160830091e-05,
      "loss": 73.477,
      "step": 38850
    },
    {
      "epoch": 151.3629268292683,
      "grad_norm": 62.70173263549805,
      "learning_rate": 1.0090531776913101e-05,
      "loss": 74.5425,
      "step": 38900
    },
    {
      "epoch": 151.5580487804878,
      "grad_norm": 97.3697738647461,
      "learning_rate": 1.0103501945525293e-05,
      "loss": 75.3634,
      "step": 38950
    },
    {
      "epoch": 151.7531707317073,
      "grad_norm": 68.08174896240234,
      "learning_rate": 1.0116472114137485e-05,
      "loss": 74.268,
      "step": 39000
    },
    {
      "epoch": 151.7531707317073,
      "eval_loss": 1.1892846822738647,
      "eval_perplexity": 3.2847306728363037,
      "eval_runtime": 32.1219,
      "eval_samples_per_second": 61.733,
      "eval_steps_per_second": 7.721,
      "step": 39000
    },
    {
      "epoch": 151.94829268292682,
      "grad_norm": 57.087337493896484,
      "learning_rate": 1.0129442282749677e-05,
      "loss": 74.5654,
      "step": 39050
    },
    {
      "epoch": 152.14048780487803,
      "grad_norm": 60.662078857421875,
      "learning_rate": 1.014241245136187e-05,
      "loss": 73.6301,
      "step": 39100
    },
    {
      "epoch": 152.33560975609757,
      "grad_norm": 59.44186019897461,
      "learning_rate": 1.0155382619974062e-05,
      "loss": 74.7096,
      "step": 39150
    },
    {
      "epoch": 152.53073170731707,
      "grad_norm": 63.45914840698242,
      "learning_rate": 1.0168352788586254e-05,
      "loss": 74.3316,
      "step": 39200
    },
    {
      "epoch": 152.53073170731707,
      "eval_loss": 1.1904985904693604,
      "eval_perplexity": 3.2887206077575684,
      "eval_runtime": 32.4609,
      "eval_samples_per_second": 61.089,
      "eval_steps_per_second": 7.64,
      "step": 39200
    },
    {
      "epoch": 152.72585365853658,
      "grad_norm": 36.952423095703125,
      "learning_rate": 1.0181322957198446e-05,
      "loss": 74.3444,
      "step": 39250
    },
    {
      "epoch": 152.92097560975608,
      "grad_norm": 60.2757682800293,
      "learning_rate": 1.0194293125810636e-05,
      "loss": 75.3779,
      "step": 39300
    },
    {
      "epoch": 153.11317073170733,
      "grad_norm": 55.91020202636719,
      "learning_rate": 1.0207263294422828e-05,
      "loss": 73.1454,
      "step": 39350
    },
    {
      "epoch": 153.30829268292683,
      "grad_norm": 52.86976623535156,
      "learning_rate": 1.022023346303502e-05,
      "loss": 74.778,
      "step": 39400
    },
    {
      "epoch": 153.30829268292683,
      "eval_loss": 1.1874616146087646,
      "eval_perplexity": 3.278747797012329,
      "eval_runtime": 32.9532,
      "eval_samples_per_second": 60.176,
      "eval_steps_per_second": 7.526,
      "step": 39400
    },
    {
      "epoch": 153.50341463414634,
      "grad_norm": 67.87635040283203,
      "learning_rate": 1.0233203631647213e-05,
      "loss": 74.5904,
      "step": 39450
    },
    {
      "epoch": 153.69853658536584,
      "grad_norm": 63.671485900878906,
      "learning_rate": 1.0246173800259405e-05,
      "loss": 74.8152,
      "step": 39500
    },
    {
      "epoch": 153.89365853658538,
      "grad_norm": 54.209720611572266,
      "learning_rate": 1.0259143968871597e-05,
      "loss": 74.7127,
      "step": 39550
    },
    {
      "epoch": 154.0858536585366,
      "grad_norm": 59.879310607910156,
      "learning_rate": 1.0272114137483789e-05,
      "loss": 73.2728,
      "step": 39600
    },
    {
      "epoch": 154.0858536585366,
      "eval_loss": 1.1862345933914185,
      "eval_perplexity": 3.2747273445129395,
      "eval_runtime": 31.9477,
      "eval_samples_per_second": 62.07,
      "eval_steps_per_second": 7.763,
      "step": 39600
    },
    {
      "epoch": 154.2809756097561,
      "grad_norm": 71.04533386230469,
      "learning_rate": 1.0285084306095981e-05,
      "loss": 74.5177,
      "step": 39650
    },
    {
      "epoch": 154.4760975609756,
      "grad_norm": 68.56959533691406,
      "learning_rate": 1.0298054474708172e-05,
      "loss": 74.2932,
      "step": 39700
    },
    {
      "epoch": 154.6712195121951,
      "grad_norm": 63.76231384277344,
      "learning_rate": 1.0311024643320364e-05,
      "loss": 74.7889,
      "step": 39750
    },
    {
      "epoch": 154.86634146341464,
      "grad_norm": 47.679447174072266,
      "learning_rate": 1.0323994811932556e-05,
      "loss": 74.5731,
      "step": 39800
    },
    {
      "epoch": 154.86634146341464,
      "eval_loss": 1.1848976612091064,
      "eval_perplexity": 3.2703521251678467,
      "eval_runtime": 32.0906,
      "eval_samples_per_second": 61.794,
      "eval_steps_per_second": 7.728,
      "step": 39800
    },
    {
      "epoch": 155.05853658536586,
      "grad_norm": 71.05780029296875,
      "learning_rate": 1.0336964980544748e-05,
      "loss": 73.0919,
      "step": 39850
    },
    {
      "epoch": 155.25365853658536,
      "grad_norm": 44.036163330078125,
      "learning_rate": 1.034993514915694e-05,
      "loss": 74.8187,
      "step": 39900
    },
    {
      "epoch": 155.44878048780487,
      "grad_norm": 55.658302307128906,
      "learning_rate": 1.0362905317769132e-05,
      "loss": 74.423,
      "step": 39950
    },
    {
      "epoch": 155.6439024390244,
      "grad_norm": 40.43292999267578,
      "learning_rate": 1.0375875486381324e-05,
      "loss": 74.5604,
      "step": 40000
    },
    {
      "epoch": 155.6439024390244,
      "eval_loss": 1.1844534873962402,
      "eval_perplexity": 3.268899917602539,
      "eval_runtime": 32.6083,
      "eval_samples_per_second": 60.813,
      "eval_steps_per_second": 7.605,
      "step": 40000
    },
    {
      "epoch": 155.8390243902439,
      "grad_norm": 41.900840759277344,
      "learning_rate": 1.0388845654993516e-05,
      "loss": 74.5649,
      "step": 40050
    },
    {
      "epoch": 156.03121951219512,
      "grad_norm": 51.548641204833984,
      "learning_rate": 1.0401815823605707e-05,
      "loss": 73.1925,
      "step": 40100
    },
    {
      "epoch": 156.22634146341463,
      "grad_norm": 66.27032470703125,
      "learning_rate": 1.0414785992217899e-05,
      "loss": 74.5105,
      "step": 40150
    },
    {
      "epoch": 156.42146341463413,
      "grad_norm": 72.78648376464844,
      "learning_rate": 1.0427756160830091e-05,
      "loss": 74.5292,
      "step": 40200
    },
    {
      "epoch": 156.42146341463413,
      "eval_loss": 1.183874249458313,
      "eval_perplexity": 3.2670068740844727,
      "eval_runtime": 32.859,
      "eval_samples_per_second": 60.349,
      "eval_steps_per_second": 7.547,
      "step": 40200
    },
    {
      "epoch": 156.61658536585367,
      "grad_norm": 45.839881896972656,
      "learning_rate": 1.0440726329442283e-05,
      "loss": 74.11,
      "step": 40250
    },
    {
      "epoch": 156.81170731707317,
      "grad_norm": 45.1749267578125,
      "learning_rate": 1.0453696498054475e-05,
      "loss": 74.7333,
      "step": 40300
    },
    {
      "epoch": 157.0039024390244,
      "grad_norm": 79.10873413085938,
      "learning_rate": 1.0466666666666668e-05,
      "loss": 73.0794,
      "step": 40350
    },
    {
      "epoch": 157.1990243902439,
      "grad_norm": 62.58580780029297,
      "learning_rate": 1.047963683527886e-05,
      "loss": 74.8749,
      "step": 40400
    },
    {
      "epoch": 157.1990243902439,
      "eval_loss": 1.1829452514648438,
      "eval_perplexity": 3.2639732360839844,
      "eval_runtime": 32.6874,
      "eval_samples_per_second": 60.666,
      "eval_steps_per_second": 7.587,
      "step": 40400
    },
    {
      "epoch": 157.39414634146343,
      "grad_norm": 54.85337829589844,
      "learning_rate": 1.0492607003891052e-05,
      "loss": 73.8928,
      "step": 40450
    },
    {
      "epoch": 157.58926829268293,
      "grad_norm": 95.65311431884766,
      "learning_rate": 1.0505577172503242e-05,
      "loss": 74.2274,
      "step": 40500
    },
    {
      "epoch": 157.78439024390244,
      "grad_norm": 61.842445373535156,
      "learning_rate": 1.0518547341115434e-05,
      "loss": 74.6175,
      "step": 40550
    },
    {
      "epoch": 157.97951219512194,
      "grad_norm": 114.61388397216797,
      "learning_rate": 1.0531517509727626e-05,
      "loss": 74.0694,
      "step": 40600
    },
    {
      "epoch": 157.97951219512194,
      "eval_loss": 1.183345913887024,
      "eval_perplexity": 3.2652812004089355,
      "eval_runtime": 33.1462,
      "eval_samples_per_second": 59.826,
      "eval_steps_per_second": 7.482,
      "step": 40600
    },
    {
      "epoch": 158.17170731707316,
      "grad_norm": 41.63867950439453,
      "learning_rate": 1.0544487678339819e-05,
      "loss": 73.4243,
      "step": 40650
    },
    {
      "epoch": 158.3668292682927,
      "grad_norm": 88.4104995727539,
      "learning_rate": 1.055745784695201e-05,
      "loss": 74.1539,
      "step": 40700
    },
    {
      "epoch": 158.5619512195122,
      "grad_norm": 54.90100860595703,
      "learning_rate": 1.0570428015564203e-05,
      "loss": 74.4486,
      "step": 40750
    },
    {
      "epoch": 158.7570731707317,
      "grad_norm": 53.95219421386719,
      "learning_rate": 1.0583398184176395e-05,
      "loss": 74.3045,
      "step": 40800
    },
    {
      "epoch": 158.7570731707317,
      "eval_loss": 1.1825929880142212,
      "eval_perplexity": 3.2628238201141357,
      "eval_runtime": 33.0859,
      "eval_samples_per_second": 59.935,
      "eval_steps_per_second": 7.496,
      "step": 40800
    },
    {
      "epoch": 158.9521951219512,
      "grad_norm": 57.46405792236328,
      "learning_rate": 1.0596368352788587e-05,
      "loss": 74.1684,
      "step": 40850
    },
    {
      "epoch": 159.14439024390245,
      "grad_norm": 57.17987823486328,
      "learning_rate": 1.0609338521400778e-05,
      "loss": 73.3719,
      "step": 40900
    },
    {
      "epoch": 159.33951219512196,
      "grad_norm": 60.00437927246094,
      "learning_rate": 1.062230869001297e-05,
      "loss": 74.5022,
      "step": 40950
    },
    {
      "epoch": 159.53463414634146,
      "grad_norm": 88.30225372314453,
      "learning_rate": 1.0635278858625162e-05,
      "loss": 74.4282,
      "step": 41000
    },
    {
      "epoch": 159.53463414634146,
      "eval_loss": 1.1811262369155884,
      "eval_perplexity": 3.2580413818359375,
      "eval_runtime": 31.7937,
      "eval_samples_per_second": 62.371,
      "eval_steps_per_second": 7.8,
      "step": 41000
    },
    {
      "epoch": 159.72975609756097,
      "grad_norm": 60.84225082397461,
      "learning_rate": 1.0648249027237354e-05,
      "loss": 73.8828,
      "step": 41050
    },
    {
      "epoch": 159.9248780487805,
      "grad_norm": 89.35126495361328,
      "learning_rate": 1.0661219195849546e-05,
      "loss": 74.0434,
      "step": 41100
    },
    {
      "epoch": 160.11707317073171,
      "grad_norm": 69.27432250976562,
      "learning_rate": 1.0674189364461738e-05,
      "loss": 73.6898,
      "step": 41150
    },
    {
      "epoch": 160.31219512195122,
      "grad_norm": 60.25503921508789,
      "learning_rate": 1.068715953307393e-05,
      "loss": 74.0147,
      "step": 41200
    },
    {
      "epoch": 160.31219512195122,
      "eval_loss": 1.181908369064331,
      "eval_perplexity": 3.2605907917022705,
      "eval_runtime": 40.8012,
      "eval_samples_per_second": 48.602,
      "eval_steps_per_second": 6.078,
      "step": 41200
    },
    {
      "epoch": 160.50731707317073,
      "grad_norm": 79.39530181884766,
      "learning_rate": 1.0700129701686124e-05,
      "loss": 74.2412,
      "step": 41250
    },
    {
      "epoch": 160.70243902439023,
      "grad_norm": 64.65458679199219,
      "learning_rate": 1.0713099870298316e-05,
      "loss": 73.7632,
      "step": 41300
    },
    {
      "epoch": 160.89756097560976,
      "grad_norm": 67.12086486816406,
      "learning_rate": 1.0726070038910508e-05,
      "loss": 74.5567,
      "step": 41350
    },
    {
      "epoch": 161.08975609756098,
      "grad_norm": 68.89734649658203,
      "learning_rate": 1.0739040207522699e-05,
      "loss": 72.772,
      "step": 41400
    },
    {
      "epoch": 161.08975609756098,
      "eval_loss": 1.1804906129837036,
      "eval_perplexity": 3.2559711933135986,
      "eval_runtime": 33.1292,
      "eval_samples_per_second": 59.857,
      "eval_steps_per_second": 7.486,
      "step": 41400
    },
    {
      "epoch": 161.28487804878048,
      "grad_norm": 63.18095779418945,
      "learning_rate": 1.0752010376134891e-05,
      "loss": 74.5304,
      "step": 41450
    },
    {
      "epoch": 161.48,
      "grad_norm": 78.2070083618164,
      "learning_rate": 1.0764980544747083e-05,
      "loss": 73.9497,
      "step": 41500
    },
    {
      "epoch": 161.67512195121952,
      "grad_norm": 54.9095344543457,
      "learning_rate": 1.0777950713359275e-05,
      "loss": 74.2459,
      "step": 41550
    },
    {
      "epoch": 161.87024390243903,
      "grad_norm": 72.1739730834961,
      "learning_rate": 1.0790920881971467e-05,
      "loss": 74.1362,
      "step": 41600
    },
    {
      "epoch": 161.87024390243903,
      "eval_loss": 1.1800636053085327,
      "eval_perplexity": 3.2545812129974365,
      "eval_runtime": 32.256,
      "eval_samples_per_second": 61.477,
      "eval_steps_per_second": 7.688,
      "step": 41600
    },
    {
      "epoch": 162.06243902439024,
      "grad_norm": 56.43564224243164,
      "learning_rate": 1.080389105058366e-05,
      "loss": 73.2334,
      "step": 41650
    },
    {
      "epoch": 162.25756097560975,
      "grad_norm": 58.27039337158203,
      "learning_rate": 1.0816861219195852e-05,
      "loss": 74.1562,
      "step": 41700
    },
    {
      "epoch": 162.45268292682925,
      "grad_norm": 51.977134704589844,
      "learning_rate": 1.0829831387808044e-05,
      "loss": 74.2139,
      "step": 41750
    },
    {
      "epoch": 162.6478048780488,
      "grad_norm": 41.77775573730469,
      "learning_rate": 1.0842801556420234e-05,
      "loss": 73.9235,
      "step": 41800
    },
    {
      "epoch": 162.6478048780488,
      "eval_loss": 1.1800416707992554,
      "eval_perplexity": 3.254509925842285,
      "eval_runtime": 32.4038,
      "eval_samples_per_second": 61.197,
      "eval_steps_per_second": 7.653,
      "step": 41800
    },
    {
      "epoch": 162.8429268292683,
      "grad_norm": 55.069889068603516,
      "learning_rate": 1.0855771725032426e-05,
      "loss": 74.1299,
      "step": 41850
    },
    {
      "epoch": 163.0351219512195,
      "grad_norm": 39.02568435668945,
      "learning_rate": 1.0868741893644618e-05,
      "loss": 72.9843,
      "step": 41900
    },
    {
      "epoch": 163.230243902439,
      "grad_norm": 74.1158676147461,
      "learning_rate": 1.088171206225681e-05,
      "loss": 74.1957,
      "step": 41950
    },
    {
      "epoch": 163.42536585365855,
      "grad_norm": 59.074684143066406,
      "learning_rate": 1.0894682230869003e-05,
      "loss": 74.3327,
      "step": 42000
    },
    {
      "epoch": 163.42536585365855,
      "eval_loss": 1.178655743598938,
      "eval_perplexity": 3.250002384185791,
      "eval_runtime": 32.2734,
      "eval_samples_per_second": 61.444,
      "eval_steps_per_second": 7.684,
      "step": 42000
    },
    {
      "epoch": 163.62048780487805,
      "grad_norm": 56.1364860534668,
      "learning_rate": 1.0907652399481195e-05,
      "loss": 73.9713,
      "step": 42050
    },
    {
      "epoch": 163.81560975609756,
      "grad_norm": 47.17007827758789,
      "learning_rate": 1.0920622568093387e-05,
      "loss": 74.3647,
      "step": 42100
    },
    {
      "epoch": 164.00780487804877,
      "grad_norm": 101.2228775024414,
      "learning_rate": 1.0933592736705579e-05,
      "loss": 72.1901,
      "step": 42150
    },
    {
      "epoch": 164.20292682926828,
      "grad_norm": 47.8951530456543,
      "learning_rate": 1.094656290531777e-05,
      "loss": 73.7766,
      "step": 42200
    },
    {
      "epoch": 164.20292682926828,
      "eval_loss": 1.1774638891220093,
      "eval_perplexity": 3.246131181716919,
      "eval_runtime": 32.1449,
      "eval_samples_per_second": 61.689,
      "eval_steps_per_second": 7.715,
      "step": 42200
    },
    {
      "epoch": 164.3980487804878,
      "grad_norm": 64.54574584960938,
      "learning_rate": 1.0959533073929962e-05,
      "loss": 74.2668,
      "step": 42250
    },
    {
      "epoch": 164.59317073170732,
      "grad_norm": 64.31012725830078,
      "learning_rate": 1.0972503242542154e-05,
      "loss": 74.2869,
      "step": 42300
    },
    {
      "epoch": 164.78829268292682,
      "grad_norm": 55.58699417114258,
      "learning_rate": 1.0985473411154346e-05,
      "loss": 73.6729,
      "step": 42350
    },
    {
      "epoch": 164.98341463414633,
      "grad_norm": 62.20510482788086,
      "learning_rate": 1.0998443579766538e-05,
      "loss": 73.9057,
      "step": 42400
    },
    {
      "epoch": 164.98341463414633,
      "eval_loss": 1.1768659353256226,
      "eval_perplexity": 3.2441906929016113,
      "eval_runtime": 34.4426,
      "eval_samples_per_second": 57.574,
      "eval_steps_per_second": 7.2,
      "step": 42400
    },
    {
      "epoch": 165.17560975609757,
      "grad_norm": 49.60068130493164,
      "learning_rate": 1.101141374837873e-05,
      "loss": 72.8785,
      "step": 42450
    },
    {
      "epoch": 165.37073170731708,
      "grad_norm": 53.96706008911133,
      "learning_rate": 1.1024383916990922e-05,
      "loss": 74.0739,
      "step": 42500
    },
    {
      "epoch": 165.56585365853658,
      "grad_norm": 72.28929138183594,
      "learning_rate": 1.1037354085603114e-05,
      "loss": 73.8805,
      "step": 42550
    },
    {
      "epoch": 165.7609756097561,
      "grad_norm": 63.342342376708984,
      "learning_rate": 1.1050324254215305e-05,
      "loss": 73.5355,
      "step": 42600
    },
    {
      "epoch": 165.7609756097561,
      "eval_loss": 1.17587149143219,
      "eval_perplexity": 3.2409660816192627,
      "eval_runtime": 32.4146,
      "eval_samples_per_second": 61.176,
      "eval_steps_per_second": 7.651,
      "step": 42600
    },
    {
      "epoch": 165.95609756097562,
      "grad_norm": 59.656429290771484,
      "learning_rate": 1.1063294422827497e-05,
      "loss": 74.3963,
      "step": 42650
    },
    {
      "epoch": 166.14829268292684,
      "grad_norm": 85.549072265625,
      "learning_rate": 1.1076264591439689e-05,
      "loss": 72.9143,
      "step": 42700
    },
    {
      "epoch": 166.34341463414634,
      "grad_norm": 73.30560302734375,
      "learning_rate": 1.1089234760051881e-05,
      "loss": 73.6495,
      "step": 42750
    },
    {
      "epoch": 166.53853658536585,
      "grad_norm": 57.6217155456543,
      "learning_rate": 1.1102204928664073e-05,
      "loss": 74.0046,
      "step": 42800
    },
    {
      "epoch": 166.53853658536585,
      "eval_loss": 1.1756974458694458,
      "eval_perplexity": 3.2404022216796875,
      "eval_runtime": 37.0655,
      "eval_samples_per_second": 53.5,
      "eval_steps_per_second": 6.691,
      "step": 42800
    },
    {
      "epoch": 166.73365853658535,
      "grad_norm": 64.71324157714844,
      "learning_rate": 1.1115175097276265e-05,
      "loss": 73.7915,
      "step": 42850
    },
    {
      "epoch": 166.9287804878049,
      "grad_norm": 60.19735336303711,
      "learning_rate": 1.1128145265888458e-05,
      "loss": 73.908,
      "step": 42900
    },
    {
      "epoch": 167.1209756097561,
      "grad_norm": 72.48501586914062,
      "learning_rate": 1.114111543450065e-05,
      "loss": 73.4906,
      "step": 42950
    },
    {
      "epoch": 167.3160975609756,
      "grad_norm": 69.6637954711914,
      "learning_rate": 1.115408560311284e-05,
      "loss": 73.8423,
      "step": 43000
    },
    {
      "epoch": 167.3160975609756,
      "eval_loss": 1.1749030351638794,
      "eval_perplexity": 3.2378289699554443,
      "eval_runtime": 33.7813,
      "eval_samples_per_second": 58.701,
      "eval_steps_per_second": 7.341,
      "step": 43000
    },
    {
      "epoch": 167.5112195121951,
      "grad_norm": 62.7782096862793,
      "learning_rate": 1.1167055771725032e-05,
      "loss": 73.5626,
      "step": 43050
    },
    {
      "epoch": 167.70634146341465,
      "grad_norm": 65.0359115600586,
      "learning_rate": 1.1180025940337224e-05,
      "loss": 73.5175,
      "step": 43100
    },
    {
      "epoch": 167.90146341463415,
      "grad_norm": 88.21428680419922,
      "learning_rate": 1.1192996108949416e-05,
      "loss": 73.9229,
      "step": 43150
    },
    {
      "epoch": 168.09365853658537,
      "grad_norm": 79.61613464355469,
      "learning_rate": 1.1205966277561609e-05,
      "loss": 72.3565,
      "step": 43200
    },
    {
      "epoch": 168.09365853658537,
      "eval_loss": 1.1751885414123535,
      "eval_perplexity": 3.2387535572052,
      "eval_runtime": 33.4364,
      "eval_samples_per_second": 59.307,
      "eval_steps_per_second": 7.417,
      "step": 43200
    },
    {
      "epoch": 168.28878048780487,
      "grad_norm": 94.82859802246094,
      "learning_rate": 1.12189364461738e-05,
      "loss": 73.6115,
      "step": 43250
    },
    {
      "epoch": 168.48390243902438,
      "grad_norm": 45.74015426635742,
      "learning_rate": 1.1231906614785993e-05,
      "loss": 74.0434,
      "step": 43300
    },
    {
      "epoch": 168.6790243902439,
      "grad_norm": 84.94303131103516,
      "learning_rate": 1.1244876783398185e-05,
      "loss": 74.2378,
      "step": 43350
    },
    {
      "epoch": 168.87414634146342,
      "grad_norm": 42.92987823486328,
      "learning_rate": 1.1257846952010375e-05,
      "loss": 74.0948,
      "step": 43400
    },
    {
      "epoch": 168.87414634146342,
      "eval_loss": 1.1745036840438843,
      "eval_perplexity": 3.2365362644195557,
      "eval_runtime": 32.8981,
      "eval_samples_per_second": 60.277,
      "eval_steps_per_second": 7.538,
      "step": 43400
    },
    {
      "epoch": 169.06634146341463,
      "grad_norm": 110.36447143554688,
      "learning_rate": 1.1270817120622568e-05,
      "loss": 71.8783,
      "step": 43450
    },
    {
      "epoch": 169.26146341463414,
      "grad_norm": 45.99699020385742,
      "learning_rate": 1.128378728923476e-05,
      "loss": 74.2624,
      "step": 43500
    },
    {
      "epoch": 169.45658536585367,
      "grad_norm": 69.45780944824219,
      "learning_rate": 1.1296757457846952e-05,
      "loss": 73.9139,
      "step": 43550
    },
    {
      "epoch": 169.65170731707317,
      "grad_norm": 87.85929870605469,
      "learning_rate": 1.1309727626459144e-05,
      "loss": 73.956,
      "step": 43600
    },
    {
      "epoch": 169.65170731707317,
      "eval_loss": 1.1737306118011475,
      "eval_perplexity": 3.234035015106201,
      "eval_runtime": 33.8936,
      "eval_samples_per_second": 58.507,
      "eval_steps_per_second": 7.317,
      "step": 43600
    },
    {
      "epoch": 169.84682926829268,
      "grad_norm": 51.599674224853516,
      "learning_rate": 1.1322697795071338e-05,
      "loss": 73.5725,
      "step": 43650
    },
    {
      "epoch": 170.0390243902439,
      "grad_norm": 49.1245231628418,
      "learning_rate": 1.133566796368353e-05,
      "loss": 72.0362,
      "step": 43700
    },
    {
      "epoch": 170.2341463414634,
      "grad_norm": 53.336177825927734,
      "learning_rate": 1.1348638132295722e-05,
      "loss": 73.5655,
      "step": 43750
    },
    {
      "epoch": 170.42926829268293,
      "grad_norm": 52.6314697265625,
      "learning_rate": 1.1361608300907914e-05,
      "loss": 73.7542,
      "step": 43800
    },
    {
      "epoch": 170.42926829268293,
      "eval_loss": 1.1730718612670898,
      "eval_perplexity": 3.231905460357666,
      "eval_runtime": 32.258,
      "eval_samples_per_second": 61.473,
      "eval_steps_per_second": 7.688,
      "step": 43800
    },
    {
      "epoch": 170.62439024390244,
      "grad_norm": 85.78620147705078,
      "learning_rate": 1.1374578469520106e-05,
      "loss": 73.932,
      "step": 43850
    },
    {
      "epoch": 170.81951219512194,
      "grad_norm": 43.9952278137207,
      "learning_rate": 1.1387548638132297e-05,
      "loss": 73.8452,
      "step": 43900
    },
    {
      "epoch": 171.01170731707316,
      "grad_norm": 46.996421813964844,
      "learning_rate": 1.1400518806744489e-05,
      "loss": 72.7473,
      "step": 43950
    },
    {
      "epoch": 171.2068292682927,
      "grad_norm": 60.549808502197266,
      "learning_rate": 1.1413488975356681e-05,
      "loss": 73.342,
      "step": 44000
    },
    {
      "epoch": 171.2068292682927,
      "eval_loss": 1.1725624799728394,
      "eval_perplexity": 3.230259418487549,
      "eval_runtime": 33.5098,
      "eval_samples_per_second": 59.177,
      "eval_steps_per_second": 7.401,
      "step": 44000
    },
    {
      "epoch": 171.4019512195122,
      "grad_norm": 81.40275573730469,
      "learning_rate": 1.1426459143968873e-05,
      "loss": 74.203,
      "step": 44050
    },
    {
      "epoch": 171.5970731707317,
      "grad_norm": 67.90068054199219,
      "learning_rate": 1.1439429312581065e-05,
      "loss": 73.3907,
      "step": 44100
    },
    {
      "epoch": 171.7921951219512,
      "grad_norm": 66.72003173828125,
      "learning_rate": 1.1452399481193257e-05,
      "loss": 73.8949,
      "step": 44150
    },
    {
      "epoch": 171.98731707317074,
      "grad_norm": 95.04782104492188,
      "learning_rate": 1.146536964980545e-05,
      "loss": 73.4177,
      "step": 44200
    },
    {
      "epoch": 171.98731707317074,
      "eval_loss": 1.172533631324768,
      "eval_perplexity": 3.230166435241699,
      "eval_runtime": 32.1591,
      "eval_samples_per_second": 61.662,
      "eval_steps_per_second": 7.712,
      "step": 44200
    },
    {
      "epoch": 172.17951219512196,
      "grad_norm": 80.98846435546875,
      "learning_rate": 1.1478339818417642e-05,
      "loss": 72.6791,
      "step": 44250
    },
    {
      "epoch": 172.37463414634146,
      "grad_norm": 84.14913940429688,
      "learning_rate": 1.1491309987029832e-05,
      "loss": 73.4376,
      "step": 44300
    },
    {
      "epoch": 172.56975609756097,
      "grad_norm": 111.94520568847656,
      "learning_rate": 1.1504280155642024e-05,
      "loss": 72.8913,
      "step": 44350
    },
    {
      "epoch": 172.76487804878047,
      "grad_norm": 57.45762252807617,
      "learning_rate": 1.1517250324254216e-05,
      "loss": 73.9689,
      "step": 44400
    },
    {
      "epoch": 172.76487804878047,
      "eval_loss": 1.170863389968872,
      "eval_perplexity": 3.224775791168213,
      "eval_runtime": 33.1808,
      "eval_samples_per_second": 59.763,
      "eval_steps_per_second": 7.474,
      "step": 44400
    },
    {
      "epoch": 172.96,
      "grad_norm": 62.95652770996094,
      "learning_rate": 1.1530220492866408e-05,
      "loss": 73.8821,
      "step": 44450
    },
    {
      "epoch": 173.15219512195122,
      "grad_norm": 42.735233306884766,
      "learning_rate": 1.15431906614786e-05,
      "loss": 73.0434,
      "step": 44500
    },
    {
      "epoch": 173.34731707317073,
      "grad_norm": 57.454837799072266,
      "learning_rate": 1.1556160830090793e-05,
      "loss": 73.0222,
      "step": 44550
    },
    {
      "epoch": 173.54243902439023,
      "grad_norm": 56.700626373291016,
      "learning_rate": 1.1569130998702985e-05,
      "loss": 73.5265,
      "step": 44600
    },
    {
      "epoch": 173.54243902439023,
      "eval_loss": 1.1707757711410522,
      "eval_perplexity": 3.2244930267333984,
      "eval_runtime": 34.3621,
      "eval_samples_per_second": 57.709,
      "eval_steps_per_second": 7.217,
      "step": 44600
    },
    {
      "epoch": 173.73756097560977,
      "grad_norm": 67.52566528320312,
      "learning_rate": 1.1582101167315177e-05,
      "loss": 73.6555,
      "step": 44650
    },
    {
      "epoch": 173.93268292682927,
      "grad_norm": 58.196929931640625,
      "learning_rate": 1.1595071335927367e-05,
      "loss": 73.5447,
      "step": 44700
    },
    {
      "epoch": 174.1248780487805,
      "grad_norm": 89.84833526611328,
      "learning_rate": 1.160804150453956e-05,
      "loss": 72.1391,
      "step": 44750
    },
    {
      "epoch": 174.32,
      "grad_norm": 40.76836013793945,
      "learning_rate": 1.1621011673151752e-05,
      "loss": 73.4002,
      "step": 44800
    },
    {
      "epoch": 174.32,
      "eval_loss": 1.17014741897583,
      "eval_perplexity": 3.2224676609039307,
      "eval_runtime": 32.733,
      "eval_samples_per_second": 60.581,
      "eval_steps_per_second": 7.576,
      "step": 44800
    },
    {
      "epoch": 174.5151219512195,
      "grad_norm": 74.43257141113281,
      "learning_rate": 1.1633981841763944e-05,
      "loss": 73.0702,
      "step": 44850
    },
    {
      "epoch": 174.71024390243903,
      "grad_norm": 58.677425384521484,
      "learning_rate": 1.1646952010376136e-05,
      "loss": 74.0091,
      "step": 44900
    },
    {
      "epoch": 174.90536585365854,
      "grad_norm": 86.42312622070312,
      "learning_rate": 1.1659922178988328e-05,
      "loss": 73.835,
      "step": 44950
    },
    {
      "epoch": 175.09756097560975,
      "grad_norm": 52.59004592895508,
      "learning_rate": 1.167289234760052e-05,
      "loss": 72.0415,
      "step": 45000
    },
    {
      "epoch": 175.09756097560975,
      "eval_loss": 1.1688202619552612,
      "eval_perplexity": 3.218193769454956,
      "eval_runtime": 32.455,
      "eval_samples_per_second": 61.1,
      "eval_steps_per_second": 7.641,
      "step": 45000
    },
    {
      "epoch": 175.29268292682926,
      "grad_norm": 46.872352600097656,
      "learning_rate": 1.1685862516212712e-05,
      "loss": 73.5346,
      "step": 45050
    },
    {
      "epoch": 175.4878048780488,
      "grad_norm": 65.442138671875,
      "learning_rate": 1.1698832684824903e-05,
      "loss": 73.7387,
      "step": 45100
    },
    {
      "epoch": 175.6829268292683,
      "grad_norm": 45.275203704833984,
      "learning_rate": 1.1711802853437095e-05,
      "loss": 73.259,
      "step": 45150
    },
    {
      "epoch": 175.8780487804878,
      "grad_norm": 55.418861389160156,
      "learning_rate": 1.1724773022049287e-05,
      "loss": 73.5903,
      "step": 45200
    },
    {
      "epoch": 175.8780487804878,
      "eval_loss": 1.1686108112335205,
      "eval_perplexity": 3.217519760131836,
      "eval_runtime": 33.0813,
      "eval_samples_per_second": 59.943,
      "eval_steps_per_second": 7.497,
      "step": 45200
    },
    {
      "epoch": 176.07024390243902,
      "grad_norm": 60.03931427001953,
      "learning_rate": 1.1737743190661479e-05,
      "loss": 72.5979,
      "step": 45250
    },
    {
      "epoch": 176.26536585365852,
      "grad_norm": 60.0462760925293,
      "learning_rate": 1.1750713359273671e-05,
      "loss": 73.4767,
      "step": 45300
    },
    {
      "epoch": 176.46048780487806,
      "grad_norm": 37.429344177246094,
      "learning_rate": 1.1763683527885863e-05,
      "loss": 73.3188,
      "step": 45350
    },
    {
      "epoch": 176.65560975609756,
      "grad_norm": 54.92472839355469,
      "learning_rate": 1.1776653696498055e-05,
      "loss": 73.4433,
      "step": 45400
    },
    {
      "epoch": 176.65560975609756,
      "eval_loss": 1.1682424545288086,
      "eval_perplexity": 3.216334819793701,
      "eval_runtime": 32.1856,
      "eval_samples_per_second": 61.611,
      "eval_steps_per_second": 7.705,
      "step": 45400
    },
    {
      "epoch": 176.85073170731707,
      "grad_norm": 67.81646728515625,
      "learning_rate": 1.1789623865110248e-05,
      "loss": 73.7912,
      "step": 45450
    },
    {
      "epoch": 177.04292682926828,
      "grad_norm": 54.06866455078125,
      "learning_rate": 1.1802594033722438e-05,
      "loss": 71.7095,
      "step": 45500
    },
    {
      "epoch": 177.23804878048782,
      "grad_norm": 81.125732421875,
      "learning_rate": 1.181556420233463e-05,
      "loss": 72.8625,
      "step": 45550
    },
    {
      "epoch": 177.43317073170732,
      "grad_norm": 68.58345031738281,
      "learning_rate": 1.1828534370946822e-05,
      "loss": 74.0488,
      "step": 45600
    },
    {
      "epoch": 177.43317073170732,
      "eval_loss": 1.167823076248169,
      "eval_perplexity": 3.2149863243103027,
      "eval_runtime": 33.2211,
      "eval_samples_per_second": 59.691,
      "eval_steps_per_second": 7.465,
      "step": 45600
    },
    {
      "epoch": 177.62829268292683,
      "grad_norm": 55.757957458496094,
      "learning_rate": 1.1841504539559014e-05,
      "loss": 73.1401,
      "step": 45650
    },
    {
      "epoch": 177.82341463414633,
      "grad_norm": 56.5950813293457,
      "learning_rate": 1.1854474708171206e-05,
      "loss": 73.5887,
      "step": 45700
    },
    {
      "epoch": 178.01560975609755,
      "grad_norm": 69.64344787597656,
      "learning_rate": 1.1867444876783399e-05,
      "loss": 72.0947,
      "step": 45750
    },
    {
      "epoch": 178.21073170731708,
      "grad_norm": 83.59888458251953,
      "learning_rate": 1.188041504539559e-05,
      "loss": 73.4894,
      "step": 45800
    },
    {
      "epoch": 178.21073170731708,
      "eval_loss": 1.16703462600708,
      "eval_perplexity": 3.2124524116516113,
      "eval_runtime": 32.1163,
      "eval_samples_per_second": 61.744,
      "eval_steps_per_second": 7.722,
      "step": 45800
    },
    {
      "epoch": 178.40585365853659,
      "grad_norm": 60.01974868774414,
      "learning_rate": 1.1893385214007783e-05,
      "loss": 73.2384,
      "step": 45850
    },
    {
      "epoch": 178.6009756097561,
      "grad_norm": 44.84304428100586,
      "learning_rate": 1.1906355382619973e-05,
      "loss": 73.2948,
      "step": 45900
    },
    {
      "epoch": 178.7960975609756,
      "grad_norm": 59.6448860168457,
      "learning_rate": 1.1919325551232165e-05,
      "loss": 73.4536,
      "step": 45950
    },
    {
      "epoch": 178.99121951219513,
      "grad_norm": 65.58756256103516,
      "learning_rate": 1.1932295719844358e-05,
      "loss": 73.2785,
      "step": 46000
    },
    {
      "epoch": 178.99121951219513,
      "eval_loss": 1.16695237159729,
      "eval_perplexity": 3.212188243865967,
      "eval_runtime": 34.0331,
      "eval_samples_per_second": 58.267,
      "eval_steps_per_second": 7.287,
      "step": 46000
    },
    {
      "epoch": 179.18341463414635,
      "grad_norm": 70.06890869140625,
      "learning_rate": 1.1945265888456551e-05,
      "loss": 72.3138,
      "step": 46050
    },
    {
      "epoch": 179.37853658536585,
      "grad_norm": 53.802467346191406,
      "learning_rate": 1.1958236057068743e-05,
      "loss": 73.1696,
      "step": 46100
    },
    {
      "epoch": 179.57365853658536,
      "grad_norm": 49.428466796875,
      "learning_rate": 1.1971206225680936e-05,
      "loss": 73.1355,
      "step": 46150
    },
    {
      "epoch": 179.7687804878049,
      "grad_norm": 76.9642105102539,
      "learning_rate": 1.1984176394293128e-05,
      "loss": 73.0443,
      "step": 46200
    },
    {
      "epoch": 179.7687804878049,
      "eval_loss": 1.1658263206481934,
      "eval_perplexity": 3.20857310295105,
      "eval_runtime": 31.2494,
      "eval_samples_per_second": 63.457,
      "eval_steps_per_second": 7.936,
      "step": 46200
    },
    {
      "epoch": 179.9639024390244,
      "grad_norm": 45.14201354980469,
      "learning_rate": 1.199714656290532e-05,
      "loss": 73.1943,
      "step": 46250
    },
    {
      "epoch": 180.1560975609756,
      "grad_norm": 60.20371627807617,
      "learning_rate": 1.2010116731517512e-05,
      "loss": 72.3573,
      "step": 46300
    },
    {
      "epoch": 180.35121951219512,
      "grad_norm": 53.40827178955078,
      "learning_rate": 1.2023086900129704e-05,
      "loss": 73.2405,
      "step": 46350
    },
    {
      "epoch": 180.54634146341462,
      "grad_norm": 71.67516326904297,
      "learning_rate": 1.2036057068741896e-05,
      "loss": 73.2421,
      "step": 46400
    },
    {
      "epoch": 180.54634146341462,
      "eval_loss": 1.1658928394317627,
      "eval_perplexity": 3.2087864875793457,
      "eval_runtime": 31.4281,
      "eval_samples_per_second": 63.096,
      "eval_steps_per_second": 7.891,
      "step": 46400
    },
    {
      "epoch": 180.74146341463415,
      "grad_norm": 42.87928771972656,
      "learning_rate": 1.2049027237354087e-05,
      "loss": 72.928,
      "step": 46450
    },
    {
      "epoch": 180.93658536585366,
      "grad_norm": 62.37205123901367,
      "learning_rate": 1.2061997405966279e-05,
      "loss": 73.4337,
      "step": 46500
    },
    {
      "epoch": 181.12878048780487,
      "grad_norm": 48.837432861328125,
      "learning_rate": 1.2074967574578471e-05,
      "loss": 72.6034,
      "step": 46550
    },
    {
      "epoch": 181.32390243902438,
      "grad_norm": 67.40969848632812,
      "learning_rate": 1.2087937743190663e-05,
      "loss": 72.6491,
      "step": 46600
    },
    {
      "epoch": 181.32390243902438,
      "eval_loss": 1.1662793159484863,
      "eval_perplexity": 3.210026979446411,
      "eval_runtime": 32.7951,
      "eval_samples_per_second": 60.466,
      "eval_steps_per_second": 7.562,
      "step": 46600
    },
    {
      "epoch": 181.5190243902439,
      "grad_norm": 87.97637939453125,
      "learning_rate": 1.2100907911802855e-05,
      "loss": 72.9309,
      "step": 46650
    },
    {
      "epoch": 181.71414634146342,
      "grad_norm": 59.57369613647461,
      "learning_rate": 1.2113878080415047e-05,
      "loss": 73.1285,
      "step": 46700
    },
    {
      "epoch": 181.90926829268292,
      "grad_norm": 63.85610580444336,
      "learning_rate": 1.212684824902724e-05,
      "loss": 73.3398,
      "step": 46750
    },
    {
      "epoch": 182.10146341463414,
      "grad_norm": 37.71181869506836,
      "learning_rate": 1.2139818417639432e-05,
      "loss": 71.8014,
      "step": 46800
    },
    {
      "epoch": 182.10146341463414,
      "eval_loss": 1.1640756130218506,
      "eval_perplexity": 3.202960729598999,
      "eval_runtime": 38.0201,
      "eval_samples_per_second": 52.157,
      "eval_steps_per_second": 6.523,
      "step": 46800
    },
    {
      "epoch": 182.29658536585364,
      "grad_norm": 106.330078125,
      "learning_rate": 1.2152788586251622e-05,
      "loss": 72.9527,
      "step": 46850
    },
    {
      "epoch": 182.49170731707318,
      "grad_norm": 54.288692474365234,
      "learning_rate": 1.2165758754863814e-05,
      "loss": 73.2502,
      "step": 46900
    },
    {
      "epoch": 182.68682926829268,
      "grad_norm": 59.237430572509766,
      "learning_rate": 1.2178728923476006e-05,
      "loss": 73.5704,
      "step": 46950
    },
    {
      "epoch": 182.8819512195122,
      "grad_norm": 42.02275466918945,
      "learning_rate": 1.2191699092088198e-05,
      "loss": 73.5936,
      "step": 47000
    },
    {
      "epoch": 182.8819512195122,
      "eval_loss": 1.1650091409683228,
      "eval_perplexity": 3.2059521675109863,
      "eval_runtime": 31.6011,
      "eval_samples_per_second": 62.751,
      "eval_steps_per_second": 7.848,
      "step": 47000
    },
    {
      "epoch": 183.0741463414634,
      "grad_norm": 70.94632720947266,
      "learning_rate": 1.220466926070039e-05,
      "loss": 71.3738,
      "step": 47050
    },
    {
      "epoch": 183.26926829268294,
      "grad_norm": 53.661346435546875,
      "learning_rate": 1.2217639429312583e-05,
      "loss": 73.0636,
      "step": 47100
    },
    {
      "epoch": 183.46439024390244,
      "grad_norm": 49.334659576416016,
      "learning_rate": 1.2230609597924775e-05,
      "loss": 73.5219,
      "step": 47150
    },
    {
      "epoch": 183.65951219512195,
      "grad_norm": 68.94295501708984,
      "learning_rate": 1.2243579766536967e-05,
      "loss": 73.155,
      "step": 47200
    },
    {
      "epoch": 183.65951219512195,
      "eval_loss": 1.1648366451263428,
      "eval_perplexity": 3.20539927482605,
      "eval_runtime": 32.0105,
      "eval_samples_per_second": 61.948,
      "eval_steps_per_second": 7.747,
      "step": 47200
    },
    {
      "epoch": 183.85463414634145,
      "grad_norm": 39.51964569091797,
      "learning_rate": 1.2256549935149157e-05,
      "loss": 72.8695,
      "step": 47250
    },
    {
      "epoch": 184.04682926829267,
      "grad_norm": 59.59587097167969,
      "learning_rate": 1.226952010376135e-05,
      "loss": 71.7459,
      "step": 47300
    },
    {
      "epoch": 184.2419512195122,
      "grad_norm": 92.68537139892578,
      "learning_rate": 1.2282490272373542e-05,
      "loss": 73.0651,
      "step": 47350
    },
    {
      "epoch": 184.4370731707317,
      "grad_norm": 64.58430480957031,
      "learning_rate": 1.2295460440985734e-05,
      "loss": 72.7018,
      "step": 47400
    },
    {
      "epoch": 184.4370731707317,
      "eval_loss": 1.1633481979370117,
      "eval_perplexity": 3.200631618499756,
      "eval_runtime": 31.4675,
      "eval_samples_per_second": 63.017,
      "eval_steps_per_second": 7.881,
      "step": 47400
    },
    {
      "epoch": 184.6321951219512,
      "grad_norm": 79.27935791015625,
      "learning_rate": 1.2308430609597926e-05,
      "loss": 73.3328,
      "step": 47450
    },
    {
      "epoch": 184.82731707317072,
      "grad_norm": 80.17192077636719,
      "learning_rate": 1.2321400778210118e-05,
      "loss": 73.3046,
      "step": 47500
    },
    {
      "epoch": 185.01951219512196,
      "grad_norm": 59.16652297973633,
      "learning_rate": 1.233437094682231e-05,
      "loss": 71.7786,
      "step": 47550
    },
    {
      "epoch": 185.21463414634147,
      "grad_norm": 37.521873474121094,
      "learning_rate": 1.23473411154345e-05,
      "loss": 72.9085,
      "step": 47600
    },
    {
      "epoch": 185.21463414634147,
      "eval_loss": 1.1634345054626465,
      "eval_perplexity": 3.2009079456329346,
      "eval_runtime": 31.399,
      "eval_samples_per_second": 63.155,
      "eval_steps_per_second": 7.898,
      "step": 47600
    },
    {
      "epoch": 185.40975609756097,
      "grad_norm": 58.47385787963867,
      "learning_rate": 1.2360311284046693e-05,
      "loss": 73.0935,
      "step": 47650
    },
    {
      "epoch": 185.60487804878048,
      "grad_norm": 63.439414978027344,
      "learning_rate": 1.2373281452658885e-05,
      "loss": 72.7141,
      "step": 47700
    },
    {
      "epoch": 185.8,
      "grad_norm": 62.54753494262695,
      "learning_rate": 1.2386251621271077e-05,
      "loss": 73.3462,
      "step": 47750
    },
    {
      "epoch": 185.99512195121952,
      "grad_norm": 55.38509750366211,
      "learning_rate": 1.2399221789883269e-05,
      "loss": 72.9063,
      "step": 47800
    },
    {
      "epoch": 185.99512195121952,
      "eval_loss": 1.1619303226470947,
      "eval_perplexity": 3.196096897125244,
      "eval_runtime": 37.3054,
      "eval_samples_per_second": 53.156,
      "eval_steps_per_second": 6.648,
      "step": 47800
    },
    {
      "epoch": 186.18731707317073,
      "grad_norm": 56.044097900390625,
      "learning_rate": 1.2412191958495461e-05,
      "loss": 71.7297,
      "step": 47850
    },
    {
      "epoch": 186.38243902439024,
      "grad_norm": 65.47098541259766,
      "learning_rate": 1.2425162127107653e-05,
      "loss": 72.7334,
      "step": 47900
    },
    {
      "epoch": 186.57756097560974,
      "grad_norm": 79.32723999023438,
      "learning_rate": 1.2438132295719845e-05,
      "loss": 73.0307,
      "step": 47950
    },
    {
      "epoch": 186.77268292682928,
      "grad_norm": 56.94413757324219,
      "learning_rate": 1.2451102464332036e-05,
      "loss": 73.133,
      "step": 48000
    },
    {
      "epoch": 186.77268292682928,
      "eval_loss": 1.1613363027572632,
      "eval_perplexity": 3.1941988468170166,
      "eval_runtime": 32.7378,
      "eval_samples_per_second": 60.572,
      "eval_steps_per_second": 7.575,
      "step": 48000
    },
    {
      "epoch": 186.96780487804878,
      "grad_norm": 48.88232421875,
      "learning_rate": 1.2464072632944228e-05,
      "loss": 72.8607,
      "step": 48050
    },
    {
      "epoch": 187.16,
      "grad_norm": 51.276145935058594,
      "learning_rate": 1.247704280155642e-05,
      "loss": 72.0608,
      "step": 48100
    },
    {
      "epoch": 187.3551219512195,
      "grad_norm": 71.06312561035156,
      "learning_rate": 1.2490012970168612e-05,
      "loss": 72.971,
      "step": 48150
    },
    {
      "epoch": 187.55024390243904,
      "grad_norm": 53.4266471862793,
      "learning_rate": 1.2502983138780804e-05,
      "loss": 72.9433,
      "step": 48200
    },
    {
      "epoch": 187.55024390243904,
      "eval_loss": 1.1607904434204102,
      "eval_perplexity": 3.192455768585205,
      "eval_runtime": 35.866,
      "eval_samples_per_second": 55.289,
      "eval_steps_per_second": 6.915,
      "step": 48200
    },
    {
      "epoch": 187.74536585365854,
      "grad_norm": 65.3938980102539,
      "learning_rate": 1.2515953307392996e-05,
      "loss": 73.0022,
      "step": 48250
    },
    {
      "epoch": 187.94048780487805,
      "grad_norm": 67.78763580322266,
      "learning_rate": 1.2528923476005189e-05,
      "loss": 72.6823,
      "step": 48300
    },
    {
      "epoch": 188.13268292682926,
      "grad_norm": 48.62816619873047,
      "learning_rate": 1.254189364461738e-05,
      "loss": 72.1419,
      "step": 48350
    },
    {
      "epoch": 188.32780487804877,
      "grad_norm": 48.85117721557617,
      "learning_rate": 1.2554863813229571e-05,
      "loss": 72.9655,
      "step": 48400
    },
    {
      "epoch": 188.32780487804877,
      "eval_loss": 1.1614854335784912,
      "eval_perplexity": 3.1946752071380615,
      "eval_runtime": 30.5949,
      "eval_samples_per_second": 64.815,
      "eval_steps_per_second": 8.106,
      "step": 48400
    },
    {
      "epoch": 188.5229268292683,
      "grad_norm": 70.3723373413086,
      "learning_rate": 1.2567833981841767e-05,
      "loss": 72.6564,
      "step": 48450
    },
    {
      "epoch": 188.7180487804878,
      "grad_norm": 46.85993576049805,
      "learning_rate": 1.2580804150453959e-05,
      "loss": 72.5965,
      "step": 48500
    },
    {
      "epoch": 188.9131707317073,
      "grad_norm": 49.169742584228516,
      "learning_rate": 1.259377431906615e-05,
      "loss": 72.7697,
      "step": 48550
    },
    {
      "epoch": 189.10536585365853,
      "grad_norm": 43.741722106933594,
      "learning_rate": 1.2606744487678341e-05,
      "loss": 72.3246,
      "step": 48600
    },
    {
      "epoch": 189.10536585365853,
      "eval_loss": 1.1618422269821167,
      "eval_perplexity": 3.195815324783325,
      "eval_runtime": 31.2511,
      "eval_samples_per_second": 63.454,
      "eval_steps_per_second": 7.936,
      "step": 48600
    },
    {
      "epoch": 189.30048780487806,
      "grad_norm": 53.27492141723633,
      "learning_rate": 1.2619714656290533e-05,
      "loss": 72.7375,
      "step": 48650
    },
    {
      "epoch": 189.49560975609756,
      "grad_norm": 78.3420639038086,
      "learning_rate": 1.2632684824902726e-05,
      "loss": 72.3972,
      "step": 48700
    },
    {
      "epoch": 189.69073170731707,
      "grad_norm": 45.18618392944336,
      "learning_rate": 1.2645654993514918e-05,
      "loss": 72.7642,
      "step": 48750
    },
    {
      "epoch": 189.88585365853658,
      "grad_norm": 57.402347564697266,
      "learning_rate": 1.265862516212711e-05,
      "loss": 72.5741,
      "step": 48800
    },
    {
      "epoch": 189.88585365853658,
      "eval_loss": 1.1598258018493652,
      "eval_perplexity": 3.189377546310425,
      "eval_runtime": 35.6765,
      "eval_samples_per_second": 55.583,
      "eval_steps_per_second": 6.951,
      "step": 48800
    },
    {
      "epoch": 190.0780487804878,
      "grad_norm": 75.8061752319336,
      "learning_rate": 1.2671595330739302e-05,
      "loss": 71.9091,
      "step": 48850
    },
    {
      "epoch": 190.27317073170732,
      "grad_norm": 62.714027404785156,
      "learning_rate": 1.2684565499351494e-05,
      "loss": 72.1539,
      "step": 48900
    },
    {
      "epoch": 190.46829268292683,
      "grad_norm": 37.64371871948242,
      "learning_rate": 1.2697535667963685e-05,
      "loss": 73.1285,
      "step": 48950
    },
    {
      "epoch": 190.66341463414633,
      "grad_norm": 57.786922454833984,
      "learning_rate": 1.2710505836575877e-05,
      "loss": 73.1587,
      "step": 49000
    },
    {
      "epoch": 190.66341463414633,
      "eval_loss": 1.1600472927093506,
      "eval_perplexity": 3.190084218978882,
      "eval_runtime": 30.7585,
      "eval_samples_per_second": 64.47,
      "eval_steps_per_second": 8.063,
      "step": 49000
    },
    {
      "epoch": 190.85853658536584,
      "grad_norm": 64.26384735107422,
      "learning_rate": 1.2723476005188069e-05,
      "loss": 72.7223,
      "step": 49050
    },
    {
      "epoch": 191.05073170731708,
      "grad_norm": 57.276344299316406,
      "learning_rate": 1.2736446173800261e-05,
      "loss": 71.5888,
      "step": 49100
    },
    {
      "epoch": 191.2458536585366,
      "grad_norm": 74.81188201904297,
      "learning_rate": 1.2749416342412453e-05,
      "loss": 72.9751,
      "step": 49150
    },
    {
      "epoch": 191.4409756097561,
      "grad_norm": 50.38323974609375,
      "learning_rate": 1.2762386511024645e-05,
      "loss": 72.1937,
      "step": 49200
    },
    {
      "epoch": 191.4409756097561,
      "eval_loss": 1.1600922346115112,
      "eval_perplexity": 3.190227508544922,
      "eval_runtime": 33.4923,
      "eval_samples_per_second": 59.208,
      "eval_steps_per_second": 7.405,
      "step": 49200
    },
    {
      "epoch": 191.6360975609756,
      "grad_norm": 56.242252349853516,
      "learning_rate": 1.2775356679636837e-05,
      "loss": 72.9133,
      "step": 49250
    },
    {
      "epoch": 191.83121951219513,
      "grad_norm": 78.67971801757812,
      "learning_rate": 1.278832684824903e-05,
      "loss": 72.5428,
      "step": 49300
    },
    {
      "epoch": 192.02341463414635,
      "grad_norm": 59.28668212890625,
      "learning_rate": 1.280129701686122e-05,
      "loss": 71.9626,
      "step": 49350
    },
    {
      "epoch": 192.21853658536585,
      "grad_norm": 63.085269927978516,
      "learning_rate": 1.2814267185473412e-05,
      "loss": 73.1394,
      "step": 49400
    },
    {
      "epoch": 192.21853658536585,
      "eval_loss": 1.1599884033203125,
      "eval_perplexity": 3.18989634513855,
      "eval_runtime": 31.581,
      "eval_samples_per_second": 62.791,
      "eval_steps_per_second": 7.853,
      "step": 49400
    },
    {
      "epoch": 192.41365853658536,
      "grad_norm": 61.43583679199219,
      "learning_rate": 1.2827237354085604e-05,
      "loss": 72.1133,
      "step": 49450
    },
    {
      "epoch": 192.60878048780486,
      "grad_norm": 124.92444610595703,
      "learning_rate": 1.2840207522697796e-05,
      "loss": 72.4962,
      "step": 49500
    },
    {
      "epoch": 192.8039024390244,
      "grad_norm": 60.67761993408203,
      "learning_rate": 1.2853177691309988e-05,
      "loss": 72.5223,
      "step": 49550
    },
    {
      "epoch": 192.9990243902439,
      "grad_norm": 62.3166618347168,
      "learning_rate": 1.286614785992218e-05,
      "loss": 73.0542,
      "step": 49600
    },
    {
      "epoch": 192.9990243902439,
      "eval_loss": 1.1583664417266846,
      "eval_perplexity": 3.1847264766693115,
      "eval_runtime": 31.3634,
      "eval_samples_per_second": 63.227,
      "eval_steps_per_second": 7.907,
      "step": 49600
    },
    {
      "epoch": 193.19121951219512,
      "grad_norm": 77.59636688232422,
      "learning_rate": 1.2879118028534373e-05,
      "loss": 71.3533,
      "step": 49650
    },
    {
      "epoch": 193.38634146341462,
      "grad_norm": 56.78572082519531,
      "learning_rate": 1.2892088197146565e-05,
      "loss": 72.596,
      "step": 49700
    },
    {
      "epoch": 193.58146341463416,
      "grad_norm": 45.604400634765625,
      "learning_rate": 1.2905058365758755e-05,
      "loss": 73.0575,
      "step": 49750
    },
    {
      "epoch": 193.77658536585366,
      "grad_norm": 51.544952392578125,
      "learning_rate": 1.2918028534370947e-05,
      "loss": 72.5605,
      "step": 49800
    },
    {
      "epoch": 193.77658536585366,
      "eval_loss": 1.1580489873886108,
      "eval_perplexity": 3.1837158203125,
      "eval_runtime": 32.5144,
      "eval_samples_per_second": 60.988,
      "eval_steps_per_second": 7.627,
      "step": 49800
    },
    {
      "epoch": 193.97170731707317,
      "grad_norm": 80.682861328125,
      "learning_rate": 1.293099870298314e-05,
      "loss": 72.5261,
      "step": 49850
    },
    {
      "epoch": 194.16390243902438,
      "grad_norm": 57.198089599609375,
      "learning_rate": 1.2943968871595332e-05,
      "loss": 71.2277,
      "step": 49900
    },
    {
      "epoch": 194.3590243902439,
      "grad_norm": 51.78819274902344,
      "learning_rate": 1.2956939040207524e-05,
      "loss": 72.9196,
      "step": 49950
    },
    {
      "epoch": 194.55414634146342,
      "grad_norm": 69.37873840332031,
      "learning_rate": 1.2969909208819716e-05,
      "loss": 72.807,
      "step": 50000
    },
    {
      "epoch": 194.55414634146342,
      "eval_loss": 1.1574710607528687,
      "eval_perplexity": 3.1818764209747314,
      "eval_runtime": 31.9288,
      "eval_samples_per_second": 62.107,
      "eval_steps_per_second": 7.767,
      "step": 50000
    },
    {
      "epoch": 194.74926829268293,
      "grad_norm": 52.30315017700195,
      "learning_rate": 1.2982879377431908e-05,
      "loss": 72.8204,
      "step": 50050
    },
    {
      "epoch": 194.94439024390243,
      "grad_norm": 88.47339630126953,
      "learning_rate": 1.29958495460441e-05,
      "loss": 72.2026,
      "step": 50100
    },
    {
      "epoch": 195.13658536585365,
      "grad_norm": 58.52598190307617,
      "learning_rate": 1.300881971465629e-05,
      "loss": 71.4849,
      "step": 50150
    },
    {
      "epoch": 195.33170731707318,
      "grad_norm": 55.9096794128418,
      "learning_rate": 1.3021789883268483e-05,
      "loss": 73.1313,
      "step": 50200
    },
    {
      "epoch": 195.33170731707318,
      "eval_loss": 1.1571451425552368,
      "eval_perplexity": 3.1808395385742188,
      "eval_runtime": 33.1318,
      "eval_samples_per_second": 59.852,
      "eval_steps_per_second": 7.485,
      "step": 50200
    },
    {
      "epoch": 195.5268292682927,
      "grad_norm": 55.12279510498047,
      "learning_rate": 1.3034760051880675e-05,
      "loss": 72.7359,
      "step": 50250
    },
    {
      "epoch": 195.7219512195122,
      "grad_norm": 50.60063171386719,
      "learning_rate": 1.3047730220492867e-05,
      "loss": 72.4965,
      "step": 50300
    },
    {
      "epoch": 195.9170731707317,
      "grad_norm": 47.88615417480469,
      "learning_rate": 1.3060700389105059e-05,
      "loss": 71.9933,
      "step": 50350
    },
    {
      "epoch": 196.1092682926829,
      "grad_norm": 61.64991760253906,
      "learning_rate": 1.3073670557717251e-05,
      "loss": 71.2063,
      "step": 50400
    },
    {
      "epoch": 196.1092682926829,
      "eval_loss": 1.1587247848510742,
      "eval_perplexity": 3.18586802482605,
      "eval_runtime": 31.1834,
      "eval_samples_per_second": 63.592,
      "eval_steps_per_second": 7.953,
      "step": 50400
    },
    {
      "epoch": 196.30439024390245,
      "grad_norm": 55.46977996826172,
      "learning_rate": 1.3086640726329443e-05,
      "loss": 72.6538,
      "step": 50450
    },
    {
      "epoch": 196.49951219512195,
      "grad_norm": 71.38274383544922,
      "learning_rate": 1.3099610894941635e-05,
      "loss": 72.1611,
      "step": 50500
    },
    {
      "epoch": 196.69463414634146,
      "grad_norm": 66.17207336425781,
      "learning_rate": 1.3112581063553826e-05,
      "loss": 72.414,
      "step": 50550
    },
    {
      "epoch": 196.88975609756096,
      "grad_norm": 51.65770721435547,
      "learning_rate": 1.3125551232166018e-05,
      "loss": 72.9752,
      "step": 50600
    },
    {
      "epoch": 196.88975609756096,
      "eval_loss": 1.1571446657180786,
      "eval_perplexity": 3.180837869644165,
      "eval_runtime": 33.2584,
      "eval_samples_per_second": 59.624,
      "eval_steps_per_second": 7.457,
      "step": 50600
    },
    {
      "epoch": 197.0819512195122,
      "grad_norm": 45.300682067871094,
      "learning_rate": 1.313852140077821e-05,
      "loss": 71.4921,
      "step": 50650
    },
    {
      "epoch": 197.2770731707317,
      "grad_norm": 55.75271224975586,
      "learning_rate": 1.3151491569390402e-05,
      "loss": 72.4506,
      "step": 50700
    },
    {
      "epoch": 197.47219512195122,
      "grad_norm": 58.3900032043457,
      "learning_rate": 1.3164461738002594e-05,
      "loss": 72.1055,
      "step": 50750
    },
    {
      "epoch": 197.66731707317072,
      "grad_norm": 73.43597412109375,
      "learning_rate": 1.3177431906614786e-05,
      "loss": 72.1061,
      "step": 50800
    },
    {
      "epoch": 197.66731707317072,
      "eval_loss": 1.155452847480774,
      "eval_perplexity": 3.1754610538482666,
      "eval_runtime": 35.2019,
      "eval_samples_per_second": 56.332,
      "eval_steps_per_second": 7.045,
      "step": 50800
    },
    {
      "epoch": 197.86243902439026,
      "grad_norm": 65.77801513671875,
      "learning_rate": 1.319040207522698e-05,
      "loss": 72.4553,
      "step": 50850
    },
    {
      "epoch": 198.05463414634147,
      "grad_norm": 63.999507904052734,
      "learning_rate": 1.3203372243839172e-05,
      "loss": 72.2035,
      "step": 50900
    },
    {
      "epoch": 198.24975609756098,
      "grad_norm": 65.44903564453125,
      "learning_rate": 1.3216342412451364e-05,
      "loss": 72.0832,
      "step": 50950
    },
    {
      "epoch": 198.44487804878048,
      "grad_norm": 52.577754974365234,
      "learning_rate": 1.3229312581063557e-05,
      "loss": 72.8101,
      "step": 51000
    },
    {
      "epoch": 198.44487804878048,
      "eval_loss": 1.1551239490509033,
      "eval_perplexity": 3.1744167804718018,
      "eval_runtime": 36.9373,
      "eval_samples_per_second": 53.686,
      "eval_steps_per_second": 6.714,
      "step": 51000
    },
    {
      "epoch": 198.64,
      "grad_norm": 52.4752197265625,
      "learning_rate": 1.3242282749675747e-05,
      "loss": 71.8263,
      "step": 51050
    },
    {
      "epoch": 198.83512195121952,
      "grad_norm": 49.80485916137695,
      "learning_rate": 1.325525291828794e-05,
      "loss": 72.6357,
      "step": 51100
    },
    {
      "epoch": 199.02731707317074,
      "grad_norm": 86.61827087402344,
      "learning_rate": 1.3268223086900131e-05,
      "loss": 71.471,
      "step": 51150
    },
    {
      "epoch": 199.22243902439024,
      "grad_norm": 57.127723693847656,
      "learning_rate": 1.3281193255512323e-05,
      "loss": 72.415,
      "step": 51200
    },
    {
      "epoch": 199.22243902439024,
      "eval_loss": 1.1554856300354004,
      "eval_perplexity": 3.175565242767334,
      "eval_runtime": 35.4951,
      "eval_samples_per_second": 55.867,
      "eval_steps_per_second": 6.987,
      "step": 51200
    },
    {
      "epoch": 199.41756097560975,
      "grad_norm": 48.26510238647461,
      "learning_rate": 1.3294163424124516e-05,
      "loss": 72.3526,
      "step": 51250
    },
    {
      "epoch": 199.61268292682928,
      "grad_norm": 56.589046478271484,
      "learning_rate": 1.3307133592736708e-05,
      "loss": 72.6076,
      "step": 51300
    },
    {
      "epoch": 199.80780487804878,
      "grad_norm": 93.4442138671875,
      "learning_rate": 1.33201037613489e-05,
      "loss": 72.4218,
      "step": 51350
    },
    {
      "epoch": 200.0,
      "grad_norm": 11.11912727355957,
      "learning_rate": 1.3333073929961092e-05,
      "loss": 71.1701,
      "step": 51400
    },
    {
      "epoch": 200.0,
      "eval_loss": 1.157070279121399,
      "eval_perplexity": 3.1806013584136963,
      "eval_runtime": 31.0111,
      "eval_samples_per_second": 63.945,
      "eval_steps_per_second": 7.997,
      "step": 51400
    },
    {
      "epoch": 200.1951219512195,
      "grad_norm": 72.54116821289062,
      "learning_rate": 1.3346044098573282e-05,
      "loss": 71.7402,
      "step": 51450
    },
    {
      "epoch": 200.390243902439,
      "grad_norm": 60.42625427246094,
      "learning_rate": 1.3359014267185474e-05,
      "loss": 72.3977,
      "step": 51500
    },
    {
      "epoch": 200.58536585365854,
      "grad_norm": 77.79137420654297,
      "learning_rate": 1.3371984435797667e-05,
      "loss": 72.5351,
      "step": 51550
    },
    {
      "epoch": 200.78048780487805,
      "grad_norm": 56.27334213256836,
      "learning_rate": 1.3384954604409859e-05,
      "loss": 72.6384,
      "step": 51600
    },
    {
      "epoch": 200.78048780487805,
      "eval_loss": 1.1541087627410889,
      "eval_perplexity": 3.1711959838867188,
      "eval_runtime": 37.2189,
      "eval_samples_per_second": 53.279,
      "eval_steps_per_second": 6.663,
      "step": 51600
    },
    {
      "epoch": 200.97560975609755,
      "grad_norm": 44.920989990234375,
      "learning_rate": 1.3397924773022051e-05,
      "loss": 72.4868,
      "step": 51650
    },
    {
      "epoch": 201.16780487804877,
      "grad_norm": 47.7791748046875,
      "learning_rate": 1.3410894941634243e-05,
      "loss": 70.8464,
      "step": 51700
    },
    {
      "epoch": 201.3629268292683,
      "grad_norm": 61.00989532470703,
      "learning_rate": 1.3423865110246435e-05,
      "loss": 72.7146,
      "step": 51750
    },
    {
      "epoch": 201.5580487804878,
      "grad_norm": 69.17314910888672,
      "learning_rate": 1.3436835278858627e-05,
      "loss": 71.9596,
      "step": 51800
    },
    {
      "epoch": 201.5580487804878,
      "eval_loss": 1.153640627861023,
      "eval_perplexity": 3.1697115898132324,
      "eval_runtime": 31.0111,
      "eval_samples_per_second": 63.945,
      "eval_steps_per_second": 7.997,
      "step": 51800
    },
    {
      "epoch": 201.7531707317073,
      "grad_norm": 63.91236877441406,
      "learning_rate": 1.3449805447470818e-05,
      "loss": 72.1757,
      "step": 51850
    },
    {
      "epoch": 201.94829268292682,
      "grad_norm": 50.710880279541016,
      "learning_rate": 1.346277561608301e-05,
      "loss": 72.4994,
      "step": 51900
    },
    {
      "epoch": 202.14048780487803,
      "grad_norm": 47.51685333251953,
      "learning_rate": 1.3475745784695202e-05,
      "loss": 71.2104,
      "step": 51950
    },
    {
      "epoch": 202.33560975609757,
      "grad_norm": 66.99068450927734,
      "learning_rate": 1.3488715953307394e-05,
      "loss": 72.7233,
      "step": 52000
    },
    {
      "epoch": 202.33560975609757,
      "eval_loss": 1.153408408164978,
      "eval_perplexity": 3.168975591659546,
      "eval_runtime": 31.5915,
      "eval_samples_per_second": 62.77,
      "eval_steps_per_second": 7.85,
      "step": 52000
    },
    {
      "epoch": 202.53073170731707,
      "grad_norm": 58.434608459472656,
      "learning_rate": 1.3501686121919586e-05,
      "loss": 71.8287,
      "step": 52050
    },
    {
      "epoch": 202.72585365853658,
      "grad_norm": 72.47517395019531,
      "learning_rate": 1.3514656290531778e-05,
      "loss": 71.4894,
      "step": 52100
    },
    {
      "epoch": 202.92097560975608,
      "grad_norm": 66.27198791503906,
      "learning_rate": 1.352762645914397e-05,
      "loss": 73.1828,
      "step": 52150
    },
    {
      "epoch": 203.11317073170733,
      "grad_norm": 51.720279693603516,
      "learning_rate": 1.3540596627756163e-05,
      "loss": 71.1433,
      "step": 52200
    },
    {
      "epoch": 203.11317073170733,
      "eval_loss": 1.1534132957458496,
      "eval_perplexity": 3.1689910888671875,
      "eval_runtime": 32.989,
      "eval_samples_per_second": 60.111,
      "eval_steps_per_second": 7.518,
      "step": 52200
    },
    {
      "epoch": 203.30829268292683,
      "grad_norm": 49.57319641113281,
      "learning_rate": 1.3553566796368353e-05,
      "loss": 72.0185,
      "step": 52250
    },
    {
      "epoch": 203.50341463414634,
      "grad_norm": 69.51728820800781,
      "learning_rate": 1.3566536964980545e-05,
      "loss": 73.0127,
      "step": 52300
    },
    {
      "epoch": 203.69853658536584,
      "grad_norm": 63.064849853515625,
      "learning_rate": 1.3579507133592737e-05,
      "loss": 71.9332,
      "step": 52350
    },
    {
      "epoch": 203.89365853658538,
      "grad_norm": 41.84553527832031,
      "learning_rate": 1.359247730220493e-05,
      "loss": 71.675,
      "step": 52400
    },
    {
      "epoch": 203.89365853658538,
      "eval_loss": 1.1545600891113281,
      "eval_perplexity": 3.1726274490356445,
      "eval_runtime": 31.1874,
      "eval_samples_per_second": 63.583,
      "eval_steps_per_second": 7.952,
      "step": 52400
    },
    {
      "epoch": 204.0858536585366,
      "grad_norm": 43.64104461669922,
      "learning_rate": 1.3605447470817122e-05,
      "loss": 71.8356,
      "step": 52450
    },
    {
      "epoch": 204.2809756097561,
      "grad_norm": 48.46567916870117,
      "learning_rate": 1.3618417639429314e-05,
      "loss": 72.2456,
      "step": 52500
    },
    {
      "epoch": 204.4760975609756,
      "grad_norm": 39.889122009277344,
      "learning_rate": 1.3631387808041506e-05,
      "loss": 72.1013,
      "step": 52550
    },
    {
      "epoch": 204.6712195121951,
      "grad_norm": 57.8751106262207,
      "learning_rate": 1.3644357976653698e-05,
      "loss": 72.2956,
      "step": 52600
    },
    {
      "epoch": 204.6712195121951,
      "eval_loss": 1.1511740684509277,
      "eval_perplexity": 3.161903142929077,
      "eval_runtime": 32.1064,
      "eval_samples_per_second": 61.763,
      "eval_steps_per_second": 7.724,
      "step": 52600
    },
    {
      "epoch": 204.86634146341464,
      "grad_norm": 56.79631805419922,
      "learning_rate": 1.3657328145265888e-05,
      "loss": 71.7343,
      "step": 52650
    },
    {
      "epoch": 205.05853658536586,
      "grad_norm": 48.61320114135742,
      "learning_rate": 1.367029831387808e-05,
      "loss": 70.5862,
      "step": 52700
    },
    {
      "epoch": 205.25365853658536,
      "grad_norm": 67.28968811035156,
      "learning_rate": 1.3683268482490273e-05,
      "loss": 72.5757,
      "step": 52750
    },
    {
      "epoch": 205.44878048780487,
      "grad_norm": 49.72463607788086,
      "learning_rate": 1.3696238651102465e-05,
      "loss": 72.494,
      "step": 52800
    },
    {
      "epoch": 205.44878048780487,
      "eval_loss": 1.1525481939315796,
      "eval_perplexity": 3.1662509441375732,
      "eval_runtime": 34.0312,
      "eval_samples_per_second": 58.27,
      "eval_steps_per_second": 7.287,
      "step": 52800
    },
    {
      "epoch": 205.6439024390244,
      "grad_norm": 54.45674514770508,
      "learning_rate": 1.3709208819714657e-05,
      "loss": 71.7605,
      "step": 52850
    },
    {
      "epoch": 205.8390243902439,
      "grad_norm": 51.03325271606445,
      "learning_rate": 1.3722178988326849e-05,
      "loss": 71.8867,
      "step": 52900
    },
    {
      "epoch": 206.03121951219512,
      "grad_norm": 63.31000900268555,
      "learning_rate": 1.3735149156939041e-05,
      "loss": 71.3022,
      "step": 52950
    },
    {
      "epoch": 206.22634146341463,
      "grad_norm": 53.515052795410156,
      "learning_rate": 1.3748119325551233e-05,
      "loss": 72.2339,
      "step": 53000
    },
    {
      "epoch": 206.22634146341463,
      "eval_loss": 1.1533350944519043,
      "eval_perplexity": 3.168743371963501,
      "eval_runtime": 31.4285,
      "eval_samples_per_second": 63.096,
      "eval_steps_per_second": 7.891,
      "step": 53000
    },
    {
      "epoch": 206.42146341463413,
      "grad_norm": 50.20814514160156,
      "learning_rate": 1.3761089494163424e-05,
      "loss": 72.0381,
      "step": 53050
    },
    {
      "epoch": 206.61658536585367,
      "grad_norm": 56.32780075073242,
      "learning_rate": 1.3774059662775616e-05,
      "loss": 72.3342,
      "step": 53100
    },
    {
      "epoch": 206.81170731707317,
      "grad_norm": 71.24171447753906,
      "learning_rate": 1.3787029831387808e-05,
      "loss": 71.6087,
      "step": 53150
    },
    {
      "epoch": 207.0039024390244,
      "grad_norm": 177.12579345703125,
      "learning_rate": 1.38e-05,
      "loss": 70.9804,
      "step": 53200
    },
    {
      "epoch": 207.0039024390244,
      "eval_loss": 1.1526858806610107,
      "eval_perplexity": 3.166686773300171,
      "eval_runtime": 35.772,
      "eval_samples_per_second": 55.434,
      "eval_steps_per_second": 6.933,
      "step": 53200
    },
    {
      "epoch": 207.1990243902439,
      "grad_norm": 42.70795440673828,
      "learning_rate": 1.3812970168612194e-05,
      "loss": 72.4584,
      "step": 53250
    },
    {
      "epoch": 207.39414634146343,
      "grad_norm": 49.54426574707031,
      "learning_rate": 1.3825940337224386e-05,
      "loss": 71.7068,
      "step": 53300
    },
    {
      "epoch": 207.58926829268293,
      "grad_norm": 58.14952850341797,
      "learning_rate": 1.3838910505836578e-05,
      "loss": 72.5034,
      "step": 53350
    },
    {
      "epoch": 207.78439024390244,
      "grad_norm": 56.39476776123047,
      "learning_rate": 1.385188067444877e-05,
      "loss": 71.5719,
      "step": 53400
    },
    {
      "epoch": 207.78439024390244,
      "eval_loss": 1.1515147686004639,
      "eval_perplexity": 3.162980556488037,
      "eval_runtime": 31.3786,
      "eval_samples_per_second": 63.196,
      "eval_steps_per_second": 7.903,
      "step": 53400
    },
    {
      "epoch": 207.97951219512194,
      "grad_norm": 70.53655242919922,
      "learning_rate": 1.3864850843060962e-05,
      "loss": 72.2985,
      "step": 53450
    },
    {
      "epoch": 208.17170731707316,
      "grad_norm": 54.86237335205078,
      "learning_rate": 1.3877821011673154e-05,
      "loss": 71.4574,
      "step": 53500
    },
    {
      "epoch": 208.3668292682927,
      "grad_norm": 88.5759048461914,
      "learning_rate": 1.3890791180285345e-05,
      "loss": 71.7309,
      "step": 53550
    },
    {
      "epoch": 208.5619512195122,
      "grad_norm": 69.07794952392578,
      "learning_rate": 1.3903761348897537e-05,
      "loss": 71.9219,
      "step": 53600
    },
    {
      "epoch": 208.5619512195122,
      "eval_loss": 1.1515048742294312,
      "eval_perplexity": 3.1629490852355957,
      "eval_runtime": 33.403,
      "eval_samples_per_second": 59.366,
      "eval_steps_per_second": 7.424,
      "step": 53600
    },
    {
      "epoch": 208.5619512195122,
      "step": 53600,
      "total_flos": 2.2024616700870132e+20,
      "train_loss": 42.385328687980994,
      "train_runtime": 61065.8149,
      "train_samples_per_second": 2684.972,
      "train_steps_per_second": 42.086
    }
  ],
  "logging_steps": 50,
  "max_steps": 2570000,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 10000,
  "save_steps": 200,
  "stateful_callbacks": {
    "EarlyStoppingCallback": {
      "args": {
        "early_stopping_patience": 5,
        "early_stopping_threshold": 0.0
      },
      "attributes": {
        "early_stopping_patience_counter": 5
      }
    },
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 2.2024616700870132e+20,
  "train_batch_size": 8,
  "trial_name": null,
  "trial_params": null
}
