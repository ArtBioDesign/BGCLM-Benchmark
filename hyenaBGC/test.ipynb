{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Imports\n",
    "# for HyenaDNA specifically\n",
    "import torch \n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from functools import partial\n",
    "from einops import rearrange\n",
    "from typing import Optional\n",
    "from functools import partial\n",
    "from torch import Tensor\n",
    "from torchvision.ops import StochasticDepth\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpcfs/fhome/yangchh/software/miniforge3/envs/evo2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#@title Huggingface Pretrained Wrapper\n",
    "# for Huggingface integration, we use a wrapper class around the model\n",
    "# to load weights\n",
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import transformers\n",
    "from transformers import PreTrainedModel, AutoModelForCausalLM, PretrainedConfig\n",
    "import re\n",
    "from hyenaDNA import HyenaDNAModel\n",
    "import torch\n",
    "\n",
    "def inject_substring(orig_str):\n",
    "    \"\"\"Hack to handle matching keys between models trained with and without\n",
    "    gradient checkpointing.\"\"\"\n",
    "\n",
    "    # modify for mixer keys\n",
    "    pattern = r\"\\.mixer\"\n",
    "    injection = \".mixer.layer\"\n",
    "\n",
    "    modified_string = re.sub(pattern, injection, orig_str)\n",
    "\n",
    "    # modify for mlp keys\n",
    "    pattern = r\"\\.mlp\"\n",
    "    injection = \".mlp.layer\"\n",
    "\n",
    "    modified_string = re.sub(pattern, injection, modified_string)\n",
    "\n",
    "    return modified_string\n",
    "\n",
    "def load_weights(scratch_dict, pretrained_dict, checkpointing=False):\n",
    "    \"\"\"Loads pretrained (backbone only) weights into the scratch state dict.\n",
    "\n",
    "    scratch_dict: dict, a state dict from a newly initialized HyenaDNA model\n",
    "    pretrained_dict: dict, a state dict from the pretrained ckpt\n",
    "    checkpointing: bool, whether the gradient checkpoint flag was used in the\n",
    "    pretrained model ckpt. This slightly changes state dict keys, so we patch\n",
    "    that if used.\n",
    "\n",
    "    return:\n",
    "    dict, a state dict with the pretrained weights loaded (head is scratch)\n",
    "\n",
    "    # loop thru state dict of scratch\n",
    "    # find the corresponding weights in the loaded model, and set it\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # need to do some state dict \"surgery\"\n",
    "    for key, value in scratch_dict.items():\n",
    "        if 'backbone' in key:\n",
    "            # the state dicts differ by one prefix, '.model', so we add that\n",
    "            key_loaded = 'model.' + key\n",
    "            # breakpoint()\n",
    "            # need to add an extra \".layer\" in key\n",
    "            if checkpointing:\n",
    "                key_loaded = inject_substring(key_loaded)\n",
    "            try:\n",
    "                scratch_dict[key] = pretrained_dict[key_loaded]\n",
    "            except:\n",
    "                raise Exception('key mismatch in the state dicts!')\n",
    "\n",
    "    # scratch_dict has been updated\n",
    "    return scratch_dict\n",
    "\n",
    "class HyenaDNAPreTrainedModel(PreTrainedModel):\n",
    "    \"\"\"\n",
    "    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n",
    "    models.\n",
    "    \"\"\"\n",
    "    base_model_prefix = \"hyenadna\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        pass\n",
    "\n",
    "    def forward(self, input_ids, **kwargs):\n",
    "        return self.model(input_ids, **kwargs)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls,\n",
    "                        path,\n",
    "                        model_name,\n",
    "                        download=False,\n",
    "                        config=None,\n",
    "                        device='cpu',\n",
    "                        use_head=False,\n",
    "                        n_classes=2,\n",
    "                      ):\n",
    "        # first check if it is a local path\n",
    "        pretrained_model_name_or_path = os.path.join(path, model_name)\n",
    "        if os.path.isdir(pretrained_model_name_or_path) and download == False:\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
    "        else:\n",
    "            hf_url = f'https://huggingface.co/LongSafari/{model_name}'\n",
    "\n",
    "            subprocess.run(f'rm -rf {pretrained_model_name_or_path}', shell=True)\n",
    "            command = f'mkdir -p {path} && cd {path} && git lfs install && git clone {hf_url}'\n",
    "            subprocess.run(command, shell=True)\n",
    "\n",
    "            if config is None:\n",
    "                config = json.load(open(os.path.join(pretrained_model_name_or_path, 'config.json')))\n",
    "\n",
    "        scratch_model = HyenaDNAModel(**config, use_head=use_head, n_classes=n_classes)  # the new model format\n",
    "        loaded_ckpt = torch.load(\n",
    "            os.path.join(pretrained_model_name_or_path, 'weights.ckpt'),\n",
    "            map_location=torch.device(device)\n",
    "        )\n",
    "\n",
    "        # need to load weights slightly different if using gradient checkpointing\n",
    "        if config.get(\"checkpoint_mixer\", False):\n",
    "            checkpointing = config[\"checkpoint_mixer\"] == True or config[\"checkpoint_mixer\"] == True\n",
    "        else:\n",
    "            checkpointing = False\n",
    "\n",
    "        # grab state dict from both and load weights\n",
    "        state_dict = load_weights(scratch_model.state_dict(), loaded_ckpt['state_dict'], checkpointing=checkpointing)\n",
    "\n",
    "        # scratch model has now been updated\n",
    "        scratch_model.load_state_dict(state_dict)\n",
    "        print(\"Loaded pretrained weights ok!\")\n",
    "        return scratch_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Tokenizer\n",
    "\n",
    "\"\"\"\n",
    "Just a simple character level tokenizer.\n",
    "\n",
    "From: https://github.com/dariush-bahrami/character-tokenizer/blob/master/charactertokenizer/core.py\n",
    "\n",
    "CharacterTokenzier for Hugging Face Transformers.\n",
    "This is heavily inspired from CanineTokenizer in transformers package.\n",
    "\"\"\"\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Sequence, Union\n",
    "\n",
    "from transformers.tokenization_utils import AddedToken, PreTrainedTokenizer\n",
    "\n",
    "class CharacterTokenizer(PreTrainedTokenizer):\n",
    "    def __init__(self, characters: Sequence[str], model_max_length: int, padding_side: str='left', **kwargs):\n",
    "        \"\"\"Character tokenizer for Hugging Face transformers.\n",
    "        Args:\n",
    "            characters (Sequence[str]): List of desired characters. Any character which\n",
    "                is not included in this list will be replaced by a special token called\n",
    "                [UNK] with id=6. Following are list of all of the special tokens with\n",
    "                their corresponding ids:\n",
    "                    \"[CLS]\": 0\n",
    "                    \"[SEP]\": 1\n",
    "                    \"[BOS]\": 2\n",
    "                    \"[MASK]\": 3\n",
    "                    \"[PAD]\": 4\n",
    "                    \"[RESERVED]\": 5\n",
    "                    \"[UNK]\": 6\n",
    "                an id (starting at 7) will be assigned to each character.\n",
    "            model_max_length (int): Model maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.characters = characters\n",
    "        self.model_max_length = model_max_length\n",
    "        bos_token = AddedToken(\"[BOS]\", lstrip=False, rstrip=False)\n",
    "        eos_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        sep_token = AddedToken(\"[SEP]\", lstrip=False, rstrip=False)\n",
    "        cls_token = AddedToken(\"[CLS]\", lstrip=False, rstrip=False)\n",
    "        pad_token = AddedToken(\"[PAD]\", lstrip=False, rstrip=False)\n",
    "        unk_token = AddedToken(\"[UNK]\", lstrip=False, rstrip=False)\n",
    "\n",
    "        mask_token = AddedToken(\"[MASK]\", lstrip=True, rstrip=False)\n",
    "\n",
    "        super().__init__(\n",
    "            bos_token=bos_token,\n",
    "            eos_token=sep_token,\n",
    "            sep_token=sep_token,\n",
    "            cls_token=cls_token,\n",
    "            pad_token=pad_token,\n",
    "            mask_token=mask_token,\n",
    "            unk_token=unk_token,\n",
    "            add_prefix_space=False,\n",
    "            model_max_length=model_max_length,\n",
    "            padding_side=padding_side,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self._vocab_str_to_int = {\n",
    "            \"[CLS]\": 0,\n",
    "            \"[SEP]\": 1,\n",
    "            \"[BOS]\": 2,\n",
    "            \"[MASK]\": 3,\n",
    "            \"[PAD]\": 4,\n",
    "            \"[RESERVED]\": 5,\n",
    "            \"[UNK]\": 6,\n",
    "            **{ch: i + 7 for i, ch in enumerate(characters)},\n",
    "        }\n",
    "        self._vocab_int_to_str = {v: k for k, v in self._vocab_str_to_int.items()}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self) -> int:\n",
    "        return len(self._vocab_str_to_int)\n",
    "\n",
    "    def _tokenize(self, text: str) -> List[str]:\n",
    "        return list(text)\n",
    "\n",
    "    def _convert_token_to_id(self, token: str) -> int:\n",
    "        return self._vocab_str_to_int.get(token, self._vocab_str_to_int[\"[UNK]\"])\n",
    "\n",
    "    def _convert_id_to_token(self, index: int) -> str:\n",
    "        return self._vocab_int_to_str[index]\n",
    "\n",
    "    def convert_tokens_to_string(self, tokens):\n",
    "        return \"\".join(tokens)\n",
    "\n",
    "    def build_inputs_with_special_tokens(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "        result = cls + token_ids_0 + sep\n",
    "        if token_ids_1 is not None:\n",
    "            result += token_ids_1 + sep\n",
    "        return result\n",
    "\n",
    "    def get_special_tokens_mask(\n",
    "        self,\n",
    "        token_ids_0: List[int],\n",
    "        token_ids_1: Optional[List[int]] = None,\n",
    "        already_has_special_tokens: bool = False,\n",
    "    ) -> List[int]:\n",
    "        if already_has_special_tokens:\n",
    "            return super().get_special_tokens_mask(\n",
    "                token_ids_0=token_ids_0,\n",
    "                token_ids_1=token_ids_1,\n",
    "                already_has_special_tokens=True,\n",
    "            )\n",
    "\n",
    "        result = [1] + ([0] * len(token_ids_0)) + [1]\n",
    "        if token_ids_1 is not None:\n",
    "            result += ([0] * len(token_ids_1)) + [1]\n",
    "        return result\n",
    "\n",
    "    def create_token_type_ids_from_sequences(\n",
    "        self, token_ids_0: List[int], token_ids_1: Optional[List[int]] = None\n",
    "    ) -> List[int]:\n",
    "        sep = [self.sep_token_id]\n",
    "        cls = [self.cls_token_id]\n",
    "\n",
    "        result = len(cls + token_ids_0 + sep) * [0]\n",
    "        if token_ids_1 is not None:\n",
    "            result += len(token_ids_1 + sep) * [1]\n",
    "        return result\n",
    "\n",
    "    def get_config(self) -> Dict:\n",
    "        return {\n",
    "            \"char_ords\": [ord(ch) for ch in self.characters],\n",
    "            \"model_max_length\": self.model_max_length,\n",
    "        }\n",
    "\n",
    "    @classmethod\n",
    "    def from_config(cls, config: Dict) -> \"CharacterTokenizer\":\n",
    "        cfg = {}\n",
    "        cfg[\"characters\"] = [chr(i) for i in config[\"char_ords\"]]\n",
    "        cfg[\"model_max_length\"] = config[\"model_max_length\"]\n",
    "        return cls(**cfg)\n",
    "\n",
    "    def save_pretrained(self, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        cfg = self.get_config()\n",
    "        with open(cfg_file, \"w\") as f:\n",
    "            json.dump(cfg, f, indent=4)\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, save_directory: Union[str, os.PathLike], **kwargs):\n",
    "        cfg_file = Path(save_directory) / \"tokenizer_config.json\"\n",
    "        with open(cfg_file) as f:\n",
    "            cfg = json.load(f)\n",
    "        return cls.from_config(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33m⚠️  Warning: 'huggingface-cli download' is deprecated. Use 'hf download' instead.\u001b[0m\n",
      "Fetching 4 files: 100%|█████████████████████████| 4/4 [00:00<00:00, 2779.07it/s]\n",
      "/hpcfs/fhome/yangchh/genome_lms/megaDNA/hyenaDNA/checkpoints/hyenadna-medium-450k-seqlen\n"
     ]
    }
   ],
   "source": [
    "! huggingface-cli download LongSafari/hyenadna-medium-450k-seqlen --local-dir ./checkpoints/hyenadna-medium-450k-seqlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights ok!\n",
      "tok_seq shape: torch.Size([1, 450000])\n",
      "torch.Size([1, 450000, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.insert(0, '/hpcfs/fhome/yangchh/genome_lms/megaDNA/hyenaDNA')\n",
    "from hyenaDNA import HyenaDNAPreTrainedModel\n",
    "from tokenizer import Character_Tokenizer as CharacterTokenizer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# instantiate pretrained model\n",
    "pretrained_model_name = 'hyenadna-medium-450k-seqlen'\n",
    "max_length = 450_000\n",
    "\n",
    "model = HyenaDNAPreTrainedModel.from_pretrained(\n",
    "    './checkpoints',\n",
    "    pretrained_model_name,\n",
    ")\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = CharacterTokenizer(\n",
    "    characters=['A', 'C', 'G', 'T', 'N'],\n",
    "    model_max_length=max_length,\n",
    ")\n",
    "\n",
    "# create a sample\n",
    "sequence = 'ACTG' * int(max_length/4)\n",
    "\n",
    "# 使用 return_tensors=\"pt\" 直接返回 PyTorch tensor\n",
    "tok_seq = tokenizer(sequence, add_special_tokens=False, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# 直接放到设备上，不需要 unsqueeze (tokenizer 已经返回 [batch, seq_len] 形状)\n",
    "tok_seq = tok_seq.to(device)\n",
    "\n",
    "print(f\"tok_seq shape: {tok_seq.shape}\")  # 应该是 [1, 450000]\n",
    "\n",
    "# prep model and forward\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "    embeddings = model(tok_seq)\n",
    "\n",
    "print(embeddings.shape)  # 输出: torch.Size([1, 450000, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights ok!\n",
      "============================================================\n",
      "模型参数统计\n",
      "============================================================\n",
      "总参数量: 6,550,528 (6.55M)\n",
      "可训练参数量: 6,550,528 (6.55M)\n",
      "\n",
      "模型结构:\n",
      "HyenaDNAModel(\n",
      "  (backbone): LMBackbone(\n",
      "    (embeddings): GPT2Embeddings(\n",
      "      (word_embeddings): Embedding(16, 256)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): Block(\n",
      "        (mixer): HyenaOperator(\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
      "          (filter_fn): HyenaFilter(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (pos_emb): PositionalEmbedding()\n",
      "            (implicit_filter): Sequential(\n",
      "              (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "              (1): Sin()\n",
      "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (3): Sin()\n",
      "              (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (5): Sin()\n",
      "              (6): Linear(in_features=64, out_features=256, bias=False)\n",
      "            )\n",
      "            (modulation): ExponentialModulation()\n",
      "          )\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1-7): 7 x Block(\n",
      "        (mixer): HyenaOperator(\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
      "          (filter_fn): HyenaFilter(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (pos_emb): PositionalEmbedding()\n",
      "            (implicit_filter): Sequential(\n",
      "              (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "              (1): Sin()\n",
      "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (3): Sin()\n",
      "              (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (5): Sin()\n",
      "              (6): Linear(in_features=64, out_features=256, bias=False)\n",
      "            )\n",
      "            (modulation): ExponentialModulation()\n",
      "          )\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (drop_f): Dropout(p=0.0, inplace=False)\n",
      "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "各层参数详情:\n",
      "  backbone.embeddings.word_embeddings.weight: torch.Size([16, 256]), 4,096 params\n",
      "  backbone.layers.0.mixer.in_proj.weight: torch.Size([768, 256]), 196,608 params\n",
      "  backbone.layers.0.mixer.in_proj.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.0.mixer.out_proj.weight: torch.Size([256, 256]), 65,536 params\n",
      "  backbone.layers.0.mixer.out_proj.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.0.mixer.short_filter.weight: torch.Size([768, 1, 3]), 2,304 params\n",
      "  backbone.layers.0.mixer.short_filter.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.0.mixer.filter_fn.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.0.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5]), 320 params\n",
      "  backbone.layers.0.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.0.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64]), 64 params\n",
      "  backbone.layers.0.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.0.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.0.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.0.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.0.mixer.filter_fn.implicit_filter.6.weight: torch.Size([256, 64]), 16,384 params\n",
      "  backbone.layers.0.norm1.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.0.norm1.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.0.mlp.fc1.weight: torch.Size([1024, 256]), 262,144 params\n",
      "  backbone.layers.0.mlp.fc1.bias: torch.Size([1024]), 1,024 params\n",
      "  backbone.layers.0.mlp.fc2.weight: torch.Size([256, 1024]), 262,144 params\n",
      "  backbone.layers.0.mlp.fc2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.0.norm2.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.0.norm2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.1.mixer.in_proj.weight: torch.Size([768, 256]), 196,608 params\n",
      "  backbone.layers.1.mixer.in_proj.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.1.mixer.out_proj.weight: torch.Size([256, 256]), 65,536 params\n",
      "  backbone.layers.1.mixer.out_proj.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.1.mixer.short_filter.weight: torch.Size([768, 1, 3]), 2,304 params\n",
      "  backbone.layers.1.mixer.short_filter.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.1.mixer.filter_fn.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.1.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5]), 320 params\n",
      "  backbone.layers.1.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.1.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64]), 64 params\n",
      "  backbone.layers.1.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.1.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.1.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.1.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.1.mixer.filter_fn.implicit_filter.6.weight: torch.Size([256, 64]), 16,384 params\n",
      "  backbone.layers.1.norm1.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.1.norm1.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.1.mlp.fc1.weight: torch.Size([1024, 256]), 262,144 params\n",
      "  backbone.layers.1.mlp.fc1.bias: torch.Size([1024]), 1,024 params\n",
      "  backbone.layers.1.mlp.fc2.weight: torch.Size([256, 1024]), 262,144 params\n",
      "  backbone.layers.1.mlp.fc2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.1.norm2.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.1.norm2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.2.mixer.in_proj.weight: torch.Size([768, 256]), 196,608 params\n",
      "  backbone.layers.2.mixer.in_proj.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.2.mixer.out_proj.weight: torch.Size([256, 256]), 65,536 params\n",
      "  backbone.layers.2.mixer.out_proj.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.2.mixer.short_filter.weight: torch.Size([768, 1, 3]), 2,304 params\n",
      "  backbone.layers.2.mixer.short_filter.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.2.mixer.filter_fn.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.2.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5]), 320 params\n",
      "  backbone.layers.2.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.2.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64]), 64 params\n",
      "  backbone.layers.2.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.2.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.2.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.2.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.2.mixer.filter_fn.implicit_filter.6.weight: torch.Size([256, 64]), 16,384 params\n",
      "  backbone.layers.2.norm1.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.2.norm1.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.2.mlp.fc1.weight: torch.Size([1024, 256]), 262,144 params\n",
      "  backbone.layers.2.mlp.fc1.bias: torch.Size([1024]), 1,024 params\n",
      "  backbone.layers.2.mlp.fc2.weight: torch.Size([256, 1024]), 262,144 params\n",
      "  backbone.layers.2.mlp.fc2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.2.norm2.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.2.norm2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.3.mixer.in_proj.weight: torch.Size([768, 256]), 196,608 params\n",
      "  backbone.layers.3.mixer.in_proj.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.3.mixer.out_proj.weight: torch.Size([256, 256]), 65,536 params\n",
      "  backbone.layers.3.mixer.out_proj.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.3.mixer.short_filter.weight: torch.Size([768, 1, 3]), 2,304 params\n",
      "  backbone.layers.3.mixer.short_filter.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.3.mixer.filter_fn.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.3.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5]), 320 params\n",
      "  backbone.layers.3.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.3.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64]), 64 params\n",
      "  backbone.layers.3.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.3.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.3.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.3.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.3.mixer.filter_fn.implicit_filter.6.weight: torch.Size([256, 64]), 16,384 params\n",
      "  backbone.layers.3.norm1.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.3.norm1.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.3.mlp.fc1.weight: torch.Size([1024, 256]), 262,144 params\n",
      "  backbone.layers.3.mlp.fc1.bias: torch.Size([1024]), 1,024 params\n",
      "  backbone.layers.3.mlp.fc2.weight: torch.Size([256, 1024]), 262,144 params\n",
      "  backbone.layers.3.mlp.fc2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.3.norm2.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.3.norm2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.4.mixer.in_proj.weight: torch.Size([768, 256]), 196,608 params\n",
      "  backbone.layers.4.mixer.in_proj.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.4.mixer.out_proj.weight: torch.Size([256, 256]), 65,536 params\n",
      "  backbone.layers.4.mixer.out_proj.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.4.mixer.short_filter.weight: torch.Size([768, 1, 3]), 2,304 params\n",
      "  backbone.layers.4.mixer.short_filter.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.4.mixer.filter_fn.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.4.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5]), 320 params\n",
      "  backbone.layers.4.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.4.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64]), 64 params\n",
      "  backbone.layers.4.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.4.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.4.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.4.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.4.mixer.filter_fn.implicit_filter.6.weight: torch.Size([256, 64]), 16,384 params\n",
      "  backbone.layers.4.norm1.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.4.norm1.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.4.mlp.fc1.weight: torch.Size([1024, 256]), 262,144 params\n",
      "  backbone.layers.4.mlp.fc1.bias: torch.Size([1024]), 1,024 params\n",
      "  backbone.layers.4.mlp.fc2.weight: torch.Size([256, 1024]), 262,144 params\n",
      "  backbone.layers.4.mlp.fc2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.4.norm2.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.4.norm2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.5.mixer.in_proj.weight: torch.Size([768, 256]), 196,608 params\n",
      "  backbone.layers.5.mixer.in_proj.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.5.mixer.out_proj.weight: torch.Size([256, 256]), 65,536 params\n",
      "  backbone.layers.5.mixer.out_proj.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.5.mixer.short_filter.weight: torch.Size([768, 1, 3]), 2,304 params\n",
      "  backbone.layers.5.mixer.short_filter.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.5.mixer.filter_fn.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.5.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5]), 320 params\n",
      "  backbone.layers.5.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.5.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64]), 64 params\n",
      "  backbone.layers.5.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.5.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.5.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.5.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.5.mixer.filter_fn.implicit_filter.6.weight: torch.Size([256, 64]), 16,384 params\n",
      "  backbone.layers.5.norm1.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.5.norm1.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.5.mlp.fc1.weight: torch.Size([1024, 256]), 262,144 params\n",
      "  backbone.layers.5.mlp.fc1.bias: torch.Size([1024]), 1,024 params\n",
      "  backbone.layers.5.mlp.fc2.weight: torch.Size([256, 1024]), 262,144 params\n",
      "  backbone.layers.5.mlp.fc2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.5.norm2.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.5.norm2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.6.mixer.in_proj.weight: torch.Size([768, 256]), 196,608 params\n",
      "  backbone.layers.6.mixer.in_proj.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.6.mixer.out_proj.weight: torch.Size([256, 256]), 65,536 params\n",
      "  backbone.layers.6.mixer.out_proj.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.6.mixer.short_filter.weight: torch.Size([768, 1, 3]), 2,304 params\n",
      "  backbone.layers.6.mixer.short_filter.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.6.mixer.filter_fn.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.6.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5]), 320 params\n",
      "  backbone.layers.6.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.6.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64]), 64 params\n",
      "  backbone.layers.6.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.6.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.6.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.6.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.6.mixer.filter_fn.implicit_filter.6.weight: torch.Size([256, 64]), 16,384 params\n",
      "  backbone.layers.6.norm1.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.6.norm1.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.6.mlp.fc1.weight: torch.Size([1024, 256]), 262,144 params\n",
      "  backbone.layers.6.mlp.fc1.bias: torch.Size([1024]), 1,024 params\n",
      "  backbone.layers.6.mlp.fc2.weight: torch.Size([256, 1024]), 262,144 params\n",
      "  backbone.layers.6.mlp.fc2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.6.norm2.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.6.norm2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.7.mixer.in_proj.weight: torch.Size([768, 256]), 196,608 params\n",
      "  backbone.layers.7.mixer.in_proj.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.7.mixer.out_proj.weight: torch.Size([256, 256]), 65,536 params\n",
      "  backbone.layers.7.mixer.out_proj.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.7.mixer.short_filter.weight: torch.Size([768, 1, 3]), 2,304 params\n",
      "  backbone.layers.7.mixer.short_filter.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.7.mixer.filter_fn.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.7.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5]), 320 params\n",
      "  backbone.layers.7.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.7.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64]), 64 params\n",
      "  backbone.layers.7.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.7.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.7.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.7.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.7.mixer.filter_fn.implicit_filter.6.weight: torch.Size([256, 64]), 16,384 params\n",
      "  backbone.layers.7.norm1.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.7.norm1.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.7.mlp.fc1.weight: torch.Size([1024, 256]), 262,144 params\n",
      "  backbone.layers.7.mlp.fc1.bias: torch.Size([1024]), 1,024 params\n",
      "  backbone.layers.7.mlp.fc2.weight: torch.Size([256, 1024]), 262,144 params\n",
      "  backbone.layers.7.mlp.fc2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.7.norm2.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.7.norm2.bias: torch.Size([256]), 256 params\n",
      "  backbone.ln_f.weight: torch.Size([256]), 256 params\n",
      "  backbone.ln_f.bias: torch.Size([256]), 256 params\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.insert(0, '/hpcfs/fhome/yangchh/genome_lms/megaDNA/hyenaDNA')\n",
    "\n",
    "from hyenaDNA import HyenaDNAPreTrainedModel\n",
    "from tokenizer import Character_Tokenizer as CharacterTokenizer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# instantiate pretrained model\n",
    "pretrained_model_name = 'hyenadna-medium-450k-seqlen'\n",
    "max_length = 450_000\n",
    "\n",
    "model = HyenaDNAPreTrainedModel.from_pretrained(\n",
    "    './checkpoints',\n",
    "    pretrained_model_name,\n",
    ")\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = CharacterTokenizer(\n",
    "    characters=['A', 'C', 'G', 'T', 'N'],\n",
    "    model_max_length=max_length,\n",
    ")\n",
    "\n",
    "# ===== 打印模型参数 =====\n",
    "print(\"=\" * 60)\n",
    "print(\"模型参数统计\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 计算总参数量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"总参数量: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "print(f\"可训练参数量: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n",
    "\n",
    "# 打印模型结构\n",
    "print(\"\\n模型结构:\")\n",
    "print(model)\n",
    "\n",
    "# 打印各层参数\n",
    "print(\"\\n各层参数详情:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: {param.shape}, {param.numel():,} params\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights ok!\n",
      "============================================================\n",
      "模型参数统计\n",
      "============================================================\n",
      "总参数量: 6,550,528 (6.55M)\n",
      "可训练参数量: 6,550,528 (6.55M)\n",
      "\n",
      "模型结构:\n",
      "HyenaDNAModel(\n",
      "  (backbone): LMBackbone(\n",
      "    (embeddings): GPT2Embeddings(\n",
      "      (word_embeddings): Embedding(16, 256)\n",
      "    )\n",
      "    (layers): ModuleList(\n",
      "      (0): Block(\n",
      "        (mixer): HyenaOperator(\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
      "          (filter_fn): HyenaFilter(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (pos_emb): PositionalEmbedding()\n",
      "            (implicit_filter): Sequential(\n",
      "              (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "              (1): Sin()\n",
      "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (3): Sin()\n",
      "              (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (5): Sin()\n",
      "              (6): Linear(in_features=64, out_features=256, bias=False)\n",
      "            )\n",
      "            (modulation): ExponentialModulation()\n",
      "          )\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "      (1-7): 7 x Block(\n",
      "        (mixer): HyenaOperator(\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
      "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
      "          (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
      "          (filter_fn): HyenaFilter(\n",
      "            (dropout): Dropout(p=0.0, inplace=False)\n",
      "            (pos_emb): PositionalEmbedding()\n",
      "            (implicit_filter): Sequential(\n",
      "              (0): Linear(in_features=5, out_features=64, bias=True)\n",
      "              (1): Sin()\n",
      "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (3): Sin()\n",
      "              (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "              (5): Sin()\n",
      "              (6): Linear(in_features=64, out_features=256, bias=False)\n",
      "            )\n",
      "            (modulation): ExponentialModulation()\n",
      "          )\n",
      "        )\n",
      "        (dropout1): Dropout(p=0.0, inplace=False)\n",
      "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): Mlp(\n",
      "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "        )\n",
      "        (dropout2): Dropout(p=0.0, inplace=False)\n",
      "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (drop_f): Dropout(p=0.0, inplace=False)\n",
      "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "各层参数详情:\n",
      "  backbone.embeddings.word_embeddings.weight: torch.Size([16, 256]), 4,096 params\n",
      "  backbone.layers.0.mixer.in_proj.weight: torch.Size([768, 256]), 196,608 params\n",
      "  backbone.layers.0.mixer.in_proj.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.0.mixer.out_proj.weight: torch.Size([256, 256]), 65,536 params\n",
      "  backbone.layers.0.mixer.out_proj.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.0.mixer.short_filter.weight: torch.Size([768, 1, 3]), 2,304 params\n",
      "  backbone.layers.0.mixer.short_filter.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.0.mixer.filter_fn.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.0.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5]), 320 params\n",
      "  backbone.layers.0.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.0.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64]), 64 params\n",
      "  backbone.layers.0.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.0.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.0.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.0.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.0.mixer.filter_fn.implicit_filter.6.weight: torch.Size([256, 64]), 16,384 params\n",
      "  backbone.layers.0.norm1.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.0.norm1.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.0.mlp.fc1.weight: torch.Size([1024, 256]), 262,144 params\n",
      "  backbone.layers.0.mlp.fc1.bias: torch.Size([1024]), 1,024 params\n",
      "  backbone.layers.0.mlp.fc2.weight: torch.Size([256, 1024]), 262,144 params\n",
      "  backbone.layers.0.mlp.fc2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.0.norm2.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.0.norm2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.1.mixer.in_proj.weight: torch.Size([768, 256]), 196,608 params\n",
      "  backbone.layers.1.mixer.in_proj.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.1.mixer.out_proj.weight: torch.Size([256, 256]), 65,536 params\n",
      "  backbone.layers.1.mixer.out_proj.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.1.mixer.short_filter.weight: torch.Size([768, 1, 3]), 2,304 params\n",
      "  backbone.layers.1.mixer.short_filter.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.1.mixer.filter_fn.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.1.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5]), 320 params\n",
      "  backbone.layers.1.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.1.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64]), 64 params\n",
      "  backbone.layers.1.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.1.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.1.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.1.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.1.mixer.filter_fn.implicit_filter.6.weight: torch.Size([256, 64]), 16,384 params\n",
      "  backbone.layers.1.norm1.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.1.norm1.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.1.mlp.fc1.weight: torch.Size([1024, 256]), 262,144 params\n",
      "  backbone.layers.1.mlp.fc1.bias: torch.Size([1024]), 1,024 params\n",
      "  backbone.layers.1.mlp.fc2.weight: torch.Size([256, 1024]), 262,144 params\n",
      "  backbone.layers.1.mlp.fc2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.1.norm2.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.1.norm2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.2.mixer.in_proj.weight: torch.Size([768, 256]), 196,608 params\n",
      "  backbone.layers.2.mixer.in_proj.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.2.mixer.out_proj.weight: torch.Size([256, 256]), 65,536 params\n",
      "  backbone.layers.2.mixer.out_proj.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.2.mixer.short_filter.weight: torch.Size([768, 1, 3]), 2,304 params\n",
      "  backbone.layers.2.mixer.short_filter.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.2.mixer.filter_fn.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.2.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5]), 320 params\n",
      "  backbone.layers.2.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.2.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64]), 64 params\n",
      "  backbone.layers.2.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.2.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.2.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.2.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.2.mixer.filter_fn.implicit_filter.6.weight: torch.Size([256, 64]), 16,384 params\n",
      "  backbone.layers.2.norm1.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.2.norm1.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.2.mlp.fc1.weight: torch.Size([1024, 256]), 262,144 params\n",
      "  backbone.layers.2.mlp.fc1.bias: torch.Size([1024]), 1,024 params\n",
      "  backbone.layers.2.mlp.fc2.weight: torch.Size([256, 1024]), 262,144 params\n",
      "  backbone.layers.2.mlp.fc2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.2.norm2.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.2.norm2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.3.mixer.in_proj.weight: torch.Size([768, 256]), 196,608 params\n",
      "  backbone.layers.3.mixer.in_proj.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.3.mixer.out_proj.weight: torch.Size([256, 256]), 65,536 params\n",
      "  backbone.layers.3.mixer.out_proj.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.3.mixer.short_filter.weight: torch.Size([768, 1, 3]), 2,304 params\n",
      "  backbone.layers.3.mixer.short_filter.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.3.mixer.filter_fn.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.3.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5]), 320 params\n",
      "  backbone.layers.3.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.3.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64]), 64 params\n",
      "  backbone.layers.3.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.3.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.3.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.3.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.3.mixer.filter_fn.implicit_filter.6.weight: torch.Size([256, 64]), 16,384 params\n",
      "  backbone.layers.3.norm1.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.3.norm1.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.3.mlp.fc1.weight: torch.Size([1024, 256]), 262,144 params\n",
      "  backbone.layers.3.mlp.fc1.bias: torch.Size([1024]), 1,024 params\n",
      "  backbone.layers.3.mlp.fc2.weight: torch.Size([256, 1024]), 262,144 params\n",
      "  backbone.layers.3.mlp.fc2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.3.norm2.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.3.norm2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.4.mixer.in_proj.weight: torch.Size([768, 256]), 196,608 params\n",
      "  backbone.layers.4.mixer.in_proj.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.4.mixer.out_proj.weight: torch.Size([256, 256]), 65,536 params\n",
      "  backbone.layers.4.mixer.out_proj.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.4.mixer.short_filter.weight: torch.Size([768, 1, 3]), 2,304 params\n",
      "  backbone.layers.4.mixer.short_filter.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.4.mixer.filter_fn.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.4.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5]), 320 params\n",
      "  backbone.layers.4.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.4.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64]), 64 params\n",
      "  backbone.layers.4.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.4.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.4.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.4.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.4.mixer.filter_fn.implicit_filter.6.weight: torch.Size([256, 64]), 16,384 params\n",
      "  backbone.layers.4.norm1.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.4.norm1.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.4.mlp.fc1.weight: torch.Size([1024, 256]), 262,144 params\n",
      "  backbone.layers.4.mlp.fc1.bias: torch.Size([1024]), 1,024 params\n",
      "  backbone.layers.4.mlp.fc2.weight: torch.Size([256, 1024]), 262,144 params\n",
      "  backbone.layers.4.mlp.fc2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.4.norm2.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.4.norm2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.5.mixer.in_proj.weight: torch.Size([768, 256]), 196,608 params\n",
      "  backbone.layers.5.mixer.in_proj.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.5.mixer.out_proj.weight: torch.Size([256, 256]), 65,536 params\n",
      "  backbone.layers.5.mixer.out_proj.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.5.mixer.short_filter.weight: torch.Size([768, 1, 3]), 2,304 params\n",
      "  backbone.layers.5.mixer.short_filter.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.5.mixer.filter_fn.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.5.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5]), 320 params\n",
      "  backbone.layers.5.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.5.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64]), 64 params\n",
      "  backbone.layers.5.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.5.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.5.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.5.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.5.mixer.filter_fn.implicit_filter.6.weight: torch.Size([256, 64]), 16,384 params\n",
      "  backbone.layers.5.norm1.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.5.norm1.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.5.mlp.fc1.weight: torch.Size([1024, 256]), 262,144 params\n",
      "  backbone.layers.5.mlp.fc1.bias: torch.Size([1024]), 1,024 params\n",
      "  backbone.layers.5.mlp.fc2.weight: torch.Size([256, 1024]), 262,144 params\n",
      "  backbone.layers.5.mlp.fc2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.5.norm2.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.5.norm2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.6.mixer.in_proj.weight: torch.Size([768, 256]), 196,608 params\n",
      "  backbone.layers.6.mixer.in_proj.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.6.mixer.out_proj.weight: torch.Size([256, 256]), 65,536 params\n",
      "  backbone.layers.6.mixer.out_proj.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.6.mixer.short_filter.weight: torch.Size([768, 1, 3]), 2,304 params\n",
      "  backbone.layers.6.mixer.short_filter.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.6.mixer.filter_fn.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.6.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5]), 320 params\n",
      "  backbone.layers.6.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.6.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64]), 64 params\n",
      "  backbone.layers.6.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.6.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.6.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.6.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.6.mixer.filter_fn.implicit_filter.6.weight: torch.Size([256, 64]), 16,384 params\n",
      "  backbone.layers.6.norm1.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.6.norm1.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.6.mlp.fc1.weight: torch.Size([1024, 256]), 262,144 params\n",
      "  backbone.layers.6.mlp.fc1.bias: torch.Size([1024]), 1,024 params\n",
      "  backbone.layers.6.mlp.fc2.weight: torch.Size([256, 1024]), 262,144 params\n",
      "  backbone.layers.6.mlp.fc2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.6.norm2.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.6.norm2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.7.mixer.in_proj.weight: torch.Size([768, 256]), 196,608 params\n",
      "  backbone.layers.7.mixer.in_proj.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.7.mixer.out_proj.weight: torch.Size([256, 256]), 65,536 params\n",
      "  backbone.layers.7.mixer.out_proj.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.7.mixer.short_filter.weight: torch.Size([768, 1, 3]), 2,304 params\n",
      "  backbone.layers.7.mixer.short_filter.bias: torch.Size([768]), 768 params\n",
      "  backbone.layers.7.mixer.filter_fn.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.7.mixer.filter_fn.implicit_filter.0.weight: torch.Size([64, 5]), 320 params\n",
      "  backbone.layers.7.mixer.filter_fn.implicit_filter.0.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.7.mixer.filter_fn.implicit_filter.1.freq: torch.Size([1, 64]), 64 params\n",
      "  backbone.layers.7.mixer.filter_fn.implicit_filter.2.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.7.mixer.filter_fn.implicit_filter.2.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.7.mixer.filter_fn.implicit_filter.4.weight: torch.Size([64, 64]), 4,096 params\n",
      "  backbone.layers.7.mixer.filter_fn.implicit_filter.4.bias: torch.Size([64]), 64 params\n",
      "  backbone.layers.7.mixer.filter_fn.implicit_filter.6.weight: torch.Size([256, 64]), 16,384 params\n",
      "  backbone.layers.7.norm1.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.7.norm1.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.7.mlp.fc1.weight: torch.Size([1024, 256]), 262,144 params\n",
      "  backbone.layers.7.mlp.fc1.bias: torch.Size([1024]), 1,024 params\n",
      "  backbone.layers.7.mlp.fc2.weight: torch.Size([256, 1024]), 262,144 params\n",
      "  backbone.layers.7.mlp.fc2.bias: torch.Size([256]), 256 params\n",
      "  backbone.layers.7.norm2.weight: torch.Size([256]), 256 params\n",
      "  backbone.layers.7.norm2.bias: torch.Size([256]), 256 params\n",
      "  backbone.ln_f.weight: torch.Size([256]), 256 params\n",
      "  backbone.ln_f.bias: torch.Size([256]), 256 params\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import sys\n",
    "sys.path.insert(0, '/hpcfs/fhome/yangchh/genome_lms/megaDNA/hyenaDNA')\n",
    "\n",
    "from hyenaDNA import HyenaDNAPreTrainedModel\n",
    "from tokenizer import Character_Tokenizer as CharacterTokenizer\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# instantiate pretrained model\n",
    "pretrained_model_name = 'hyenadna-large-1m-seqlen'\n",
    "max_length = 1_000_000\n",
    "\n",
    "model = HyenaDNAPreTrainedModel.from_pretrained(\n",
    "    './checkpoints',\n",
    "    pretrained_model_name,\n",
    ")\n",
    "\n",
    "# create tokenizer\n",
    "tokenizer = CharacterTokenizer(\n",
    "    characters=['A', 'C', 'G', 'T', 'N'],\n",
    "    model_max_length=max_length,\n",
    ")\n",
    "\n",
    "# ===== 打印模型参数 =====\n",
    "print(\"=\" * 60)\n",
    "print(\"模型参数统计\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 计算总参数量\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"总参数量: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "print(f\"可训练参数量: {trainable_params:,} ({trainable_params/1e6:.2f}M)\")\n",
    "\n",
    "# 打印模型结构\n",
    "print(\"\\n模型结构:\")\n",
    "print(model)\n",
    "\n",
    "# 打印各层参数|\n",
    "print(\"\\n各层参数详情:\")\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"  {name}: {param.shape}, {param.numel():,} params\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evo2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
